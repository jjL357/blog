<!DOCTYPE html>
<html lang="zh-cn" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning | JJ&#39;s Blog</title>
<meta name="keywords" content="MLLM, Speculative Decoding, Paper Note">
<meta name="description" content="
Conference: EMNLP&#39;25
Github: https://github.com/zju-jiyicheng/SpecVLM

1. Motivation
Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.
ä¾‹å¦‚ï¼š
LLaVA-OneVision (Li et al., 2024a) å°†æ¯ä¸€å¸§å¤„ç†ä¸º 196 ä¸ªè§†è§‰ tokenã€‚è‹¥è§†é¢‘ä¸ºä¸¤åˆ†é’Ÿã€60 FPSï¼Œåˆ™æ€» token æ•°é‡è¶…è¿‡ 100 ä¸‡ã€‚
å¦‚æ­¤å¤§é‡çš„ video tokens å¯¼è‡´ï¼š

åºåˆ—é•¿åº¦æ€¥å‰§å¢åŠ ï¼›
Prefill é˜¶æ®µçš„ attention å¼€é”€å‘ˆå¹³æ–¹çº§å¢é•¿ï¼›
Decoding é˜¶æ®µ KV cache æ€¥é€Ÿè†¨èƒ€ï¼Œæˆä¸ºæ˜¾è‘—çš„ GPU å†…å­˜ç“¶é¢ˆã€‚

åœ¨ autoregressive ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯æ­¥ç”Ÿæˆçš„ KV cache éƒ½å¿…é¡»ä¸æ¨¡å‹å‚æ•°ä¸€èµ·åŠ è½½ä¸å­˜å‚¨äº GPU æ˜¾å­˜ï¼Œå¯¼è‡´æ˜¾è‘—çš„ memory-bound ç°è±¡ã€‚">
<meta name="author" content="">
<link rel="canonical" href="https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/">
<link crossorigin="anonymous" href="https://jjl357.github.io/blog/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jjl357.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jjl357.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jjl357.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jjl357.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://jjl357.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh-cn" href="https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><script type="text/javascript"
        async
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<meta property="og:url" content="https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/">
  <meta property="og:site_name" content="JJ&#39;s Blog">
  <meta property="og:title" content="SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning">
  <meta property="og:description" content="
Conference: EMNLP&#39;25 Github: https://github.com/zju-jiyicheng/SpecVLM
1. Motivation Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.
ä¾‹å¦‚ï¼š LLaVA-OneVision (Li et al., 2024a) å°†æ¯ä¸€å¸§å¤„ç†ä¸º 196 ä¸ªè§†è§‰ tokenã€‚è‹¥è§†é¢‘ä¸ºä¸¤åˆ†é’Ÿã€60 FPSï¼Œåˆ™æ€» token æ•°é‡è¶…è¿‡ 100 ä¸‡ã€‚ å¦‚æ­¤å¤§é‡çš„ video tokens å¯¼è‡´ï¼š
åºåˆ—é•¿åº¦æ€¥å‰§å¢åŠ ï¼› Prefill é˜¶æ®µçš„ attention å¼€é”€å‘ˆå¹³æ–¹çº§å¢é•¿ï¼› Decoding é˜¶æ®µ KV cache æ€¥é€Ÿè†¨èƒ€ï¼Œæˆä¸ºæ˜¾è‘—çš„ GPU å†…å­˜ç“¶é¢ˆã€‚ åœ¨ autoregressive ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯æ­¥ç”Ÿæˆçš„ KV cache éƒ½å¿…é¡»ä¸æ¨¡å‹å‚æ•°ä¸€èµ·åŠ è½½ä¸å­˜å‚¨äº GPU æ˜¾å­˜ï¼Œå¯¼è‡´æ˜¾è‘—çš„ memory-bound ç°è±¡ã€‚">
  <meta property="og:locale" content="zh-cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-29T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-29T00:00:00+00:00">
    <meta property="article:tag" content="MLLM">
    <meta property="article:tag" content="Speculative Decoding">
    <meta property="article:tag" content="Paper Note">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning">
<meta name="twitter:description" content="
Conference: EMNLP&#39;25
Github: https://github.com/zju-jiyicheng/SpecVLM

1. Motivation
Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.
ä¾‹å¦‚ï¼š
LLaVA-OneVision (Li et al., 2024a) å°†æ¯ä¸€å¸§å¤„ç†ä¸º 196 ä¸ªè§†è§‰ tokenã€‚è‹¥è§†é¢‘ä¸ºä¸¤åˆ†é’Ÿã€60 FPSï¼Œåˆ™æ€» token æ•°é‡è¶…è¿‡ 100 ä¸‡ã€‚
å¦‚æ­¤å¤§é‡çš„ video tokens å¯¼è‡´ï¼š

åºåˆ—é•¿åº¦æ€¥å‰§å¢åŠ ï¼›
Prefill é˜¶æ®µçš„ attention å¼€é”€å‘ˆå¹³æ–¹çº§å¢é•¿ï¼›
Decoding é˜¶æ®µ KV cache æ€¥é€Ÿè†¨èƒ€ï¼Œæˆä¸ºæ˜¾è‘—çš„ GPU å†…å­˜ç“¶é¢ˆã€‚

åœ¨ autoregressive ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯æ­¥ç”Ÿæˆçš„ KV cache éƒ½å¿…é¡»ä¸æ¨¡å‹å‚æ•°ä¸€èµ·åŠ è½½ä¸å­˜å‚¨äº GPU æ˜¾å­˜ï¼Œå¯¼è‡´æ˜¾è‘—çš„ memory-bound ç°è±¡ã€‚">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jjl357.github.io/blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
      "item": "https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
  "name": "SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
  "description": "\nConference: EMNLP'25 Github: https://github.com/zju-jiyicheng/SpecVLM\n1. Motivation Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.\nä¾‹å¦‚ï¼š LLaVA-OneVision (Li et al., 2024a) å°†æ¯ä¸€å¸§å¤„ç†ä¸º 196 ä¸ªè§†è§‰ tokenã€‚è‹¥è§†é¢‘ä¸ºä¸¤åˆ†é’Ÿã€60 FPSï¼Œåˆ™æ€» token æ•°é‡è¶…è¿‡ 100 ä¸‡ã€‚ å¦‚æ­¤å¤§é‡çš„ video tokens å¯¼è‡´ï¼š\nåºåˆ—é•¿åº¦æ€¥å‰§å¢åŠ ï¼› Prefill é˜¶æ®µçš„ attention å¼€é”€å‘ˆå¹³æ–¹çº§å¢é•¿ï¼› Decoding é˜¶æ®µ KV cache æ€¥é€Ÿè†¨èƒ€ï¼Œæˆä¸ºæ˜¾è‘—çš„ GPU å†…å­˜ç“¶é¢ˆã€‚ åœ¨ autoregressive ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯æ­¥ç”Ÿæˆçš„ KV cache éƒ½å¿…é¡»ä¸æ¨¡å‹å‚æ•°ä¸€èµ·åŠ è½½ä¸å­˜å‚¨äº GPU æ˜¾å­˜ï¼Œå¯¼è‡´æ˜¾è‘—çš„ memory-bound ç°è±¡ã€‚\n",
  "keywords": [
    "MLLM", "Speculative Decoding", "Paper Note"
  ],
  "articleBody": "\nConference: EMNLP'25 Github: https://github.com/zju-jiyicheng/SpecVLM\n1. Motivation Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.\nä¾‹å¦‚ï¼š LLaVA-OneVision (Li et al., 2024a) å°†æ¯ä¸€å¸§å¤„ç†ä¸º 196 ä¸ªè§†è§‰ tokenã€‚è‹¥è§†é¢‘ä¸ºä¸¤åˆ†é’Ÿã€60 FPSï¼Œåˆ™æ€» token æ•°é‡è¶…è¿‡ 100 ä¸‡ã€‚ å¦‚æ­¤å¤§é‡çš„ video tokens å¯¼è‡´ï¼š\nåºåˆ—é•¿åº¦æ€¥å‰§å¢åŠ ï¼› Prefill é˜¶æ®µçš„ attention å¼€é”€å‘ˆå¹³æ–¹çº§å¢é•¿ï¼› Decoding é˜¶æ®µ KV cache æ€¥é€Ÿè†¨èƒ€ï¼Œæˆä¸ºæ˜¾è‘—çš„ GPU å†…å­˜ç“¶é¢ˆã€‚ åœ¨ autoregressive ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯æ­¥ç”Ÿæˆçš„ KV cache éƒ½å¿…é¡»ä¸æ¨¡å‹å‚æ•°ä¸€èµ·åŠ è½½ä¸å­˜å‚¨äº GPU æ˜¾å­˜ï¼Œå¯¼è‡´æ˜¾è‘—çš„ memory-bound ç°è±¡ã€‚\n2. Challenge ä¸ºç¼“è§£ video tokens æ•°é‡å¼•å‘çš„è®¡ç®—ä¸å­˜å‚¨çˆ†ç‚¸ï¼Œè¿‘æœŸç ”ç©¶æå‡ºäº†å¤šç§ token pruning ç­–ç•¥ï¼Œé€šè¿‡è¯†åˆ« token å†—ä½™æ€§ã€è®¡ç®—é‡è¦æ€§å·®å¼‚ï¼Œåœ¨ prefill é˜¶æ®µ è¿›è¡Œå‰ªæä»¥å‡å°‘åç»­è§£ç å¼€é”€ã€‚\nç„¶è€Œï¼Œç›´æ¥ç§»é™¤ tokens ä¼šå¸¦æ¥ä¿¡æ¯æŸå¤± â€”â€” è¿™å¯¹è§†é¢‘ç†è§£å°¤å…¶è‡´å‘½ï¼Œå› ä¸ºä¸°å¯Œçš„æ—¶ç©ºçº¿ç´¢å¯¹äºé«˜è´¨é‡ç”Ÿæˆè‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œå•çº¯çš„ pruning æ–¹æ³•åªèƒ½å¸¦æ¥æœ‰é™çš„åŠ é€Ÿï¼Œå› ä¸ºåœ¨æ¯æ­¥ç”Ÿæˆæ—¶ä»éœ€è®¿é—®å®Œæ•´å‚æ•°ã€‚\nSpeculative Decoding as a Solution Speculative decoding (SD) æä¾›äº†ä¸€ä¸ªæ€è·¯ï¼š ä½¿ç”¨ä¸€ä¸ªè½»é‡ draft æ¨¡å‹å…ˆç”Ÿæˆå¤šä¸ªå€™é€‰ tokenï¼Œç„¶åç”± target æ¨¡å‹ï¼ˆverifierï¼‰å¹¶è¡ŒéªŒè¯ã€‚ ç†è®ºä¸Šï¼Œè¿™èƒ½åœ¨ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡çš„æƒ…å†µä¸‹å¤§å¹…æå‡è§£ç é€Ÿåº¦ï¼ˆLeviathan et al., 2023ï¼‰ã€‚\nä½†å°† SD åº”ç”¨äº Vid-LLMs å­˜åœ¨ä¸¤å¤§æŒ‘æˆ˜ï¼š\nDraft æ¨¡å‹ KV cache çš„çº¿æ€§å¢é•¿ï¼šå¯¹äºé•¿è§†é¢‘è¾“å…¥ï¼Œdraft çš„ KV cache ä¼šéšæ—¶é—´è†¨èƒ€ï¼Œä½¿å…¶å»¶è¿Ÿåè€Œæˆä¸ºä¸»è¦ç“¶é¢ˆï¼› è§†é¢‘æ¨¡æ€çš„é«˜å†—ä½™æ€§ä¸ä½å¯†åº¦ä¿¡æ¯åˆ†å¸ƒï¼šç°æœ‰é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡çš„ SD æ–¹æ³•ï¼ˆSun et al., 2024; Chen et al., 2025; Yang et al., 2025aï¼‰éƒ½æ˜¯â€œæ¨¡æ€æ— å…³â€çš„ï¼Œæ— æ³•åˆ©ç”¨è§†é¢‘æ³¨æ„åŠ›åˆ†å¸ƒçš„ç‰¹æ®Šæ€§ï¼Œå› æ­¤åœ¨è§†é¢‘ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ äºæ˜¯è®ºæ–‡æå‡ºï¼š\né€šè¿‡ åœ¨ draft æ¨¡å‹ä¸­å¼•å…¥è§†é¢‘ token å‰Šå‡ï¼ˆvideo token pruningï¼‰ï¼Œå¯æœ‰æ•ˆå‡å°å…¶ KV cache å¤§å°ï¼Œä»è€Œæå‡ speculative decoding æ•ˆç‡ã€‚\n2.1 Naive Speculative Decoding for Vid-LLMs è®¾ï¼š\nç›®æ ‡æ¨¡å‹ï¼ˆverifierï¼‰ï¼š( M_t ) è‰ç¨¿æ¨¡å‹ï¼ˆdraftï¼‰ï¼š( M_d ) ( T_t )ï¼šç›®æ ‡æ¨¡å‹å• token è§£ç æ—¶é—´ ( T_d )ï¼šè‰ç¨¿æ¨¡å‹å• token è§£ç æ—¶é—´ ( T_t^\\gamma )ï¼šç›®æ ‡æ¨¡å‹éªŒè¯ Î³ ä¸ª token çš„æ—¶é—´ åˆ™æ¯æ¬¡ speculative decoding step çš„æ€»æ—¶é—´ä¸ºï¼š\n$$ T_{\\text{step}}^\\gamma = \\gamma \\cdot T_d + T_t^\\gamma $$\nå¹³å‡æ¯ token æ—¶é—´ä¸ºï¼š\n$$ T_{\\text{token}}^\\gamma = \\frac{T_{\\text{step}}^\\gamma}{\\tau} $$\nå…¶ä¸­ (\\tau) ä¸ºå¹³å‡ accept lengthã€‚\nå› æ­¤é€Ÿåº¦æå‡æ¯”ä¸ºï¼š\n$$ \\text{Speedup} = \\frac{T_t}{T_{\\text{token}}^\\gamma} = \\frac{\\tau \\cdot T_t}{\\gamma \\cdot T_d + T_t^\\gamma} = \\frac{\\tau}{\\gamma \\cdot \\frac{T_d}{T_t} + \\frac{T_t^\\gamma}{T_t}} $$\né€šå¸¸æƒ…å†µä¸‹ ( T_t^\\gamma / T_t \\approx 1 )ï¼Œå› æ­¤ é€Ÿåº¦ä¸»è¦ç”±å¹³å‡æ¥å—é•¿åº¦ Ï„ å’Œ latency æ¯” (T_d/T_t) å†³å®šã€‚\nå¯¹äºè§†é¢‘æ¨¡å‹ï¼Œéšç€è¾“å…¥é•¿åº¦å¢åŠ ï¼Œdraft çš„ KV cache è¿…é€Ÿè†¨èƒ€ï¼Œä½¿ (T_d) å¢å¤§ï¼Œä»è€Œå‰Šå¼± speculative decoding çš„åŠ é€Ÿæ•ˆæœã€‚\n2.2 Speculation Sensitivity for Token Pruning ä¸ºäº†é™ä½ draft æ¨¡å‹çš„ KV cacheï¼Œæœ¬ç ”ç©¶å¼•å…¥ è§†é¢‘ token å‰Šå‡ã€‚ ä½†é—®é¢˜åœ¨äºï¼štoken å‡å°‘æ„å‘³ç€è§†è§‰ä¿¡æ¯æŸå¤±ï¼Œæ˜¯å¦ä¼šé™ä½ speculation çš„å‡†ç¡®æ€§ï¼Ÿ\nè®ºæ–‡é€šè¿‡å®éªŒå‘ç°ï¼š\nåœ¨ VideoDetailCaption åŸºå‡†ä¸Šè¿›è¡Œéšæœº token pruningï¼› å½“ pruning ratio â‰¤ 50% æ—¶ï¼Œå¹³å‡æ¥å—é•¿åº¦ Ï„ å‡ ä¹ä¸å˜ï¼› åœ¨æŸäº›æƒ…å†µä¸‹ï¼ˆé€‚åº¦ pruningï¼‰ç”šè‡³æå‡ Ï„ï¼› å½“å®Œå…¨ç§»é™¤æ‰€æœ‰è§†é¢‘ tokenï¼ˆ100% pruningï¼‰æ—¶ï¼ŒÏ„ ä¸æ€»ä½“åŠ é€Ÿæ˜æ˜¾ä¸‹é™ã€‚ è¿™è¯´æ˜ï¼š\nè§†é¢‘è¾“å…¥å­˜åœ¨å¤§é‡å†—ä½™ã€‚é€‚åº¦å‰Šå‡ä¸ä»…ä¸ä¼šæŸå®³ speculative å‡†ç¡®åº¦ï¼Œåè€Œèƒ½å»é™¤å¹²æ‰°æ€§å†—ä½™ï¼Œæå‡æ¨¡å‹ä¸“æ³¨åº¦ã€‚\nç„¶è€Œéšæœºå‰Šå‡ï¼ˆRandom pruningï¼‰åœ¨é«˜æ¯”ç‡æ—¶è¡¨ç°ä¸ç¨³ï¼Œå°¤å…¶å½“å…³é”®å¸§æˆ–å…³é”®ç‰©ä½“è¢«åˆ é™¤æ—¶ä¼šä¸¥é‡æŸå®³æ€§èƒ½ã€‚\n3. Contribution SPECVLM æå‡ºäº†ä¸€ç§ verifier-guidedã€ä¸¤é˜¶æ®µè§†é¢‘ token å‰Šå‡ï¼ˆstaged video token pruningï¼‰ ç­–ç•¥ï¼Œæœ‰æ•ˆå»¶ä¼¸ speculative decoding åœ¨é«˜å‰ªææ¯”ä¾‹ä¸‹çš„åŠ é€Ÿä¼˜åŠ¿ã€‚\næ ¸å¿ƒæ€æƒ³ï¼š\né€šè¿‡ ç›®æ ‡æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼ˆattention guidanceï¼‰ è¯†åˆ«å…³é”®è§†é¢‘ tokenï¼› é«˜æ³¨æ„åŠ›åŒºåŸŸ é‡‡ç”¨ Top-P ä¿ç•™ï¼› ä½æ³¨æ„åŠ›åŒºåŸŸ é‡‡ç”¨ç©ºé—´å‡åŒ€ä¸‹é‡‡æ ·ï¼› å‰Šå‡åçš„ tokens è¢«é€å…¥ draft æ¨¡å‹ï¼Œä»¥æ˜¾è‘—å‡å°å…¶ KV cacheï¼Œä»è€Œæå‡ speculative decoding æ•ˆç‡ã€‚ ä¸»è¦è´¡çŒ®æ€»ç»“ï¼š\né¦–æ¬¡æ¢ç´¢è§†é¢‘ LLM çš„æ— æŸ speculative decoding åŠ é€Ÿã€‚ å‘ç° â€œè§†é¢‘ token çˆ†ç‚¸â€ æ˜¯ draft slowdown çš„æ ¸å¿ƒåŸå› ï¼Œå¹¶æå‡ºé’ˆå¯¹æ€§å‰Šå‡æ–¹æ¡ˆã€‚\nå‘ç° draft æ¨¡å‹å¯¹éšæœºå‰ªæçš„ä¸æ•æ„Ÿæ€§ï¼ˆspeculation insensitivityï¼‰ï¼Œç”±æ­¤æå‡º Verifier-guided staged pruning ç­–ç•¥ï¼Œåœ¨é«˜æ¯”ä¾‹å‰ªæä¸‹ä»ä¿æŒé«˜æ¥å—ç‡ã€‚\nå®éªŒç»“æœï¼š\nå‰ªæ 90% è§†é¢‘ tokens åä»ä¿ç•™çº¦ 90% speculation accuracyï¼› LLaVA-OneVision åŠ é€Ÿ 2.68Ã—ï¼› Qwen2.5-VL åŠ é€Ÿ 2.11Ã—ã€‚ 4. Method 4.1 Attention-Guided Token Importance Estimation SPECVLM åˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„ language-to-video attention æ¥åˆ¤æ–­è§†é¢‘ token çš„é‡è¦æ€§ã€‚\nè®¾ï¼š\n(L)ï¼šè¯­è¨€ token é›†ï¼› (V)ï¼šè§†é¢‘ token é›†ï¼› (G \\in \\mathbb{R}^{|L|\\times|V|})ï¼šè¯­è¨€åˆ°è§†é¢‘çš„æ³¨æ„åŠ›çŸ©é˜µã€‚ å®šä¹‰æ¯ä¸ªè§†é¢‘ token (j) çš„é‡è¦æ€§åˆ†æ•°ä¸ºï¼š\n$$ a_j = \\frac{1}{|L|} \\sum_{i=1}^{|L|} G_{i,j} $$\nå³è¯­è¨€ token å¯¹ç¬¬ j ä¸ªè§†é¢‘ token çš„å¹³å‡æ³¨æ„åŠ›ã€‚ åœ¨å®ç°ä¸­ï¼Œä¼šå¯¹æ‰€æœ‰å±‚ä¸å¤´å–å¹³å‡ï¼Œå½¢æˆæœ€ç»ˆçš„ attention map (A = {a_j})ã€‚\n4.2 Two-Stage Video Token Pruning SPECVLM æå‡º Two-Stage Token Pruningï¼š ï¼ˆ1ï¼‰Top-P Retentionï¼› ï¼ˆ2ï¼‰Spatially Uniform Reductionã€‚\nStage I â€” Top-P Retention è®ºæ–‡å‘ç° video attention åˆ†å¸ƒå‘ˆé•¿å°¾å½¢æ€ï¼šå°‘æ•° token å æ®äº†å¤§éƒ¨åˆ†æ³¨æ„åŠ›ã€‚ å› æ­¤é¦–å…ˆé€‰å–é«˜æ³¨æ„åŠ› tokenã€‚\nå®šä¹‰ç´¯è®¡æ³¨æ„åŠ›é˜ˆå€¼ (\\lambda_r)ï¼Œæ±‚å‡ºæœ€å° c æ»¡è¶³ï¼š\n$$ \\frac{\\sum_{i=1}^{c} a_{(i)}}{\\sum_{j=1}^{|V|} a_j} \\ge \\lambda_r $$\nå…¶ä¸­ (a_{(i)}) ä¸ºç¬¬ i å¤§çš„ attention å€¼ã€‚\nä¿ç•™è¿™ c ä¸ª token æ„æˆé›†åˆ (V_R)ã€‚\n(\\lambda_r) ä¸ pruning ratio (r) é€šè¿‡å°è§„æ¨¡æ ¡å‡†é›†ç¡®å®šï¼Œä¾‹å¦‚åœ¨ LLaVA-OneVision ä¸Šä½¿ç”¨ (\\lambda_r=0.4) å¯¹åº” (r=0.9)ã€‚\nStage II â€” Spatially Uniform Reduction å¯¹äºå‰©ä½™ tokens (V \\setminus V_R)ï¼Œç”±äºæ³¨æ„åŠ›å€¼æ¥è¿‘ä¸”ç©ºé—´ä½ç½®ç›¸è¿‘ï¼Œç›´æ¥åˆ é™¤ä¼šç ´åè§†é¢‘çš„æ—¶ç©ºç»“æ„ã€‚ å› æ­¤è®ºæ–‡è®¾è®¡äº†ç©ºé—´å‡åŒ€é‡‡æ ·ç­–ç•¥ï¼š\nè®¾å‰©ä½™ token æ•°é‡ä¸º (|V| - |V_R|)ï¼Œåˆ™é‡‡æ ·é—´éš”ä¸ºï¼š\n$$ I = \\frac{|V| - |V_R|}{(1-r)|V|} $$\nä»¥æ­¤é—´éš”åœ¨ç©ºé—´ä¸Šï¼ˆå¦‚æ¯å¸§çš„ 14Ã—14 patch gridï¼‰å‡åŒ€é‡‡æ ·ï¼Œå½¢æˆé›†åˆ (V_U)ã€‚\næœ€ç»ˆä¿ç•™é›†åˆï¼š\n$$ Vâ€™ = V_R \\cup V_U $$\nè¿™æ ·æ—¢ä¿ç•™äº†è¯­ä¹‰å…³é”®ä¿¡æ¯ï¼Œåˆç»´æŒäº†ç©ºé—´ç»“æ„çš„å®Œæ•´æ€§ã€‚\n4.3 Why Verifier Guidance Works ç”±äº verifier æ˜¯æœ€ç»ˆç”Ÿæˆåˆ†å¸ƒçš„å‚è€ƒï¼Œå…¶ language-to-video attention èƒ½ç›´æ¥åæ˜ å“ªäº›è§†é¢‘åŒºåŸŸè¢«è¯­è¨€è¾“å‡ºæ‰€ä¾èµ–ã€‚ å› æ­¤ï¼Œè¿™ç§ attention guidance æ˜¯ä¸€ç§è‡ªç„¶çš„â€œimportance estimatorâ€ï¼Œæ¯”å¯å‘å¼å‰ªææ›´å¯é ã€‚\n4.4 Complexity Analysis å‡è®¾åŸå§‹è§†é¢‘ tokens ä¸º (N)ï¼Œprune æ¯”ä¾‹ä¸º (r)ï¼Œåˆ™ draft KV cache å¤§å°ä¸‹é™åˆ° ((1-r)N)ã€‚ prefill ä¸ decode çš„ KV load/store å¼€é”€è¿‘ä¼¼çº¿æ€§å‡å°ã€‚\næ€»ä½“ speculative step latencyï¼š $$ T_{\\text{step}}^\\gamma = \\gamma \\cdot T_d(r) + T_t^\\gamma $$\nç”±äº (T_d(r) \\propto (1-r))ï¼Œå¯å¾—ç†è®ºåŠ é€Ÿï¼š $$ \\text{Speedup} \\approx \\frac{\\tau}{\\gamma(1-r)\\frac{T_d}{T_t}+1} $$\n5. Evaluation 5.1 å®éªŒè®¾ç½® æ¨¡å‹ç³»åˆ—ï¼š\nLLaVA-OneVision (72B / 7B) Qwen2.5-VL (32B / 7B) åœºæ™¯ï¼š\nStandard SD (Std.-SD)ï¼šå¤§æ¨¡å‹ + å° draft æ¨¡å‹ï¼› Self-SDï¼šåŒä¸€æ¨¡å‹è‡ªç”Ÿæˆï¼Œè‡ªå‰ªæã€‚ ä»»åŠ¡åŸºå‡†ï¼š\nVideoDetailCaption (LMMs-Lab, 2024) MVBench, MVLU, LongVideoBench è¾“å…¥è®¾å®šï¼š\né‡‡æ · 64â€“128 å¸§ï¼› æ¯å¸§ 196 tokensï¼› é»˜è®¤å‰ªæç‡ (r=0.9)ã€‚ ç¡¬ä»¶ï¼š\n8Ã—A100 GPUsï¼› Spec length Î³=5ï¼› å¹³å‡ over 50 samplesã€‚ 5.2 Baselines Baseline æè¿° Vanilla æ™®é€š autoregressive è§£ç  SD-Tree EAGLE é£æ ¼æ ‘å½¢ speculation SD-Rand SD + éšæœº token å‰ªæ SD-Window / Frame / DyCoke / FastVID ç©ºé—´/æ—¶é—´å†—ä½™å‰ªæåŸºçº¿ SD-Uniform ç©ºé—´å‡åŒ€æŠ½æ ·ï¼ˆæ—  verifier guidanceï¼‰ 5.3 ä¸»ç»“æœåˆ†æ LLaVA-OneVision (72B-7B, Std.-SD) æ–¹æ³• Ï„ Tokens/s Speedup Vanilla â€“ 2.94 1.0Ã— SD-Tree 3.57 6.41 2.18Ã— SD-Rand (r=0.9) 3.19 7.36 2.50Ã— SPECVLM (r=0.9) 3.48 7.88 2.68Ã— ç»“è®ºï¼š\nå‰ªæ 90% tokens åä»ä¿æŒ 97% çš„ Ï„ï¼› draft KV å‡å°‘ â†’ prefill+decode latency é™ä½ï¼› æ•´ä½“åŠ é€Ÿæ¯”éšæœºå‰ªææ›´é«˜ã€‚ Qwen2.5-VL (32B-7B, Std.-SD) æ–¹æ³• Ï„ Tokens/s Speedup Vanilla â€“ 2.56 1.0Ã— SPECVLM (r=0.9) 3.25 5.40 2.11Ã— è¯´æ˜ SPECVLM å¯¹ä¸åŒä½“ç³»æ¶æ„å…·æœ‰ä¸€è‡´åŠ é€Ÿæ•ˆæœã€‚\nSelf-SD åœºæ™¯ å½“æ²¡æœ‰ç‹¬ç«‹ draft æ¨¡å‹æ—¶ï¼ŒSPECVLM åœ¨ Self-SD ä¸‹ä»å¸¦æ¥çº¦ 1.3Ã— åŠ é€Ÿã€‚ æ­¤æ—¶ pruning ä»…å‡å°‘è‡ªèº« KV cacheï¼Œæ— éœ€é¢å¤–æ¨¡å‹ã€‚\n5.4 Scaling Law for Pruning Ratio è®ºæ–‡åœ¨ Figure 6 å±•ç¤ºäº†éšå‰ªæç‡ r å˜åŒ–çš„ Ï„ æ›²çº¿ï¼š\néšç€ r ä» 0 â†’ 0.9ï¼ŒSPECVLM çš„ Ï„ ä¸‹é™å¹…åº¦è¿œå°äº Random / Uniformï¼› å½“ r è¶…è¿‡ 0.8 æ—¶ï¼ŒSPECVLM ä¾æ—§ä¿æŒç¨³å®šï¼› è¯´æ˜ verifier-guided ç­–ç•¥èƒ½åœ¨æé«˜å‹ç¼©ç‡ä¸‹ä¿ç•™å…³é”®ä¿¡æ¯ã€‚ 5.5 Ablation Study ä»…ä½¿ç”¨ spatial uniformï¼ˆæ—  attention guidanceï¼‰ï¼š Ï„ æ˜æ˜¾ä¸‹é™ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹ï¼Œè¯´æ˜ attention å¼•å¯¼å¯¹é²æ£’æ€§å…³é”®ã€‚\nä»…ä½¿ç”¨ Top-Pï¼ˆæ—  uniform samplingï¼‰ï¼š é«˜æ³¨æ„åŠ›åŒºä¿ç•™ä½†ç©ºé—´ç»“æ„æ–­è£‚ï¼Œé€ æˆè¯­ä¹‰ä¸è¿è´¯ã€‚\nåŒé˜¶æ®µä¿ç•™ï¼ˆSPECVLMï¼‰ï¼š å…¼é¡¾è¯­ä¹‰ä¸ç©ºé—´ä¸€è‡´æ€§ï¼Œåœ¨å„æ¯”ä¾‹ä¸‹è¡¨ç°æœ€ä¼˜ã€‚\n5.6 Latency Breakdown è¡¨ 5 æ˜¾ç¤ºæ—¶é—´åˆ†å¸ƒï¼ˆLLaVA-72B/7Bï¼Œè¾“å‡º 256 tokensï¼‰ï¼š\næ¨¡å— Vanilla SPECVLM Prefill (draft) 24.2s 9.6s Draft decode 28.9s 12.5s Target verify 57.9s 35.2s Total 111s 57s å‰Šå‡ draft KV å¤§å°ç›´æ¥å‡å°‘ draft prefill ä¸ decode å»¶è¿Ÿï¼Œä½¿æ€»æ¨ç†æ—¶é—´å‡åŠã€‚\n5.7 Early-step Accept Length Stability è®ºæ–‡è¿›ä¸€æ­¥ç ”ç©¶ Ï„ åœ¨è§£ç è¿‡ç¨‹çš„åˆ†å¸ƒï¼ˆè¡¨ 3ï¼‰ï¼š\nå‰ 10 æ­¥çš„ Ï„ ä¸æ•´ä½“å¹³å‡ Ï„ å‡ ä¹ä¸€è‡´ï¼› è¯´æ˜ SPECVLM å‰ªæä¸ä¼šå¯¼è‡´æ—©æœŸ speculation å¤±æ•ˆã€‚ 5.8 å¯è§†åŒ–ç»“æœ å·¦å›¾æ˜¾ç¤º SPECVLM ä¿ç•™ token åçš„æ³¨æ„åŠ›çƒ­å›¾ä»ä¸åŸå§‹æ¨¡å‹å¯¹é½è‰¯å¥½ï¼› ä¸­å›¾å±•ç¤ºä¸åŒ r ä¸‹ Ï„ ç¨³å®šï¼› å³å›¾ä¸º speedup ä¸ Ï„ çš„å¹³è¡¡æ›²çº¿ã€‚\n6. Limitation ä¸»è¦é€‚ç”¨äºé•¿è§†é¢‘ã€èµ„æºå—é™åœºæ™¯ï¼š å½“ GPU å¸¦å®½ä¸ºä¸»è¦ç“¶é¢ˆæ—¶æ•ˆæœæ˜¾è‘—ã€‚\néœ€è¦é¢å¤– draft æ¨¡å‹ï¼š\nè™½ç„¶å¼€é”€ç›¸å¯¹è¾ƒå°ï¼Œä»éœ€é€‰æ‹©åˆé€‚è‰ç¨¿æ¨¡å‹ã€‚\nTraining-free è®¾è®¡é™åˆ¶æœ€å¤§åŠ é€Ÿï¼š è‹¥æœªæ¥èƒ½è®­ç»ƒæ›´è½»é‡åŒ– Vid-LLM draftï¼Œå¯è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚ 7. Conclusion We propose SPECVLM, the first training-free speculative decoding framework tailored for accelerating video LLMs. Building on the low speculation sensitivity to token pruning, SPECVLM leverages verifier-guided attention to remove redundant video tokens, significantly reducing the draft modelâ€™s KV cache without compromising generation quality.\nSPECVLM achieves:\n2.68Ã— speedup on LLaVA-OneVision-72B 2.11Ã— speedup on Qwen2.5-VL-32B It provides a general, plug-and-play, training-free acceleration framework for long video reasoning.\n",
  "wordCount" : "956",
  "inLanguage": "zh-cn",
  "datePublished": "2025-10-29T00:00:00Z",
  "dateModified": "2025-10-29T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JJ's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jjl357.github.io/blog/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
  <nav class="nav">
    <div class="logo">
      
      <a href="https://jjl357.github.io/" accesskey="h" title="ğŸ¥› â˜• ğŸµ (Alt + H)">
        <span>ğŸ¥› â˜• ğŸµ</span>
        
      </a>

      <div class="logo-switches">
        <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
          <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      </div>
    </div>
    <ul id="menu">
    </ul>
  </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning
    </h1>
    <div class="post-meta"><span title='2025-10-29 00:00:00 +0000 UTC'>October 29, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/title.png"></p>
<p><strong>Conference:</strong> <strong>EMNLP'25</strong>
<strong>Github:</strong> <a href="https://github.com/zju-jiyicheng/SpecVLM">https://github.com/zju-jiyicheng/SpecVLM</a></p>
<hr>
<h2 id="1-motivation">1. Motivation<a hidden class="anchor" aria-hidden="true" href="#1-motivation">#</a></h2>
<p>Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.</p>
<p>ä¾‹å¦‚ï¼š
LLaVA-OneVision (Li et al., 2024a) å°†æ¯ä¸€å¸§å¤„ç†ä¸º 196 ä¸ªè§†è§‰ tokenã€‚è‹¥è§†é¢‘ä¸ºä¸¤åˆ†é’Ÿã€60 FPSï¼Œåˆ™æ€» token æ•°é‡è¶…è¿‡ 100 ä¸‡ã€‚
å¦‚æ­¤å¤§é‡çš„ video tokens å¯¼è‡´ï¼š</p>
<ul>
<li>åºåˆ—é•¿åº¦æ€¥å‰§å¢åŠ ï¼›</li>
<li>Prefill é˜¶æ®µçš„ attention å¼€é”€å‘ˆå¹³æ–¹çº§å¢é•¿ï¼›</li>
<li>Decoding é˜¶æ®µ KV cache æ€¥é€Ÿè†¨èƒ€ï¼Œæˆä¸ºæ˜¾è‘—çš„ GPU å†…å­˜ç“¶é¢ˆã€‚</li>
</ul>
<p>åœ¨ autoregressive ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯æ­¥ç”Ÿæˆçš„ KV cache éƒ½å¿…é¡»ä¸æ¨¡å‹å‚æ•°ä¸€èµ·åŠ è½½ä¸å­˜å‚¨äº GPU æ˜¾å­˜ï¼Œå¯¼è‡´æ˜¾è‘—çš„ memory-bound ç°è±¡ã€‚</p>
<hr>
<h2 id="2-challenge">2. Challenge<a hidden class="anchor" aria-hidden="true" href="#2-challenge">#</a></h2>
<p>ä¸ºç¼“è§£ video tokens æ•°é‡å¼•å‘çš„è®¡ç®—ä¸å­˜å‚¨çˆ†ç‚¸ï¼Œè¿‘æœŸç ”ç©¶æå‡ºäº†å¤šç§ token pruning ç­–ç•¥ï¼Œé€šè¿‡è¯†åˆ« token å†—ä½™æ€§ã€è®¡ç®—é‡è¦æ€§å·®å¼‚ï¼Œåœ¨ <strong>prefill é˜¶æ®µ</strong> è¿›è¡Œå‰ªæä»¥å‡å°‘åç»­è§£ç å¼€é”€ã€‚</p>
<p>ç„¶è€Œï¼Œ<strong>ç›´æ¥ç§»é™¤ tokens ä¼šå¸¦æ¥ä¿¡æ¯æŸå¤±</strong> â€”â€” è¿™å¯¹è§†é¢‘ç†è§£å°¤å…¶è‡´å‘½ï¼Œå› ä¸ºä¸°å¯Œçš„æ—¶ç©ºçº¿ç´¢å¯¹äºé«˜è´¨é‡ç”Ÿæˆè‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œå•çº¯çš„ pruning æ–¹æ³•åªèƒ½å¸¦æ¥æœ‰é™çš„åŠ é€Ÿï¼Œå› ä¸ºåœ¨æ¯æ­¥ç”Ÿæˆæ—¶ä»éœ€è®¿é—®å®Œæ•´å‚æ•°ã€‚</p>
<hr>
<h3 id="speculative-decoding-as-a-solution">Speculative Decoding as a Solution<a hidden class="anchor" aria-hidden="true" href="#speculative-decoding-as-a-solution">#</a></h3>
<p>Speculative decoding (SD) æä¾›äº†ä¸€ä¸ªæ€è·¯ï¼š
ä½¿ç”¨ä¸€ä¸ªè½»é‡ draft æ¨¡å‹å…ˆç”Ÿæˆå¤šä¸ªå€™é€‰ tokenï¼Œç„¶åç”± target æ¨¡å‹ï¼ˆverifierï¼‰å¹¶è¡ŒéªŒè¯ã€‚
ç†è®ºä¸Šï¼Œè¿™èƒ½åœ¨ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡çš„æƒ…å†µä¸‹å¤§å¹…æå‡è§£ç é€Ÿåº¦ï¼ˆLeviathan et al., 2023ï¼‰ã€‚</p>
<p>ä½†å°† SD åº”ç”¨äº Vid-LLMs å­˜åœ¨ä¸¤å¤§æŒ‘æˆ˜ï¼š</p>
<ol>
<li><strong>Draft æ¨¡å‹ KV cache çš„çº¿æ€§å¢é•¿</strong>ï¼šå¯¹äºé•¿è§†é¢‘è¾“å…¥ï¼Œdraft çš„ KV cache ä¼šéšæ—¶é—´è†¨èƒ€ï¼Œä½¿å…¶å»¶è¿Ÿåè€Œæˆä¸ºä¸»è¦ç“¶é¢ˆï¼›</li>
<li><strong>è§†é¢‘æ¨¡æ€çš„é«˜å†—ä½™æ€§ä¸ä½å¯†åº¦ä¿¡æ¯åˆ†å¸ƒ</strong>ï¼šç°æœ‰é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡çš„ SD æ–¹æ³•ï¼ˆSun et al., 2024; Chen et al., 2025; Yang et al., 2025aï¼‰éƒ½æ˜¯â€œæ¨¡æ€æ— å…³â€çš„ï¼Œæ— æ³•åˆ©ç”¨è§†é¢‘æ³¨æ„åŠ›åˆ†å¸ƒçš„ç‰¹æ®Šæ€§ï¼Œå› æ­¤åœ¨è§†é¢‘ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
</ol>
<p>äºæ˜¯è®ºæ–‡æå‡ºï¼š</p>
<blockquote>
<p>é€šè¿‡ <strong>åœ¨ draft æ¨¡å‹ä¸­å¼•å…¥è§†é¢‘ token å‰Šå‡ï¼ˆvideo token pruningï¼‰</strong>ï¼Œå¯æœ‰æ•ˆå‡å°å…¶ KV cache å¤§å°ï¼Œä»è€Œæå‡ speculative decoding æ•ˆç‡ã€‚</p>
</blockquote>
<hr>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/figure2.png"></p>
<hr>
<h2 id="21-naive-speculative-decoding-for-vid-llms">2.1 Naive Speculative Decoding for Vid-LLMs<a hidden class="anchor" aria-hidden="true" href="#21-naive-speculative-decoding-for-vid-llms">#</a></h2>
<p>è®¾ï¼š</p>
<ul>
<li>ç›®æ ‡æ¨¡å‹ï¼ˆverifierï¼‰ï¼š( M_t )</li>
<li>è‰ç¨¿æ¨¡å‹ï¼ˆdraftï¼‰ï¼š( M_d )</li>
<li>( T_t )ï¼šç›®æ ‡æ¨¡å‹å• token è§£ç æ—¶é—´</li>
<li>( T_d )ï¼šè‰ç¨¿æ¨¡å‹å• token è§£ç æ—¶é—´</li>
<li>( T_t^\gamma )ï¼šç›®æ ‡æ¨¡å‹éªŒè¯ Î³ ä¸ª token çš„æ—¶é—´</li>
</ul>
<p>åˆ™æ¯æ¬¡ speculative decoding step çš„æ€»æ—¶é—´ä¸ºï¼š</p>
<p>$$
T_{\text{step}}^\gamma = \gamma \cdot T_d + T_t^\gamma
$$</p>
<p>å¹³å‡æ¯ token æ—¶é—´ä¸ºï¼š</p>
<p>$$
T_{\text{token}}^\gamma = \frac{T_{\text{step}}^\gamma}{\tau}
$$</p>
<p>å…¶ä¸­ (\tau) ä¸ºå¹³å‡ accept lengthã€‚</p>
<p>å› æ­¤é€Ÿåº¦æå‡æ¯”ä¸ºï¼š</p>
<p>$$
\text{Speedup} = \frac{T_t}{T_{\text{token}}^\gamma} = \frac{\tau \cdot T_t}{\gamma \cdot T_d + T_t^\gamma} = \frac{\tau}{\gamma \cdot \frac{T_d}{T_t} + \frac{T_t^\gamma}{T_t}}
$$</p>
<p>é€šå¸¸æƒ…å†µä¸‹ ( T_t^\gamma / T_t \approx 1 )ï¼Œå› æ­¤ <strong>é€Ÿåº¦ä¸»è¦ç”±å¹³å‡æ¥å—é•¿åº¦ Ï„ å’Œ latency æ¯” (T_d/T_t) å†³å®š</strong>ã€‚</p>
<p>å¯¹äºè§†é¢‘æ¨¡å‹ï¼Œéšç€è¾“å…¥é•¿åº¦å¢åŠ ï¼Œdraft çš„ KV cache è¿…é€Ÿè†¨èƒ€ï¼Œä½¿ (T_d) å¢å¤§ï¼Œä»è€Œå‰Šå¼± speculative decoding çš„åŠ é€Ÿæ•ˆæœã€‚</p>
<hr>
<h3 id="22-speculation-sensitivity-for-token-pruning">2.2 Speculation Sensitivity for Token Pruning<a hidden class="anchor" aria-hidden="true" href="#22-speculation-sensitivity-for-token-pruning">#</a></h3>
<p>ä¸ºäº†é™ä½ draft æ¨¡å‹çš„ KV cacheï¼Œæœ¬ç ”ç©¶å¼•å…¥ <strong>è§†é¢‘ token å‰Šå‡</strong>ã€‚
ä½†é—®é¢˜åœ¨äºï¼štoken å‡å°‘æ„å‘³ç€è§†è§‰ä¿¡æ¯æŸå¤±ï¼Œæ˜¯å¦ä¼šé™ä½ speculation çš„å‡†ç¡®æ€§ï¼Ÿ</p>
<p>è®ºæ–‡é€šè¿‡å®éªŒå‘ç°ï¼š</p>
<ul>
<li>åœ¨ VideoDetailCaption åŸºå‡†ä¸Šè¿›è¡Œéšæœº token pruningï¼›</li>
<li>å½“ pruning ratio â‰¤ 50% æ—¶ï¼Œå¹³å‡æ¥å—é•¿åº¦ Ï„ å‡ ä¹ä¸å˜ï¼›</li>
<li>åœ¨æŸäº›æƒ…å†µä¸‹ï¼ˆé€‚åº¦ pruningï¼‰ç”šè‡³æå‡ Ï„ï¼›</li>
<li>å½“å®Œå…¨ç§»é™¤æ‰€æœ‰è§†é¢‘ tokenï¼ˆ100% pruningï¼‰æ—¶ï¼ŒÏ„ ä¸æ€»ä½“åŠ é€Ÿæ˜æ˜¾ä¸‹é™ã€‚</li>
</ul>
<p>è¿™è¯´æ˜ï¼š</p>
<blockquote>
<p>è§†é¢‘è¾“å…¥å­˜åœ¨å¤§é‡å†—ä½™ã€‚é€‚åº¦å‰Šå‡ä¸ä»…ä¸ä¼šæŸå®³ speculative å‡†ç¡®åº¦ï¼Œåè€Œèƒ½å»é™¤å¹²æ‰°æ€§å†—ä½™ï¼Œæå‡æ¨¡å‹ä¸“æ³¨åº¦ã€‚</p>
</blockquote>
<p>ç„¶è€Œéšæœºå‰Šå‡ï¼ˆRandom pruningï¼‰åœ¨é«˜æ¯”ç‡æ—¶è¡¨ç°ä¸ç¨³ï¼Œå°¤å…¶å½“å…³é”®å¸§æˆ–å…³é”®ç‰©ä½“è¢«åˆ é™¤æ—¶ä¼šä¸¥é‡æŸå®³æ€§èƒ½ã€‚</p>
<hr>
<h2 id="3-contribution">3. Contribution<a hidden class="anchor" aria-hidden="true" href="#3-contribution">#</a></h2>
<p>SPECVLM æå‡ºäº†ä¸€ç§ <strong>verifier-guidedã€ä¸¤é˜¶æ®µè§†é¢‘ token å‰Šå‡ï¼ˆstaged video token pruningï¼‰</strong> ç­–ç•¥ï¼Œæœ‰æ•ˆå»¶ä¼¸ speculative decoding åœ¨é«˜å‰ªææ¯”ä¾‹ä¸‹çš„åŠ é€Ÿä¼˜åŠ¿ã€‚</p>
<p>æ ¸å¿ƒæ€æƒ³ï¼š</p>
<ul>
<li>é€šè¿‡ <strong>ç›®æ ‡æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼ˆattention guidanceï¼‰</strong> è¯†åˆ«å…³é”®è§†é¢‘ tokenï¼›</li>
<li><strong>é«˜æ³¨æ„åŠ›åŒºåŸŸ</strong> é‡‡ç”¨ Top-P ä¿ç•™ï¼›</li>
<li><strong>ä½æ³¨æ„åŠ›åŒºåŸŸ</strong> é‡‡ç”¨ç©ºé—´å‡åŒ€ä¸‹é‡‡æ ·ï¼›</li>
<li>å‰Šå‡åçš„ tokens è¢«é€å…¥ draft æ¨¡å‹ï¼Œä»¥æ˜¾è‘—å‡å°å…¶ KV cacheï¼Œä»è€Œæå‡ speculative decoding æ•ˆç‡ã€‚</li>
</ul>
<p><strong>ä¸»è¦è´¡çŒ®æ€»ç»“ï¼š</strong></p>
<ol>
<li>
<p><strong>é¦–æ¬¡æ¢ç´¢è§†é¢‘ LLM çš„æ— æŸ speculative decoding åŠ é€Ÿã€‚</strong>
å‘ç° â€œè§†é¢‘ token çˆ†ç‚¸â€ æ˜¯ draft slowdown çš„æ ¸å¿ƒåŸå› ï¼Œå¹¶æå‡ºé’ˆå¯¹æ€§å‰Šå‡æ–¹æ¡ˆã€‚</p>
</li>
<li>
<p><strong>å‘ç° draft æ¨¡å‹å¯¹éšæœºå‰ªæçš„ä¸æ•æ„Ÿæ€§ï¼ˆspeculation insensitivityï¼‰</strong>ï¼Œç”±æ­¤æå‡º <strong>Verifier-guided staged pruning</strong> ç­–ç•¥ï¼Œåœ¨é«˜æ¯”ä¾‹å‰ªæä¸‹ä»ä¿æŒé«˜æ¥å—ç‡ã€‚</p>
</li>
<li>
<p><strong>å®éªŒç»“æœï¼š</strong></p>
<ul>
<li>å‰ªæ 90% è§†é¢‘ tokens åä»ä¿ç•™çº¦ 90% speculation accuracyï¼›</li>
<li>LLaVA-OneVision åŠ é€Ÿ 2.68Ã—ï¼›</li>
<li>Qwen2.5-VL åŠ é€Ÿ 2.11Ã—ã€‚</li>
</ul>
</li>
</ol>
<hr>
<h2 id="4-method">4. Method<a hidden class="anchor" aria-hidden="true" href="#4-method">#</a></h2>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/figure3.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/figure4.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/figure5.png"></p>
<hr>
<h3 id="41-attention-guided-token-importance-estimation">4.1 Attention-Guided Token Importance Estimation<a hidden class="anchor" aria-hidden="true" href="#41-attention-guided-token-importance-estimation">#</a></h3>
<p>SPECVLM åˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„ <strong>language-to-video attention</strong> æ¥åˆ¤æ–­è§†é¢‘ token çš„é‡è¦æ€§ã€‚</p>
<p>è®¾ï¼š</p>
<ul>
<li>(L)ï¼šè¯­è¨€ token é›†ï¼›</li>
<li>(V)ï¼šè§†é¢‘ token é›†ï¼›</li>
<li>(G \in \mathbb{R}^{|L|\times|V|})ï¼šè¯­è¨€åˆ°è§†é¢‘çš„æ³¨æ„åŠ›çŸ©é˜µã€‚</li>
</ul>
<p>å®šä¹‰æ¯ä¸ªè§†é¢‘ token (j) çš„é‡è¦æ€§åˆ†æ•°ä¸ºï¼š</p>
<p>$$
a_j = \frac{1}{|L|} \sum_{i=1}^{|L|} G_{i,j}
$$</p>
<p>å³è¯­è¨€ token å¯¹ç¬¬ j ä¸ªè§†é¢‘ token çš„å¹³å‡æ³¨æ„åŠ›ã€‚
åœ¨å®ç°ä¸­ï¼Œä¼šå¯¹æ‰€æœ‰å±‚ä¸å¤´å–å¹³å‡ï¼Œå½¢æˆæœ€ç»ˆçš„ attention map (A = {a_j})ã€‚</p>
<hr>
<h3 id="42-two-stage-video-token-pruning">4.2 Two-Stage Video Token Pruning<a hidden class="anchor" aria-hidden="true" href="#42-two-stage-video-token-pruning">#</a></h3>
<p>SPECVLM æå‡º <strong>Two-Stage Token Pruning</strong>ï¼š
ï¼ˆ1ï¼‰Top-P Retentionï¼›
ï¼ˆ2ï¼‰Spatially Uniform Reductionã€‚</p>
<hr>
<h4 id="stage-i--top-p-retention"><strong>Stage I â€” Top-P Retention</strong><a hidden class="anchor" aria-hidden="true" href="#stage-i--top-p-retention">#</a></h4>
<p>è®ºæ–‡å‘ç° video attention åˆ†å¸ƒå‘ˆé•¿å°¾å½¢æ€ï¼šå°‘æ•° token å æ®äº†å¤§éƒ¨åˆ†æ³¨æ„åŠ›ã€‚
å› æ­¤é¦–å…ˆé€‰å–é«˜æ³¨æ„åŠ› tokenã€‚</p>
<p>å®šä¹‰ç´¯è®¡æ³¨æ„åŠ›é˜ˆå€¼ (\lambda_r)ï¼Œæ±‚å‡ºæœ€å° c æ»¡è¶³ï¼š</p>
<p>$$
\frac{\sum_{i=1}^{c} a_{(i)}}{\sum_{j=1}^{|V|} a_j} \ge \lambda_r
$$</p>
<p>å…¶ä¸­ (a_{(i)}) ä¸ºç¬¬ i å¤§çš„ attention å€¼ã€‚</p>
<p>ä¿ç•™è¿™ c ä¸ª token æ„æˆé›†åˆ (V_R)ã€‚</p>
<p>(\lambda_r) ä¸ pruning ratio (r) é€šè¿‡å°è§„æ¨¡æ ¡å‡†é›†ç¡®å®šï¼Œä¾‹å¦‚åœ¨ LLaVA-OneVision ä¸Šä½¿ç”¨ (\lambda_r=0.4) å¯¹åº” (r=0.9)ã€‚</p>
<hr>
<h4 id="stage-ii--spatially-uniform-reduction"><strong>Stage II â€” Spatially Uniform Reduction</strong><a hidden class="anchor" aria-hidden="true" href="#stage-ii--spatially-uniform-reduction">#</a></h4>
<p>å¯¹äºå‰©ä½™ tokens (V \setminus V_R)ï¼Œç”±äºæ³¨æ„åŠ›å€¼æ¥è¿‘ä¸”ç©ºé—´ä½ç½®ç›¸è¿‘ï¼Œç›´æ¥åˆ é™¤ä¼šç ´åè§†é¢‘çš„æ—¶ç©ºç»“æ„ã€‚
å› æ­¤è®ºæ–‡è®¾è®¡äº†ç©ºé—´å‡åŒ€é‡‡æ ·ç­–ç•¥ï¼š</p>
<p>è®¾å‰©ä½™ token æ•°é‡ä¸º (|V| - |V_R|)ï¼Œåˆ™é‡‡æ ·é—´éš”ä¸ºï¼š</p>
<p>$$
I = \frac{|V| - |V_R|}{(1-r)|V|}
$$</p>
<p>ä»¥æ­¤é—´éš”åœ¨ç©ºé—´ä¸Šï¼ˆå¦‚æ¯å¸§çš„ 14Ã—14 patch gridï¼‰å‡åŒ€é‡‡æ ·ï¼Œå½¢æˆé›†åˆ (V_U)ã€‚</p>
<p>æœ€ç»ˆä¿ç•™é›†åˆï¼š</p>
<p>$$
V&rsquo; = V_R \cup V_U
$$</p>
<p>è¿™æ ·æ—¢ä¿ç•™äº†è¯­ä¹‰å…³é”®ä¿¡æ¯ï¼Œåˆç»´æŒäº†ç©ºé—´ç»“æ„çš„å®Œæ•´æ€§ã€‚</p>
<hr>
<h3 id="43-why-verifier-guidance-works">4.3 Why Verifier Guidance Works<a hidden class="anchor" aria-hidden="true" href="#43-why-verifier-guidance-works">#</a></h3>
<p>ç”±äº verifier æ˜¯æœ€ç»ˆç”Ÿæˆåˆ†å¸ƒçš„å‚è€ƒï¼Œå…¶ language-to-video attention èƒ½ç›´æ¥åæ˜ å“ªäº›è§†é¢‘åŒºåŸŸè¢«è¯­è¨€è¾“å‡ºæ‰€ä¾èµ–ã€‚
å› æ­¤ï¼Œè¿™ç§ attention guidance æ˜¯ä¸€ç§è‡ªç„¶çš„â€œimportance estimatorâ€ï¼Œæ¯”å¯å‘å¼å‰ªææ›´å¯é ã€‚</p>
<hr>
<h3 id="44-complexity-analysis">4.4 Complexity Analysis<a hidden class="anchor" aria-hidden="true" href="#44-complexity-analysis">#</a></h3>
<p>å‡è®¾åŸå§‹è§†é¢‘ tokens ä¸º (N)ï¼Œprune æ¯”ä¾‹ä¸º (r)ï¼Œåˆ™ draft KV cache å¤§å°ä¸‹é™åˆ° ((1-r)N)ã€‚
prefill ä¸ decode çš„ KV load/store å¼€é”€è¿‘ä¼¼çº¿æ€§å‡å°ã€‚</p>
<p>æ€»ä½“ speculative step latencyï¼š
$$
T_{\text{step}}^\gamma = \gamma \cdot T_d(r) + T_t^\gamma
$$</p>
<p>ç”±äº (T_d(r) \propto (1-r))ï¼Œå¯å¾—ç†è®ºåŠ é€Ÿï¼š
$$
\text{Speedup} \approx \frac{\tau}{\gamma(1-r)\frac{T_d}{T_t}+1}
$$</p>
<hr>
<h2 id="5-evaluation">5. Evaluation<a hidden class="anchor" aria-hidden="true" href="#5-evaluation">#</a></h2>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/table12.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/table3.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/figure678.png"></p>
<hr>
<h3 id="51-å®éªŒè®¾ç½®">5.1 å®éªŒè®¾ç½®<a hidden class="anchor" aria-hidden="true" href="#51-å®éªŒè®¾ç½®">#</a></h3>
<ul>
<li>
<p><strong>æ¨¡å‹ç³»åˆ—</strong>ï¼š</p>
<ul>
<li>LLaVA-OneVision (72B / 7B)</li>
<li>Qwen2.5-VL (32B / 7B)</li>
</ul>
</li>
<li>
<p><strong>åœºæ™¯</strong>ï¼š</p>
<ul>
<li>Standard SD (Std.-SD)ï¼šå¤§æ¨¡å‹ + å° draft æ¨¡å‹ï¼›</li>
<li>Self-SDï¼šåŒä¸€æ¨¡å‹è‡ªç”Ÿæˆï¼Œè‡ªå‰ªæã€‚</li>
</ul>
</li>
<li>
<p><strong>ä»»åŠ¡åŸºå‡†</strong>ï¼š</p>
<ul>
<li>VideoDetailCaption (LMMs-Lab, 2024)</li>
<li>MVBench, MVLU, LongVideoBench</li>
</ul>
</li>
<li>
<p><strong>è¾“å…¥è®¾å®š</strong>ï¼š</p>
<ul>
<li>é‡‡æ · 64â€“128 å¸§ï¼›</li>
<li>æ¯å¸§ 196 tokensï¼›</li>
<li>é»˜è®¤å‰ªæç‡ (r=0.9)ã€‚</li>
</ul>
</li>
<li>
<p><strong>ç¡¬ä»¶</strong>ï¼š</p>
<ul>
<li>8Ã—A100 GPUsï¼›</li>
<li>Spec length Î³=5ï¼›</li>
<li>å¹³å‡ over 50 samplesã€‚</li>
</ul>
</li>
</ul>
<hr>
<h3 id="52-baselines">5.2 Baselines<a hidden class="anchor" aria-hidden="true" href="#52-baselines">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Baseline</th>
          <th>æè¿°</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Vanilla</strong></td>
          <td>æ™®é€š autoregressive è§£ç </td>
      </tr>
      <tr>
          <td><strong>SD-Tree</strong></td>
          <td>EAGLE é£æ ¼æ ‘å½¢ speculation</td>
      </tr>
      <tr>
          <td><strong>SD-Rand</strong></td>
          <td>SD + éšæœº token å‰ªæ</td>
      </tr>
      <tr>
          <td><strong>SD-Window / Frame / DyCoke / FastVID</strong></td>
          <td>ç©ºé—´/æ—¶é—´å†—ä½™å‰ªæåŸºçº¿</td>
      </tr>
      <tr>
          <td><strong>SD-Uniform</strong></td>
          <td>ç©ºé—´å‡åŒ€æŠ½æ ·ï¼ˆæ—  verifier guidanceï¼‰</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="53-ä¸»ç»“æœåˆ†æ">5.3 ä¸»ç»“æœåˆ†æ<a hidden class="anchor" aria-hidden="true" href="#53-ä¸»ç»“æœåˆ†æ">#</a></h3>
<h4 id="llava-onevision-72b-7b-std-sd"><strong>LLaVA-OneVision (72B-7B, Std.-SD)</strong><a hidden class="anchor" aria-hidden="true" href="#llava-onevision-72b-7b-std-sd">#</a></h4>
<table>
  <thead>
      <tr>
          <th>æ–¹æ³•</th>
          <th>Ï„</th>
          <th>Tokens/s</th>
          <th>Speedup</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Vanilla</td>
          <td>â€“</td>
          <td>2.94</td>
          <td>1.0Ã—</td>
      </tr>
      <tr>
          <td>SD-Tree</td>
          <td>3.57</td>
          <td>6.41</td>
          <td>2.18Ã—</td>
      </tr>
      <tr>
          <td>SD-Rand (r=0.9)</td>
          <td>3.19</td>
          <td>7.36</td>
          <td>2.50Ã—</td>
      </tr>
      <tr>
          <td><strong>SPECVLM (r=0.9)</strong></td>
          <td><strong>3.48</strong></td>
          <td><strong>7.88</strong></td>
          <td><strong>2.68Ã—</strong></td>
      </tr>
  </tbody>
</table>
<p>ç»“è®ºï¼š</p>
<ul>
<li>å‰ªæ 90% tokens åä»ä¿æŒ 97% çš„ Ï„ï¼›</li>
<li>draft KV å‡å°‘ â†’ prefill+decode latency é™ä½ï¼›</li>
<li>æ•´ä½“åŠ é€Ÿæ¯”éšæœºå‰ªææ›´é«˜ã€‚</li>
</ul>
<hr>
<h4 id="qwen25-vl-32b-7b-std-sd"><strong>Qwen2.5-VL (32B-7B, Std.-SD)</strong><a hidden class="anchor" aria-hidden="true" href="#qwen25-vl-32b-7b-std-sd">#</a></h4>
<table>
  <thead>
      <tr>
          <th>æ–¹æ³•</th>
          <th>Ï„</th>
          <th>Tokens/s</th>
          <th>Speedup</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Vanilla</td>
          <td>â€“</td>
          <td>2.56</td>
          <td>1.0Ã—</td>
      </tr>
      <tr>
          <td><strong>SPECVLM (r=0.9)</strong></td>
          <td><strong>3.25</strong></td>
          <td><strong>5.40</strong></td>
          <td><strong>2.11Ã—</strong></td>
      </tr>
  </tbody>
</table>
<p>è¯´æ˜ SPECVLM å¯¹ä¸åŒä½“ç³»æ¶æ„å…·æœ‰ä¸€è‡´åŠ é€Ÿæ•ˆæœã€‚</p>
<hr>
<h4 id="self-sd-åœºæ™¯"><strong>Self-SD åœºæ™¯</strong><a hidden class="anchor" aria-hidden="true" href="#self-sd-åœºæ™¯">#</a></h4>
<p>å½“æ²¡æœ‰ç‹¬ç«‹ draft æ¨¡å‹æ—¶ï¼ŒSPECVLM åœ¨ Self-SD ä¸‹ä»å¸¦æ¥çº¦ 1.3Ã— åŠ é€Ÿã€‚
æ­¤æ—¶ pruning ä»…å‡å°‘è‡ªèº« KV cacheï¼Œæ— éœ€é¢å¤–æ¨¡å‹ã€‚</p>
<hr>
<h3 id="54-scaling-law-for-pruning-ratio">5.4 Scaling Law for Pruning Ratio<a hidden class="anchor" aria-hidden="true" href="#54-scaling-law-for-pruning-ratio">#</a></h3>
<p>è®ºæ–‡åœ¨ Figure 6 å±•ç¤ºäº†éšå‰ªæç‡ r å˜åŒ–çš„ Ï„ æ›²çº¿ï¼š</p>
<ul>
<li>éšç€ r ä» 0 â†’ 0.9ï¼ŒSPECVLM çš„ Ï„ ä¸‹é™å¹…åº¦è¿œå°äº Random / Uniformï¼›</li>
<li>å½“ r è¶…è¿‡ 0.8 æ—¶ï¼ŒSPECVLM ä¾æ—§ä¿æŒç¨³å®šï¼›</li>
<li>è¯´æ˜ verifier-guided ç­–ç•¥èƒ½åœ¨æé«˜å‹ç¼©ç‡ä¸‹ä¿ç•™å…³é”®ä¿¡æ¯ã€‚</li>
</ul>
<hr>
<h3 id="55-ablation-study">5.5 Ablation Study<a hidden class="anchor" aria-hidden="true" href="#55-ablation-study">#</a></h3>
<ol>
<li>
<p><strong>ä»…ä½¿ç”¨ spatial uniformï¼ˆæ—  attention guidanceï¼‰</strong>ï¼š
Ï„ æ˜æ˜¾ä¸‹é™ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹ï¼Œè¯´æ˜ attention å¼•å¯¼å¯¹é²æ£’æ€§å…³é”®ã€‚</p>
</li>
<li>
<p><strong>ä»…ä½¿ç”¨ Top-Pï¼ˆæ—  uniform samplingï¼‰</strong>ï¼š
é«˜æ³¨æ„åŠ›åŒºä¿ç•™ä½†ç©ºé—´ç»“æ„æ–­è£‚ï¼Œé€ æˆè¯­ä¹‰ä¸è¿è´¯ã€‚</p>
</li>
<li>
<p><strong>åŒé˜¶æ®µä¿ç•™ï¼ˆSPECVLMï¼‰</strong>ï¼š
å…¼é¡¾è¯­ä¹‰ä¸ç©ºé—´ä¸€è‡´æ€§ï¼Œåœ¨å„æ¯”ä¾‹ä¸‹è¡¨ç°æœ€ä¼˜ã€‚</p>
</li>
</ol>
<hr>
<h3 id="56-latency-breakdown">5.6 Latency Breakdown<a hidden class="anchor" aria-hidden="true" href="#56-latency-breakdown">#</a></h3>
<p>è¡¨ 5 æ˜¾ç¤ºæ—¶é—´åˆ†å¸ƒï¼ˆLLaVA-72B/7Bï¼Œè¾“å‡º 256 tokensï¼‰ï¼š</p>
<table>
  <thead>
      <tr>
          <th>æ¨¡å—</th>
          <th>Vanilla</th>
          <th>SPECVLM</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Prefill (draft)</td>
          <td>24.2s</td>
          <td>9.6s</td>
      </tr>
      <tr>
          <td>Draft decode</td>
          <td>28.9s</td>
          <td>12.5s</td>
      </tr>
      <tr>
          <td>Target verify</td>
          <td>57.9s</td>
          <td>35.2s</td>
      </tr>
      <tr>
          <td><strong>Total</strong></td>
          <td><strong>111s</strong></td>
          <td><strong>57s</strong></td>
      </tr>
  </tbody>
</table>
<p>å‰Šå‡ draft KV å¤§å°ç›´æ¥å‡å°‘ draft prefill ä¸ decode å»¶è¿Ÿï¼Œä½¿æ€»æ¨ç†æ—¶é—´å‡åŠã€‚</p>
<hr>
<h3 id="57-early-step-accept-length-stability">5.7 Early-step Accept Length Stability<a hidden class="anchor" aria-hidden="true" href="#57-early-step-accept-length-stability">#</a></h3>
<p>è®ºæ–‡è¿›ä¸€æ­¥ç ”ç©¶ Ï„ åœ¨è§£ç è¿‡ç¨‹çš„åˆ†å¸ƒï¼ˆè¡¨ 3ï¼‰ï¼š</p>
<ul>
<li>å‰ 10 æ­¥çš„ Ï„ ä¸æ•´ä½“å¹³å‡ Ï„ å‡ ä¹ä¸€è‡´ï¼›</li>
<li>è¯´æ˜ SPECVLM å‰ªæä¸ä¼šå¯¼è‡´æ—©æœŸ speculation å¤±æ•ˆã€‚</li>
</ul>
<hr>
<h3 id="58-å¯è§†åŒ–ç»“æœ">5.8 å¯è§†åŒ–ç»“æœ<a hidden class="anchor" aria-hidden="true" href="#58-å¯è§†åŒ–ç»“æœ">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/figure678.png"></p>
<p>å·¦å›¾æ˜¾ç¤º SPECVLM ä¿ç•™ token åçš„æ³¨æ„åŠ›çƒ­å›¾ä»ä¸åŸå§‹æ¨¡å‹å¯¹é½è‰¯å¥½ï¼›
ä¸­å›¾å±•ç¤ºä¸åŒ r ä¸‹ Ï„ ç¨³å®šï¼›
å³å›¾ä¸º speedup ä¸ Ï„ çš„å¹³è¡¡æ›²çº¿ã€‚</p>
<hr>
<h2 id="6-limitation">6. Limitation<a hidden class="anchor" aria-hidden="true" href="#6-limitation">#</a></h2>
<ol>
<li>
<p><strong>ä¸»è¦é€‚ç”¨äºé•¿è§†é¢‘ã€èµ„æºå—é™åœºæ™¯</strong>ï¼š
å½“ GPU å¸¦å®½ä¸ºä¸»è¦ç“¶é¢ˆæ—¶æ•ˆæœæ˜¾è‘—ã€‚</p>
</li>
<li>
<p><strong>éœ€è¦é¢å¤– draft æ¨¡å‹</strong>ï¼š</p>
</li>
</ol>
<p>è™½ç„¶å¼€é”€ç›¸å¯¹è¾ƒå°ï¼Œä»éœ€é€‰æ‹©åˆé€‚è‰ç¨¿æ¨¡å‹ã€‚</p>
<ol start="3">
<li><strong>Training-free è®¾è®¡é™åˆ¶æœ€å¤§åŠ é€Ÿ</strong>ï¼š
è‹¥æœªæ¥èƒ½è®­ç»ƒæ›´è½»é‡åŒ– Vid-LLM draftï¼Œå¯è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚</li>
</ol>
<hr>
<h2 id="7-conclusion">7. Conclusion<a hidden class="anchor" aria-hidden="true" href="#7-conclusion">#</a></h2>
<p>We propose <strong>SPECVLM</strong>, the first training-free speculative decoding framework tailored for accelerating video LLMs.
Building on the <strong>low speculation sensitivity to token pruning</strong>, SPECVLM leverages <strong>verifier-guided attention</strong> to remove redundant video tokens, significantly reducing the draft modelâ€™s KV cache <strong>without compromising generation quality</strong>.</p>
<p>SPECVLM achieves:</p>
<ul>
<li><strong>2.68Ã— speedup</strong> on <strong>LLaVA-OneVision-72B</strong></li>
<li><strong>2.11Ã— speedup</strong> on <strong>Qwen2.5-VL-32B</strong></li>
</ul>
<p>It provides a general, plug-and-play, training-free acceleration framework for long video reasoning.</p>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jjl357.github.io/blog/tags/mllm/">MLLM</a></li>
      <li><a href="https://jjl357.github.io/blog/tags/speculative-decoding/">Speculative Decoding</a></li>
      <li><a href="https://jjl357.github.io/blog/tags/paper-note/">Paper Note</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://jjl357.github.io/blog/">JJ&#39;s Blog</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
