\<!DOCTYPE html>
<html lang="zh-cn" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning | JJ&#39;s Blog</title>
<meta name="keywords" content="MLLM, Speculative Decoding, Paper Note">
<meta name="description" content="
Conference: EMNLP&#39;25
Github: https://github.com/zju-jiyicheng/SpecVLM

1. Motivation
Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.
例如：
LLaVA-OneVision (Li et al., 2024a) 将每一帧处理为 196 个视觉 token。若视频为两分钟、60 FPS，则总 token 数量超过 100 万。
如此大量的 video tokens 导致：

序列长度急剧增加；
Prefill 阶段的 attention 开销呈平方级增长；
Decoding 阶段 KV cache 急速膨胀，成为显著的 GPU 内存瓶颈。

在 autoregressive 生成过程中，每步生成的 KV cache 都必须与模型参数一起加载与存储于 GPU 显存，导致显著的 memory-bound 现象。">
<meta name="author" content="">
<link rel="canonical" href="https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/">
<link crossorigin="anonymous" href="https://jjl357.github.io/blog/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jjl357.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jjl357.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jjl357.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jjl357.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://jjl357.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh-cn" href="https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><script type="text/javascript"
        async
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<meta property="og:url" content="https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/">
  <meta property="og:site_name" content="JJ&#39;s Blog">
  <meta property="og:title" content="SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning">
  <meta property="og:description" content="
Conference: EMNLP&#39;25 Github: https://github.com/zju-jiyicheng/SpecVLM
1. Motivation Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.
例如： LLaVA-OneVision (Li et al., 2024a) 将每一帧处理为 196 个视觉 token。若视频为两分钟、60 FPS，则总 token 数量超过 100 万。 如此大量的 video tokens 导致：
序列长度急剧增加； Prefill 阶段的 attention 开销呈平方级增长； Decoding 阶段 KV cache 急速膨胀，成为显著的 GPU 内存瓶颈。 在 autoregressive 生成过程中，每步生成的 KV cache 都必须与模型参数一起加载与存储于 GPU 显存，导致显著的 memory-bound 现象。">
  <meta property="og:locale" content="zh-cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-29T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-29T00:00:00+00:00">
    <meta property="article:tag" content="MLLM">
    <meta property="article:tag" content="Speculative Decoding">
    <meta property="article:tag" content="Paper Note">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning">
<meta name="twitter:description" content="
Conference: EMNLP&#39;25
Github: https://github.com/zju-jiyicheng/SpecVLM

1. Motivation
Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.
例如：
LLaVA-OneVision (Li et al., 2024a) 将每一帧处理为 196 个视觉 token。若视频为两分钟、60 FPS，则总 token 数量超过 100 万。
如此大量的 video tokens 导致：

序列长度急剧增加；
Prefill 阶段的 attention 开销呈平方级增长；
Decoding 阶段 KV cache 急速膨胀，成为显著的 GPU 内存瓶颈。

在 autoregressive 生成过程中，每步生成的 KV cache 都必须与模型参数一起加载与存储于 GPU 显存，导致显著的 memory-bound 现象。">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jjl357.github.io/blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
      "item": "https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
  "name": "SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
  "description": "\nConference: EMNLP'25 Github: https://github.com/zju-jiyicheng/SpecVLM\n1. Motivation Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.\n例如： LLaVA-OneVision (Li et al., 2024a) 将每一帧处理为 196 个视觉 token。若视频为两分钟、60 FPS，则总 token 数量超过 100 万。 如此大量的 video tokens 导致：\n序列长度急剧增加； Prefill 阶段的 attention 开销呈平方级增长； Decoding 阶段 KV cache 急速膨胀，成为显著的 GPU 内存瓶颈。 在 autoregressive 生成过程中，每步生成的 KV cache 都必须与模型参数一起加载与存储于 GPU 显存，导致显著的 memory-bound 现象。\n",
  "keywords": [
    "MLLM", "Speculative Decoding", "Paper Note"
  ],
  "articleBody": "\nConference: EMNLP'25 Github: https://github.com/zju-jiyicheng/SpecVLM\n1. Motivation Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.\n例如： LLaVA-OneVision (Li et al., 2024a) 将每一帧处理为 196 个视觉 token。若视频为两分钟、60 FPS，则总 token 数量超过 100 万。 如此大量的 video tokens 导致：\n序列长度急剧增加； Prefill 阶段的 attention 开销呈平方级增长； Decoding 阶段 KV cache 急速膨胀，成为显著的 GPU 内存瓶颈。 在 autoregressive 生成过程中，每步生成的 KV cache 都必须与模型参数一起加载与存储于 GPU 显存，导致显著的 memory-bound 现象。\n2. Challenge 为缓解 video tokens 数量引发的计算与存储爆炸，近期研究提出了多种 token pruning 策略，通过识别 token 冗余性、计算重要性差异，在 prefill 阶段 进行剪枝以减少后续解码开销。\n然而，直接移除 tokens 会带来信息损失 —— 这对视频理解尤其致命，因为丰富的时空线索对于高质量生成至关重要。此外，单纯的 pruning 方法只能带来有限的加速，因为在每步生成时仍需访问完整参数。\nSpeculative Decoding as a Solution Speculative decoding (SD) 提供了一个思路： 使用一个轻量 draft 模型先生成多个候选 token，然后由 target 模型（verifier）并行验证。 理论上，这能在不牺牲生成质量的情况下大幅提升解码速度（Leviathan et al., 2023）。\n但将 SD 应用于 Vid-LLMs 存在两大挑战：\nDraft 模型 KV cache 的线性增长：对于长视频输入，draft 的 KV cache 会随时间膨胀，使其延迟反而成为主要瓶颈； 视频模态的高冗余性与低密度信息分布：现有针对长上下文的 SD 方法（Sun et al., 2024; Chen et al., 2025; Yang et al., 2025a）都是“模态无关”的，无法利用视频注意力分布的特殊性，因此在视频任务中表现不佳。 于是论文提出：\n通过 在 draft 模型中引入视频 token 削减（video token pruning），可有效减小其 KV cache 大小，从而提升 speculative decoding 效率。\n2.1 Naive Speculative Decoding for Vid-LLMs 设：\n目标模型（verifier）：( M_t ) 草稿模型（draft）：( M_d ) ( T_t )：目标模型单 token 解码时间 ( T_d )：草稿模型单 token 解码时间 ( T_t^\\gamma )：目标模型验证 γ 个 token 的时间 则每次 speculative decoding step 的总时间为：\n$$ T_{\\text{step}}^\\gamma = \\gamma \\cdot T_d + T_t^\\gamma $$\n平均每 token 时间为：\n$$ T_{\\text{token}}^\\gamma = \\frac{T_{\\text{step}}^\\gamma}{\\tau} $$\n其中 (\\tau) 为平均 accept length。\n因此速度提升比为：\n$$ \\text{Speedup} = \\frac{T_t}{T_{\\text{token}}^\\gamma} = \\frac{\\tau \\cdot T_t}{\\gamma \\cdot T_d + T_t^\\gamma} = \\frac{\\tau}{\\gamma \\cdot \\frac{T_d}{T_t} + \\frac{T_t^\\gamma}{T_t}} $$\n通常情况下 ( T_t^\\gamma / T_t \\approx 1 )，因此 速度主要由平均接受长度 τ 和 latency 比 (T_d/T_t) 决定。\n对于视频模型，随着输入长度增加，draft 的 KV cache 迅速膨胀，使 (T_d) 增大，从而削弱 speculative decoding 的加速效果。\n2.2 Speculation Sensitivity for Token Pruning 为了降低 draft 模型的 KV cache，本研究引入 视频 token 削减。 但问题在于：token 减少意味着视觉信息损失，是否会降低 speculation 的准确性？\n论文通过实验发现：\n在 VideoDetailCaption 基准上进行随机 token pruning； 当 pruning ratio ≤ 50% 时，平均接受长度 τ 几乎不变； 在某些情况下（适度 pruning）甚至提升 τ； 当完全移除所有视频 token（100% pruning）时，τ 与总体加速明显下降。 这说明：\n视频输入存在大量冗余。适度削减不仅不会损害 speculative 准确度，反而能去除干扰性冗余，提升模型专注度。\n然而随机削减（Random pruning）在高比率时表现不稳，尤其当关键帧或关键物体被删除时会严重损害性能。\n3. Contribution SPECVLM 提出了一种 verifier-guided、两阶段视频 token 削减（staged video token pruning） 策略，有效延伸 speculative decoding 在高剪枝比例下的加速优势。\n核心思想：\n通过 目标模型的注意力分布（attention guidance） 识别关键视频 token； 高注意力区域 采用 Top-P 保留； 低注意力区域 采用空间均匀下采样； 削减后的 tokens 被送入 draft 模型，以显著减小其 KV cache，从而提升 speculative decoding 效率。 主要贡献总结：\n首次探索视频 LLM 的无损 speculative decoding 加速。 发现 “视频 token 爆炸” 是 draft slowdown 的核心原因，并提出针对性削减方案。\n发现 draft 模型对随机剪枝的不敏感性（speculation insensitivity），由此提出 Verifier-guided staged pruning 策略，在高比例剪枝下仍保持高接受率。\n实验结果：\n剪枝 90% 视频 tokens 后仍保留约 90% speculation accuracy； LLaVA-OneVision 加速 2.68×； Qwen2.5-VL 加速 2.11×。 4. Method 4.1 Attention-Guided Token Importance Estimation SPECVLM 利用目标模型的 language-to-video attention 来判断视频 token 的重要性。\n设：\n(L)：语言 token 集； (V)：视频 token 集； (G \\in \\mathbb{R}^{|L|\\times|V|})：语言到视频的注意力矩阵。 定义每个视频 token (j) 的重要性分数为：\n$$ a_j = \\frac{1}{|L|} \\sum_{i=1}^{|L|} G_{i,j} $$\n即语言 token 对第 j 个视频 token 的平均注意力。 在实现中，会对所有层与头取平均，形成最终的 attention map (A = {a_j})。\n4.2 Two-Stage Video Token Pruning SPECVLM 提出 Two-Stage Token Pruning： （1）Top-P Retention； （2）Spatially Uniform Reduction。\nStage I — Top-P Retention 论文发现 video attention 分布呈长尾形态：少数 token 占据了大部分注意力。 因此首先选取高注意力 token。\n定义累计注意力阈值 (\\lambda_r)，求出最小 c 满足：\n$$ \\frac{\\sum_{i=1}^{c} a_{(i)}}{\\sum_{j=1}^{|V|} a_j} \\ge \\lambda_r $$\n其中 (a_{(i)}) 为第 i 大的 attention 值。\n保留这 c 个 token 构成集合 (V_R)。\n(\\lambda_r) 与 pruning ratio (r) 通过小规模校准集确定，例如在 LLaVA-OneVision 上使用 (\\lambda_r=0.4) 对应 (r=0.9)。\nStage II — Spatially Uniform Reduction 对于剩余 tokens (V \\setminus V_R)，由于注意力值接近且空间位置相近，直接删除会破坏视频的时空结构。 因此论文设计了空间均匀采样策略：\n设剩余 token 数量为 (|V| - |V_R|)，则采样间隔为：\n$$ I = \\frac{|V| - |V_R|}{(1-r)|V|} $$\n以此间隔在空间上（如每帧的 14×14 patch grid）均匀采样，形成集合 (V_U)。\n最终保留集合：\n$$ V’ = V_R \\cup V_U $$\n这样既保留了语义关键信息，又维持了空间结构的完整性。\n4.3 Why Verifier Guidance Works 由于 verifier 是最终生成分布的参考，其 language-to-video attention 能直接反映哪些视频区域被语言输出所依赖。 因此，这种 attention guidance 是一种自然的“importance estimator”，比启发式剪枝更可靠。\n4.4 Complexity Analysis 假设原始视频 tokens 为 (N)，prune 比例为 (r)，则 draft KV cache 大小下降到 ((1-r)N)。 prefill 与 decode 的 KV load/store 开销近似线性减小。\n总体 speculative step latency： $$ T_{\\text{step}}^\\gamma = \\gamma \\cdot T_d(r) + T_t^\\gamma $$\n由于 (T_d(r) \\propto (1-r))，可得理论加速： $$ \\text{Speedup} \\approx \\frac{\\tau}{\\gamma(1-r)\\frac{T_d}{T_t}+1} $$\n5. Evaluation 5.1 实验设置 模型系列：\nLLaVA-OneVision (72B / 7B) Qwen2.5-VL (32B / 7B) 场景：\nStandard SD (Std.-SD)：大模型 + 小 draft 模型； Self-SD：同一模型自生成，自剪枝。 任务基准：\nVideoDetailCaption (LMMs-Lab, 2024) MVBench, MVLU, LongVideoBench 输入设定：\n采样 64–128 帧； 每帧 196 tokens； 默认剪枝率 (r=0.9)。 硬件：\n8×A100 GPUs； Spec length γ=5； 平均 over 50 samples。 5.2 Baselines Baseline 描述 Vanilla 普通 autoregressive 解码 SD-Tree EAGLE 风格树形 speculation SD-Rand SD + 随机 token 剪枝 SD-Window / Frame / DyCoke / FastVID 空间/时间冗余剪枝基线 SD-Uniform 空间均匀抽样（无 verifier guidance） 5.3 主结果分析 LLaVA-OneVision (72B-7B, Std.-SD) 方法 τ Tokens/s Speedup Vanilla – 2.94 1.0× SD-Tree 3.57 6.41 2.18× SD-Rand (r=0.9) 3.19 7.36 2.50× SPECVLM (r=0.9) 3.48 7.88 2.68× 结论：\n剪枝 90% tokens 后仍保持 97% 的 τ； draft KV 减少 → prefill+decode latency 降低； 整体加速比随机剪枝更高。 Qwen2.5-VL (32B-7B, Std.-SD) 方法 τ Tokens/s Speedup Vanilla – 2.56 1.0× SPECVLM (r=0.9) 3.25 5.40 2.11× 说明 SPECVLM 对不同体系架构具有一致加速效果。\nSelf-SD 场景 当没有独立 draft 模型时，SPECVLM 在 Self-SD 下仍带来约 1.3× 加速。 此时 pruning 仅减少自身 KV cache，无需额外模型。\n5.4 Scaling Law for Pruning Ratio 论文在 Figure 6 展示了随剪枝率 r 变化的 τ 曲线：\n随着 r 从 0 → 0.9，SPECVLM 的 τ 下降幅度远小于 Random / Uniform； 当 r 超过 0.8 时，SPECVLM 依旧保持稳定； 说明 verifier-guided 策略能在极高压缩率下保留关键信息。 5.5 Ablation Study 仅使用 spatial uniform（无 attention guidance）： τ 明显下降，尤其在复杂场景下，说明 attention 引导对鲁棒性关键。\n仅使用 Top-P（无 uniform sampling）： 高注意力区保留但空间结构断裂，造成语义不连贯。\n双阶段保留（SPECVLM）： 兼顾语义与空间一致性，在各比例下表现最优。\n5.6 Latency Breakdown 表 5 显示时间分布（LLaVA-72B/7B，输出 256 tokens）：\n模块 Vanilla SPECVLM Prefill (draft) 24.2s 9.6s Draft decode 28.9s 12.5s Target verify 57.9s 35.2s Total 111s 57s 削减 draft KV 大小直接减少 draft prefill 与 decode 延迟，使总推理时间减半。\n5.7 Early-step Accept Length Stability 论文进一步研究 τ 在解码过程的分布（表 3）：\n前 10 步的 τ 与整体平均 τ 几乎一致； 说明 SPECVLM 剪枝不会导致早期 speculation 失效。 5.8 可视化结果 左图显示 SPECVLM 保留 token 后的注意力热图仍与原始模型对齐良好； 中图展示不同 r 下 τ 稳定； 右图为 speedup 与 τ 的平衡曲线。\n6. Limitation 主要适用于长视频、资源受限场景： 当 GPU 带宽为主要瓶颈时效果显著。\n需要额外 draft 模型：\n虽然开销相对较小，仍需选择合适草稿模型。\nTraining-free 设计限制最大加速： 若未来能训练更轻量化 Vid-LLM draft，可进一步提升性能。 7. Conclusion We propose SPECVLM, the first training-free speculative decoding framework tailored for accelerating video LLMs. Building on the low speculation sensitivity to token pruning, SPECVLM leverages verifier-guided attention to remove redundant video tokens, significantly reducing the draft model’s KV cache without compromising generation quality.\nSPECVLM achieves:\n2.68× speedup on LLaVA-OneVision-72B 2.11× speedup on Qwen2.5-VL-32B It provides a general, plug-and-play, training-free acceleration framework for long video reasoning.\n",
  "wordCount" : "956",
  "inLanguage": "zh-cn",
  "datePublished": "2025-10-29T00:00:00Z",
  "dateModified": "2025-10-29T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JJ's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jjl357.github.io/blog/favicon.ico"
    }
  }
}
</script>

    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$','$'], ['\\(','\\)']],
                processEscapes: true
            }
        });
    </script>

    
    <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jjl357.github.io/blog/" accesskey="h" title="JJ&#39;s Blog (Alt + H)">JJ&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning
    </h1>
    <div class="post-meta"><span title='2025-10-29 00:00:00 +0000 UTC'>October 29, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/title.png"></p>
<p><strong>Conference:</strong> <strong>EMNLP'25</strong>
<strong>Github:</strong> <a href="https://github.com/zju-jiyicheng/SpecVLM">https://github.com/zju-jiyicheng/SpecVLM</a></p>
<hr>
<h2 id="1-motivation">1. Motivation<a hidden class="anchor" aria-hidden="true" href="#1-motivation">#</a></h2>
<p>Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.</p>
<p>例如：
LLaVA-OneVision (Li et al., 2024a) 将每一帧处理为 196 个视觉 token。若视频为两分钟、60 FPS，则总 token 数量超过 100 万。
如此大量的 video tokens 导致：</p>
<ul>
<li>序列长度急剧增加；</li>
<li>Prefill 阶段的 attention 开销呈平方级增长；</li>
<li>Decoding 阶段 KV cache 急速膨胀，成为显著的 GPU 内存瓶颈。</li>
</ul>
<p>在 autoregressive 生成过程中，每步生成的 KV cache 都必须与模型参数一起加载与存储于 GPU 显存，导致显著的 memory-bound 现象。</p>
<hr>
<h2 id="2-challenge">2. Challenge<a hidden class="anchor" aria-hidden="true" href="#2-challenge">#</a></h2>
<p>为缓解 video tokens 数量引发的计算与存储爆炸，近期研究提出了多种 token pruning 策略，通过识别 token 冗余性、计算重要性差异，在 <strong>prefill 阶段</strong> 进行剪枝以减少后续解码开销。</p>
<p>然而，<strong>直接移除 tokens 会带来信息损失</strong> —— 这对视频理解尤其致命，因为丰富的时空线索对于高质量生成至关重要。此外，单纯的 pruning 方法只能带来有限的加速，因为在每步生成时仍需访问完整参数。</p>
<hr>
<h3 id="speculative-decoding-as-a-solution">Speculative Decoding as a Solution<a hidden class="anchor" aria-hidden="true" href="#speculative-decoding-as-a-solution">#</a></h3>
<p>Speculative decoding (SD) 提供了一个思路：
使用一个轻量 draft 模型先生成多个候选 token，然后由 target 模型（verifier）并行验证。
理论上，这能在不牺牲生成质量的情况下大幅提升解码速度（Leviathan et al., 2023）。</p>
<p>但将 SD 应用于 Vid-LLMs 存在两大挑战：</p>
<ol>
<li><strong>Draft 模型 KV cache 的线性增长</strong>：对于长视频输入，draft 的 KV cache 会随时间膨胀，使其延迟反而成为主要瓶颈；</li>
<li><strong>视频模态的高冗余性与低密度信息分布</strong>：现有针对长上下文的 SD 方法（Sun et al., 2024; Chen et al., 2025; Yang et al., 2025a）都是“模态无关”的，无法利用视频注意力分布的特殊性，因此在视频任务中表现不佳。</li>
</ol>
<p>于是论文提出：</p>
<blockquote>
<p>通过 <strong>在 draft 模型中引入视频 token 削减（video token pruning）</strong>，可有效减小其 KV cache 大小，从而提升 speculative decoding 效率。</p>
</blockquote>
<hr>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/figure2.png"></p>
<hr>
<h2 id="21-naive-speculative-decoding-for-vid-llms">2.1 Naive Speculative Decoding for Vid-LLMs<a hidden class="anchor" aria-hidden="true" href="#21-naive-speculative-decoding-for-vid-llms">#</a></h2>
<p>设：</p>
<ul>
<li>目标模型（verifier）：( M_t )</li>
<li>草稿模型（draft）：( M_d )</li>
<li>( T_t )：目标模型单 token 解码时间</li>
<li>( T_d )：草稿模型单 token 解码时间</li>
<li>( T_t^\gamma )：目标模型验证 γ 个 token 的时间</li>
</ul>
<p>则每次 speculative decoding step 的总时间为：</p>
<p>$$
T_{\text{step}}^\gamma = \gamma \cdot T_d + T_t^\gamma
$$</p>
<p>平均每 token 时间为：</p>
<p>$$
T_{\text{token}}^\gamma = \frac{T_{\text{step}}^\gamma}{\tau}
$$</p>
<p>其中 (\tau) 为平均 accept length。</p>
<p>因此速度提升比为：</p>
<p>$$
\text{Speedup} = \frac{T_t}{T_{\text{token}}^\gamma} = \frac{\tau \cdot T_t}{\gamma \cdot T_d + T_t^\gamma} = \frac{\tau}{\gamma \cdot \frac{T_d}{T_t} + \frac{T_t^\gamma}{T_t}}
$$</p>
<p>通常情况下 ( T_t^\gamma / T_t \approx 1 )，因此 <strong>速度主要由平均接受长度 τ 和 latency 比 (T_d/T_t) 决定</strong>。</p>
<p>对于视频模型，随着输入长度增加，draft 的 KV cache 迅速膨胀，使 (T_d) 增大，从而削弱 speculative decoding 的加速效果。</p>
<hr>
<h3 id="22-speculation-sensitivity-for-token-pruning">2.2 Speculation Sensitivity for Token Pruning<a hidden class="anchor" aria-hidden="true" href="#22-speculation-sensitivity-for-token-pruning">#</a></h3>
<p>为了降低 draft 模型的 KV cache，本研究引入 <strong>视频 token 削减</strong>。
但问题在于：token 减少意味着视觉信息损失，是否会降低 speculation 的准确性？</p>
<p>论文通过实验发现：</p>
<ul>
<li>在 VideoDetailCaption 基准上进行随机 token pruning；</li>
<li>当 pruning ratio ≤ 50% 时，平均接受长度 τ 几乎不变；</li>
<li>在某些情况下（适度 pruning）甚至提升 τ；</li>
<li>当完全移除所有视频 token（100% pruning）时，τ 与总体加速明显下降。</li>
</ul>
<p>这说明：</p>
<blockquote>
<p>视频输入存在大量冗余。适度削减不仅不会损害 speculative 准确度，反而能去除干扰性冗余，提升模型专注度。</p>
</blockquote>
<p>然而随机削减（Random pruning）在高比率时表现不稳，尤其当关键帧或关键物体被删除时会严重损害性能。</p>
<hr>
<h2 id="3-contribution">3. Contribution<a hidden class="anchor" aria-hidden="true" href="#3-contribution">#</a></h2>
<p>SPECVLM 提出了一种 <strong>verifier-guided、两阶段视频 token 削减（staged video token pruning）</strong> 策略，有效延伸 speculative decoding 在高剪枝比例下的加速优势。</p>
<p>核心思想：</p>
<ul>
<li>通过 <strong>目标模型的注意力分布（attention guidance）</strong> 识别关键视频 token；</li>
<li><strong>高注意力区域</strong> 采用 Top-P 保留；</li>
<li><strong>低注意力区域</strong> 采用空间均匀下采样；</li>
<li>削减后的 tokens 被送入 draft 模型，以显著减小其 KV cache，从而提升 speculative decoding 效率。</li>
</ul>
<p><strong>主要贡献总结：</strong></p>
<ol>
<li>
<p><strong>首次探索视频 LLM 的无损 speculative decoding 加速。</strong>
发现 “视频 token 爆炸” 是 draft slowdown 的核心原因，并提出针对性削减方案。</p>
</li>
<li>
<p><strong>发现 draft 模型对随机剪枝的不敏感性（speculation insensitivity）</strong>，由此提出 <strong>Verifier-guided staged pruning</strong> 策略，在高比例剪枝下仍保持高接受率。</p>
</li>
<li>
<p><strong>实验结果：</strong></p>
<ul>
<li>剪枝 90% 视频 tokens 后仍保留约 90% speculation accuracy；</li>
<li>LLaVA-OneVision 加速 2.68×；</li>
<li>Qwen2.5-VL 加速 2.11×。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="4-method">4. Method<a hidden class="anchor" aria-hidden="true" href="#4-method">#</a></h2>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/figure3.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/figure4.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/figure5.png"></p>
<hr>
<h3 id="41-attention-guided-token-importance-estimation">4.1 Attention-Guided Token Importance Estimation<a hidden class="anchor" aria-hidden="true" href="#41-attention-guided-token-importance-estimation">#</a></h3>
<p>SPECVLM 利用目标模型的 <strong>language-to-video attention</strong> 来判断视频 token 的重要性。</p>
<p>设：</p>
<ul>
<li>(L)：语言 token 集；</li>
<li>(V)：视频 token 集；</li>
<li>(G \in \mathbb{R}^{|L|\times|V|})：语言到视频的注意力矩阵。</li>
</ul>
<p>定义每个视频 token (j) 的重要性分数为：</p>
<p>$$
a_j = \frac{1}{|L|} \sum_{i=1}^{|L|} G_{i,j}
$$</p>
<p>即语言 token 对第 j 个视频 token 的平均注意力。
在实现中，会对所有层与头取平均，形成最终的 attention map (A = {a_j})。</p>
<hr>
<h3 id="42-two-stage-video-token-pruning">4.2 Two-Stage Video Token Pruning<a hidden class="anchor" aria-hidden="true" href="#42-two-stage-video-token-pruning">#</a></h3>
<p>SPECVLM 提出 <strong>Two-Stage Token Pruning</strong>：
（1）Top-P Retention；
（2）Spatially Uniform Reduction。</p>
<hr>
<h4 id="stage-i--top-p-retention"><strong>Stage I — Top-P Retention</strong><a hidden class="anchor" aria-hidden="true" href="#stage-i--top-p-retention">#</a></h4>
<p>论文发现 video attention 分布呈长尾形态：少数 token 占据了大部分注意力。
因此首先选取高注意力 token。</p>
<p>定义累计注意力阈值 (\lambda_r)，求出最小 c 满足：</p>
<p>$$
\frac{\sum_{i=1}^{c} a_{(i)}}{\sum_{j=1}^{|V|} a_j} \ge \lambda_r
$$</p>
<p>其中 (a_{(i)}) 为第 i 大的 attention 值。</p>
<p>保留这 c 个 token 构成集合 (V_R)。</p>
<p>(\lambda_r) 与 pruning ratio (r) 通过小规模校准集确定，例如在 LLaVA-OneVision 上使用 (\lambda_r=0.4) 对应 (r=0.9)。</p>
<hr>
<h4 id="stage-ii--spatially-uniform-reduction"><strong>Stage II — Spatially Uniform Reduction</strong><a hidden class="anchor" aria-hidden="true" href="#stage-ii--spatially-uniform-reduction">#</a></h4>
<p>对于剩余 tokens (V \setminus V_R)，由于注意力值接近且空间位置相近，直接删除会破坏视频的时空结构。
因此论文设计了空间均匀采样策略：</p>
<p>设剩余 token 数量为 (|V| - |V_R|)，则采样间隔为：</p>
<p>$$
I = \frac{|V| - |V_R|}{(1-r)|V|}
$$</p>
<p>以此间隔在空间上（如每帧的 14×14 patch grid）均匀采样，形成集合 (V_U)。</p>
<p>最终保留集合：</p>
<p>$$
V&rsquo; = V_R \cup V_U
$$</p>
<p>这样既保留了语义关键信息，又维持了空间结构的完整性。</p>
<hr>
<h3 id="43-why-verifier-guidance-works">4.3 Why Verifier Guidance Works<a hidden class="anchor" aria-hidden="true" href="#43-why-verifier-guidance-works">#</a></h3>
<p>由于 verifier 是最终生成分布的参考，其 language-to-video attention 能直接反映哪些视频区域被语言输出所依赖。
因此，这种 attention guidance 是一种自然的“importance estimator”，比启发式剪枝更可靠。</p>
<hr>
<h3 id="44-complexity-analysis">4.4 Complexity Analysis<a hidden class="anchor" aria-hidden="true" href="#44-complexity-analysis">#</a></h3>
<p>假设原始视频 tokens 为 (N)，prune 比例为 (r)，则 draft KV cache 大小下降到 ((1-r)N)。
prefill 与 decode 的 KV load/store 开销近似线性减小。</p>
<p>总体 speculative step latency：
$$
T_{\text{step}}^\gamma = \gamma \cdot T_d(r) + T_t^\gamma
$$</p>
<p>由于 (T_d(r) \propto (1-r))，可得理论加速：
$$
\text{Speedup} \approx \frac{\tau}{\gamma(1-r)\frac{T_d}{T_t}+1}
$$</p>
<hr>
<h2 id="5-evaluation">5. Evaluation<a hidden class="anchor" aria-hidden="true" href="#5-evaluation">#</a></h2>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/table12.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/table3.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/figure678.png"></p>
<hr>
<h3 id="51-实验设置">5.1 实验设置<a hidden class="anchor" aria-hidden="true" href="#51-实验设置">#</a></h3>
<ul>
<li>
<p><strong>模型系列</strong>：</p>
<ul>
<li>LLaVA-OneVision (72B / 7B)</li>
<li>Qwen2.5-VL (32B / 7B)</li>
</ul>
</li>
<li>
<p><strong>场景</strong>：</p>
<ul>
<li>Standard SD (Std.-SD)：大模型 + 小 draft 模型；</li>
<li>Self-SD：同一模型自生成，自剪枝。</li>
</ul>
</li>
<li>
<p><strong>任务基准</strong>：</p>
<ul>
<li>VideoDetailCaption (LMMs-Lab, 2024)</li>
<li>MVBench, MVLU, LongVideoBench</li>
</ul>
</li>
<li>
<p><strong>输入设定</strong>：</p>
<ul>
<li>采样 64–128 帧；</li>
<li>每帧 196 tokens；</li>
<li>默认剪枝率 (r=0.9)。</li>
</ul>
</li>
<li>
<p><strong>硬件</strong>：</p>
<ul>
<li>8×A100 GPUs；</li>
<li>Spec length γ=5；</li>
<li>平均 over 50 samples。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="52-baselines">5.2 Baselines<a hidden class="anchor" aria-hidden="true" href="#52-baselines">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Baseline</th>
          <th>描述</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Vanilla</strong></td>
          <td>普通 autoregressive 解码</td>
      </tr>
      <tr>
          <td><strong>SD-Tree</strong></td>
          <td>EAGLE 风格树形 speculation</td>
      </tr>
      <tr>
          <td><strong>SD-Rand</strong></td>
          <td>SD + 随机 token 剪枝</td>
      </tr>
      <tr>
          <td><strong>SD-Window / Frame / DyCoke / FastVID</strong></td>
          <td>空间/时间冗余剪枝基线</td>
      </tr>
      <tr>
          <td><strong>SD-Uniform</strong></td>
          <td>空间均匀抽样（无 verifier guidance）</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="53-主结果分析">5.3 主结果分析<a hidden class="anchor" aria-hidden="true" href="#53-主结果分析">#</a></h3>
<h4 id="llava-onevision-72b-7b-std-sd"><strong>LLaVA-OneVision (72B-7B, Std.-SD)</strong><a hidden class="anchor" aria-hidden="true" href="#llava-onevision-72b-7b-std-sd">#</a></h4>
<table>
  <thead>
      <tr>
          <th>方法</th>
          <th>τ</th>
          <th>Tokens/s</th>
          <th>Speedup</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Vanilla</td>
          <td>–</td>
          <td>2.94</td>
          <td>1.0×</td>
      </tr>
      <tr>
          <td>SD-Tree</td>
          <td>3.57</td>
          <td>6.41</td>
          <td>2.18×</td>
      </tr>
      <tr>
          <td>SD-Rand (r=0.9)</td>
          <td>3.19</td>
          <td>7.36</td>
          <td>2.50×</td>
      </tr>
      <tr>
          <td><strong>SPECVLM (r=0.9)</strong></td>
          <td><strong>3.48</strong></td>
          <td><strong>7.88</strong></td>
          <td><strong>2.68×</strong></td>
      </tr>
  </tbody>
</table>
<p>结论：</p>
<ul>
<li>剪枝 90% tokens 后仍保持 97% 的 τ；</li>
<li>draft KV 减少 → prefill+decode latency 降低；</li>
<li>整体加速比随机剪枝更高。</li>
</ul>
<hr>
<h4 id="qwen25-vl-32b-7b-std-sd"><strong>Qwen2.5-VL (32B-7B, Std.-SD)</strong><a hidden class="anchor" aria-hidden="true" href="#qwen25-vl-32b-7b-std-sd">#</a></h4>
<table>
  <thead>
      <tr>
          <th>方法</th>
          <th>τ</th>
          <th>Tokens/s</th>
          <th>Speedup</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Vanilla</td>
          <td>–</td>
          <td>2.56</td>
          <td>1.0×</td>
      </tr>
      <tr>
          <td><strong>SPECVLM (r=0.9)</strong></td>
          <td><strong>3.25</strong></td>
          <td><strong>5.40</strong></td>
          <td><strong>2.11×</strong></td>
      </tr>
  </tbody>
</table>
<p>说明 SPECVLM 对不同体系架构具有一致加速效果。</p>
<hr>
<h4 id="self-sd-场景"><strong>Self-SD 场景</strong><a hidden class="anchor" aria-hidden="true" href="#self-sd-场景">#</a></h4>
<p>当没有独立 draft 模型时，SPECVLM 在 Self-SD 下仍带来约 1.3× 加速。
此时 pruning 仅减少自身 KV cache，无需额外模型。</p>
<hr>
<h3 id="54-scaling-law-for-pruning-ratio">5.4 Scaling Law for Pruning Ratio<a hidden class="anchor" aria-hidden="true" href="#54-scaling-law-for-pruning-ratio">#</a></h3>
<p>论文在 Figure 6 展示了随剪枝率 r 变化的 τ 曲线：</p>
<ul>
<li>随着 r 从 0 → 0.9，SPECVLM 的 τ 下降幅度远小于 Random / Uniform；</li>
<li>当 r 超过 0.8 时，SPECVLM 依旧保持稳定；</li>
<li>说明 verifier-guided 策略能在极高压缩率下保留关键信息。</li>
</ul>
<hr>
<h3 id="55-ablation-study">5.5 Ablation Study<a hidden class="anchor" aria-hidden="true" href="#55-ablation-study">#</a></h3>
<ol>
<li>
<p><strong>仅使用 spatial uniform（无 attention guidance）</strong>：
τ 明显下降，尤其在复杂场景下，说明 attention 引导对鲁棒性关键。</p>
</li>
<li>
<p><strong>仅使用 Top-P（无 uniform sampling）</strong>：
高注意力区保留但空间结构断裂，造成语义不连贯。</p>
</li>
<li>
<p><strong>双阶段保留（SPECVLM）</strong>：
兼顾语义与空间一致性，在各比例下表现最优。</p>
</li>
</ol>
<hr>
<h3 id="56-latency-breakdown">5.6 Latency Breakdown<a hidden class="anchor" aria-hidden="true" href="#56-latency-breakdown">#</a></h3>
<p>表 5 显示时间分布（LLaVA-72B/7B，输出 256 tokens）：</p>
<table>
  <thead>
      <tr>
          <th>模块</th>
          <th>Vanilla</th>
          <th>SPECVLM</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Prefill (draft)</td>
          <td>24.2s</td>
          <td>9.6s</td>
      </tr>
      <tr>
          <td>Draft decode</td>
          <td>28.9s</td>
          <td>12.5s</td>
      </tr>
      <tr>
          <td>Target verify</td>
          <td>57.9s</td>
          <td>35.2s</td>
      </tr>
      <tr>
          <td><strong>Total</strong></td>
          <td><strong>111s</strong></td>
          <td><strong>57s</strong></td>
      </tr>
  </tbody>
</table>
<p>削减 draft KV 大小直接减少 draft prefill 与 decode 延迟，使总推理时间减半。</p>
<hr>
<h3 id="57-early-step-accept-length-stability">5.7 Early-step Accept Length Stability<a hidden class="anchor" aria-hidden="true" href="#57-early-step-accept-length-stability">#</a></h3>
<p>论文进一步研究 τ 在解码过程的分布（表 3）：</p>
<ul>
<li>前 10 步的 τ 与整体平均 τ 几乎一致；</li>
<li>说明 SPECVLM 剪枝不会导致早期 speculation 失效。</li>
</ul>
<hr>
<h3 id="58-可视化结果">5.8 可视化结果<a hidden class="anchor" aria-hidden="true" href="#58-可视化结果">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/SpecVLM/figure678.png"></p>
<p>左图显示 SPECVLM 保留 token 后的注意力热图仍与原始模型对齐良好；
中图展示不同 r 下 τ 稳定；
右图为 speedup 与 τ 的平衡曲线。</p>
<hr>
<h2 id="6-limitation">6. Limitation<a hidden class="anchor" aria-hidden="true" href="#6-limitation">#</a></h2>
<ol>
<li>
<p><strong>主要适用于长视频、资源受限场景</strong>：
当 GPU 带宽为主要瓶颈时效果显著。</p>
</li>
<li>
<p><strong>需要额外 draft 模型</strong>：</p>
</li>
</ol>
<p>虽然开销相对较小，仍需选择合适草稿模型。</p>
<ol start="3">
<li><strong>Training-free 设计限制最大加速</strong>：
若未来能训练更轻量化 Vid-LLM draft，可进一步提升性能。</li>
</ol>
<hr>
<h2 id="7-conclusion">7. Conclusion<a hidden class="anchor" aria-hidden="true" href="#7-conclusion">#</a></h2>
<p>We propose <strong>SPECVLM</strong>, the first training-free speculative decoding framework tailored for accelerating video LLMs.
Building on the <strong>low speculation sensitivity to token pruning</strong>, SPECVLM leverages <strong>verifier-guided attention</strong> to remove redundant video tokens, significantly reducing the draft model’s KV cache <strong>without compromising generation quality</strong>.</p>
<p>SPECVLM achieves:</p>
<ul>
<li><strong>2.68× speedup</strong> on <strong>LLaVA-OneVision-72B</strong></li>
<li><strong>2.11× speedup</strong> on <strong>Qwen2.5-VL-32B</strong></li>
</ul>
<p>It provides a general, plug-and-play, training-free acceleration framework for long video reasoning.</p>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jjl357.github.io/blog/tags/mllm/">MLLM</a></li>
      <li><a href="https://jjl357.github.io/blog/tags/speculative-decoding/">Speculative Decoding</a></li>
      <li><a href="https://jjl357.github.io/blog/tags/paper-note/">Paper Note</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://jjl357.github.io/blog/">JJ&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
