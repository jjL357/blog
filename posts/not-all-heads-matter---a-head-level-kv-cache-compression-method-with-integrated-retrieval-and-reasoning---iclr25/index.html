<!DOCTYPE html>
<html lang="zh-cn" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>NOT ALL HEADS MATTER: A HEAD-LEVEL KV CACHE COMPRESSION METHOD WITH INTEGRATED RETRIEVAL AND REASONING | JJ&#39;s Blog</title>
<meta name="keywords" content="Paper Note, KV Cache">
<meta name="description" content="
Conference: ICLR&#39;25
Github: https://github.com/FYYFU/HeadKV.

My Thoughts
è¿™ç¯‡å·¥ä½œå’Œ DuoAttention çš„å…³æ³¨ç‚¹ç±»ä¼¼ï¼Œéƒ½æ˜¯å…³æ³¨ä¸åŒ attention head å¯¹æ¨¡å‹ä¸åŒèƒ½åŠ›çš„è´¡çŒ®ä¸åŒï¼Œè¿™ç¯‡å·¥ä½œæ›´å…³æ³¨ attention head å¯¹æ¨¡å‹ Retrieval ä¸ Reasoning çš„ importance score, æ¥å®ç° KV Cache çš„ nonuniform budget allocation ã€‚
1. Motivation
ç°ä»£ LLM è¶Šæ¥è¶Šæ”¯æŒæé•¿ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ GPT-4ã€Llama-3ã€Qwen-2ã€Claude ç­‰ï¼‰ï¼Œä½†éšç€è¾“å…¥é•¿åº¦å¢é•¿ï¼ŒTransformer çš„ self-attention å¯¼è‡´ KV cacheï¼ˆattention çš„ key/value çŠ¶æ€ï¼‰å ç”¨å†…å­˜çº¿æ€§å¢é•¿ï¼Œæˆä¸ºæ¨ç†é˜¶æ®µçš„ä¸»è¦ç“¶é¢ˆã€‚å·²æœ‰å·¥ä½œé€šè¿‡ token eviction / å±‚çº§ç¼“å­˜å‹ç¼©æ¥ç¼“è§£ï¼Œä½†å‡ ä¹æ²¡æœ‰ç ”ç©¶åœ¨â€œå¤´ï¼ˆheadï¼‰çº§åˆ«â€ä¸Šå¯¹ KV cache å¤§å°è¿›è¡Œå·®å¼‚åŒ–åˆ†é…ã€‚ä½œè€…è§‚å¯Ÿåˆ° attention heads åœ¨åŠŸèƒ½ä¸Šé«˜åº¦å¼‚è´¨ï¼ˆå¦‚ retrieval headsã€reasoning heads ç­‰ï¼‰ï¼Œå› æ­¤æå‡ºåŸºäºå¤´é‡è¦æ€§çš„ head-level KV cache å‹ç¼©æ–¹æ³•ï¼ˆHeadKVï¼‰ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºç»“åˆæ£€ç´¢ä¸æ¨ç†èƒ½åŠ›è¯„ä¼°çš„ HeadKV-R2ã€‚

2. Relative Work
2.1 Attention heads
å›é¡¾äº†å¯¹å¤šå¤´æ³¨æ„åŠ›ä¸­ head åŠŸèƒ½çš„ç ”ç©¶ï¼ˆVoita et al., Olsson et al., Wu et al., Zheng et al. ç­‰ï¼‰ï¼Œå¹¶æŒ‡å‡ºä¸åŒ head åœ¨è¯æ³•ã€ç»“æ„ã€å¤åˆ¶ï¼ˆinductionï¼‰ã€æ£€ç´¢ç­‰æ–¹é¢æ‰®æ¼”ä¸åŒè§’è‰²ã€‚è¿™äº›è§‚å¯Ÿä¸ºæŒ‰ head åˆ†é… KV cache æä¾›ç†è®ºåŸºç¡€ã€‚">
<meta name="author" content="">
<link rel="canonical" href="https://jjl357.github.io/blog/posts/not-all-heads-matter---a-head-level-kv-cache-compression-method-with-integrated-retrieval-and-reasoning---iclr25/">
<link crossorigin="anonymous" href="https://jjl357.github.io/blog/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jjl357.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jjl357.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jjl357.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jjl357.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://jjl357.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh-cn" href="https://jjl357.github.io/blog/posts/not-all-heads-matter---a-head-level-kv-cache-compression-method-with-integrated-retrieval-and-reasoning---iclr25/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><script type="text/javascript"
        async
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<meta property="og:url" content="https://jjl357.github.io/blog/posts/not-all-heads-matter---a-head-level-kv-cache-compression-method-with-integrated-retrieval-and-reasoning---iclr25/">
  <meta property="og:site_name" content="JJ&#39;s Blog">
  <meta property="og:title" content="NOT ALL HEADS MATTER: A HEAD-LEVEL KV CACHE COMPRESSION METHOD WITH INTEGRATED RETRIEVAL AND REASONING">
  <meta property="og:description" content="
Conference: ICLR&#39;25 Github: https://github.com/FYYFU/HeadKV.
My Thoughts è¿™ç¯‡å·¥ä½œå’Œ DuoAttention çš„å…³æ³¨ç‚¹ç±»ä¼¼ï¼Œéƒ½æ˜¯å…³æ³¨ä¸åŒ attention head å¯¹æ¨¡å‹ä¸åŒèƒ½åŠ›çš„è´¡çŒ®ä¸åŒï¼Œè¿™ç¯‡å·¥ä½œæ›´å…³æ³¨ attention head å¯¹æ¨¡å‹ Retrieval ä¸ Reasoning çš„ importance score, æ¥å®ç° KV Cache çš„ nonuniform budget allocation ã€‚
1. Motivation ç°ä»£ LLM è¶Šæ¥è¶Šæ”¯æŒæé•¿ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ GPT-4ã€Llama-3ã€Qwen-2ã€Claude ç­‰ï¼‰ï¼Œä½†éšç€è¾“å…¥é•¿åº¦å¢é•¿ï¼ŒTransformer çš„ self-attention å¯¼è‡´ KV cacheï¼ˆattention çš„ key/value çŠ¶æ€ï¼‰å ç”¨å†…å­˜çº¿æ€§å¢é•¿ï¼Œæˆä¸ºæ¨ç†é˜¶æ®µçš„ä¸»è¦ç“¶é¢ˆã€‚å·²æœ‰å·¥ä½œé€šè¿‡ token eviction / å±‚çº§ç¼“å­˜å‹ç¼©æ¥ç¼“è§£ï¼Œä½†å‡ ä¹æ²¡æœ‰ç ”ç©¶åœ¨â€œå¤´ï¼ˆheadï¼‰çº§åˆ«â€ä¸Šå¯¹ KV cache å¤§å°è¿›è¡Œå·®å¼‚åŒ–åˆ†é…ã€‚ä½œè€…è§‚å¯Ÿåˆ° attention heads åœ¨åŠŸèƒ½ä¸Šé«˜åº¦å¼‚è´¨ï¼ˆå¦‚ retrieval headsã€reasoning heads ç­‰ï¼‰ï¼Œå› æ­¤æå‡ºåŸºäºå¤´é‡è¦æ€§çš„ head-level KV cache å‹ç¼©æ–¹æ³•ï¼ˆHeadKVï¼‰ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºç»“åˆæ£€ç´¢ä¸æ¨ç†èƒ½åŠ›è¯„ä¼°çš„ HeadKV-R2ã€‚
2. Relative Work 2.1 Attention heads å›é¡¾äº†å¯¹å¤šå¤´æ³¨æ„åŠ›ä¸­ head åŠŸèƒ½çš„ç ”ç©¶ï¼ˆVoita et al., Olsson et al., Wu et al., Zheng et al. ç­‰ï¼‰ï¼Œå¹¶æŒ‡å‡ºä¸åŒ head åœ¨è¯æ³•ã€ç»“æ„ã€å¤åˆ¶ï¼ˆinductionï¼‰ã€æ£€ç´¢ç­‰æ–¹é¢æ‰®æ¼”ä¸åŒè§’è‰²ã€‚è¿™äº›è§‚å¯Ÿä¸ºæŒ‰ head åˆ†é… KV cache æä¾›ç†è®ºåŸºç¡€ã€‚">
  <meta property="og:locale" content="zh-cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-31T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-31T00:00:00+00:00">
    <meta property="article:tag" content="Paper Note">
    <meta property="article:tag" content="KV Cache">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NOT ALL HEADS MATTER: A HEAD-LEVEL KV CACHE COMPRESSION METHOD WITH INTEGRATED RETRIEVAL AND REASONING">
<meta name="twitter:description" content="
Conference: ICLR&#39;25
Github: https://github.com/FYYFU/HeadKV.

My Thoughts
è¿™ç¯‡å·¥ä½œå’Œ DuoAttention çš„å…³æ³¨ç‚¹ç±»ä¼¼ï¼Œéƒ½æ˜¯å…³æ³¨ä¸åŒ attention head å¯¹æ¨¡å‹ä¸åŒèƒ½åŠ›çš„è´¡çŒ®ä¸åŒï¼Œè¿™ç¯‡å·¥ä½œæ›´å…³æ³¨ attention head å¯¹æ¨¡å‹ Retrieval ä¸ Reasoning çš„ importance score, æ¥å®ç° KV Cache çš„ nonuniform budget allocation ã€‚
1. Motivation
ç°ä»£ LLM è¶Šæ¥è¶Šæ”¯æŒæé•¿ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ GPT-4ã€Llama-3ã€Qwen-2ã€Claude ç­‰ï¼‰ï¼Œä½†éšç€è¾“å…¥é•¿åº¦å¢é•¿ï¼ŒTransformer çš„ self-attention å¯¼è‡´ KV cacheï¼ˆattention çš„ key/value çŠ¶æ€ï¼‰å ç”¨å†…å­˜çº¿æ€§å¢é•¿ï¼Œæˆä¸ºæ¨ç†é˜¶æ®µçš„ä¸»è¦ç“¶é¢ˆã€‚å·²æœ‰å·¥ä½œé€šè¿‡ token eviction / å±‚çº§ç¼“å­˜å‹ç¼©æ¥ç¼“è§£ï¼Œä½†å‡ ä¹æ²¡æœ‰ç ”ç©¶åœ¨â€œå¤´ï¼ˆheadï¼‰çº§åˆ«â€ä¸Šå¯¹ KV cache å¤§å°è¿›è¡Œå·®å¼‚åŒ–åˆ†é…ã€‚ä½œè€…è§‚å¯Ÿåˆ° attention heads åœ¨åŠŸèƒ½ä¸Šé«˜åº¦å¼‚è´¨ï¼ˆå¦‚ retrieval headsã€reasoning heads ç­‰ï¼‰ï¼Œå› æ­¤æå‡ºåŸºäºå¤´é‡è¦æ€§çš„ head-level KV cache å‹ç¼©æ–¹æ³•ï¼ˆHeadKVï¼‰ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºç»“åˆæ£€ç´¢ä¸æ¨ç†èƒ½åŠ›è¯„ä¼°çš„ HeadKV-R2ã€‚

2. Relative Work
2.1 Attention heads
å›é¡¾äº†å¯¹å¤šå¤´æ³¨æ„åŠ›ä¸­ head åŠŸèƒ½çš„ç ”ç©¶ï¼ˆVoita et al., Olsson et al., Wu et al., Zheng et al. ç­‰ï¼‰ï¼Œå¹¶æŒ‡å‡ºä¸åŒ head åœ¨è¯æ³•ã€ç»“æ„ã€å¤åˆ¶ï¼ˆinductionï¼‰ã€æ£€ç´¢ç­‰æ–¹é¢æ‰®æ¼”ä¸åŒè§’è‰²ã€‚è¿™äº›è§‚å¯Ÿä¸ºæŒ‰ head åˆ†é… KV cache æä¾›ç†è®ºåŸºç¡€ã€‚">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jjl357.github.io/blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "NOT ALL HEADS MATTER: A HEAD-LEVEL KV CACHE COMPRESSION METHOD WITH INTEGRATED RETRIEVAL AND REASONING",
      "item": "https://jjl357.github.io/blog/posts/not-all-heads-matter---a-head-level-kv-cache-compression-method-with-integrated-retrieval-and-reasoning---iclr25/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "NOT ALL HEADS MATTER: A HEAD-LEVEL KV CACHE COMPRESSION METHOD WITH INTEGRATED RETRIEVAL AND REASONING",
  "name": "NOT ALL HEADS MATTER: A HEAD-LEVEL KV CACHE COMPRESSION METHOD WITH INTEGRATED RETRIEVAL AND REASONING",
  "description": "\nConference: ICLR'25 Github: https://github.com/FYYFU/HeadKV.\nMy Thoughts è¿™ç¯‡å·¥ä½œå’Œ DuoAttention çš„å…³æ³¨ç‚¹ç±»ä¼¼ï¼Œéƒ½æ˜¯å…³æ³¨ä¸åŒ attention head å¯¹æ¨¡å‹ä¸åŒèƒ½åŠ›çš„è´¡çŒ®ä¸åŒï¼Œè¿™ç¯‡å·¥ä½œæ›´å…³æ³¨ attention head å¯¹æ¨¡å‹ Retrieval ä¸ Reasoning çš„ importance score, æ¥å®ç° KV Cache çš„ nonuniform budget allocation ã€‚\n1. Motivation ç°ä»£ LLM è¶Šæ¥è¶Šæ”¯æŒæé•¿ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ GPT-4ã€Llama-3ã€Qwen-2ã€Claude ç­‰ï¼‰ï¼Œä½†éšç€è¾“å…¥é•¿åº¦å¢é•¿ï¼ŒTransformer çš„ self-attention å¯¼è‡´ KV cacheï¼ˆattention çš„ key/value çŠ¶æ€ï¼‰å ç”¨å†…å­˜çº¿æ€§å¢é•¿ï¼Œæˆä¸ºæ¨ç†é˜¶æ®µçš„ä¸»è¦ç“¶é¢ˆã€‚å·²æœ‰å·¥ä½œé€šè¿‡ token eviction / å±‚çº§ç¼“å­˜å‹ç¼©æ¥ç¼“è§£ï¼Œä½†å‡ ä¹æ²¡æœ‰ç ”ç©¶åœ¨â€œå¤´ï¼ˆheadï¼‰çº§åˆ«â€ä¸Šå¯¹ KV cache å¤§å°è¿›è¡Œå·®å¼‚åŒ–åˆ†é…ã€‚ä½œè€…è§‚å¯Ÿåˆ° attention heads åœ¨åŠŸèƒ½ä¸Šé«˜åº¦å¼‚è´¨ï¼ˆå¦‚ retrieval headsã€reasoning heads ç­‰ï¼‰ï¼Œå› æ­¤æå‡ºåŸºäºå¤´é‡è¦æ€§çš„ head-level KV cache å‹ç¼©æ–¹æ³•ï¼ˆHeadKVï¼‰ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºç»“åˆæ£€ç´¢ä¸æ¨ç†èƒ½åŠ›è¯„ä¼°çš„ HeadKV-R2ã€‚\n2. Relative Work 2.1 Attention heads å›é¡¾äº†å¯¹å¤šå¤´æ³¨æ„åŠ›ä¸­ head åŠŸèƒ½çš„ç ”ç©¶ï¼ˆVoita et al., Olsson et al., Wu et al., Zheng et al. ç­‰ï¼‰ï¼Œå¹¶æŒ‡å‡ºä¸åŒ head åœ¨è¯æ³•ã€ç»“æ„ã€å¤åˆ¶ï¼ˆinductionï¼‰ã€æ£€ç´¢ç­‰æ–¹é¢æ‰®æ¼”ä¸åŒè§’è‰²ã€‚è¿™äº›è§‚å¯Ÿä¸ºæŒ‰ head åˆ†é… KV cache æä¾›ç†è®ºåŸºç¡€ã€‚\n",
  "keywords": [
    "Paper Note", "KV Cache"
  ],
  "articleBody": "\nConference: ICLR'25 Github: https://github.com/FYYFU/HeadKV.\nMy Thoughts è¿™ç¯‡å·¥ä½œå’Œ DuoAttention çš„å…³æ³¨ç‚¹ç±»ä¼¼ï¼Œéƒ½æ˜¯å…³æ³¨ä¸åŒ attention head å¯¹æ¨¡å‹ä¸åŒèƒ½åŠ›çš„è´¡çŒ®ä¸åŒï¼Œè¿™ç¯‡å·¥ä½œæ›´å…³æ³¨ attention head å¯¹æ¨¡å‹ Retrieval ä¸ Reasoning çš„ importance score, æ¥å®ç° KV Cache çš„ nonuniform budget allocation ã€‚\n1. Motivation ç°ä»£ LLM è¶Šæ¥è¶Šæ”¯æŒæé•¿ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ GPT-4ã€Llama-3ã€Qwen-2ã€Claude ç­‰ï¼‰ï¼Œä½†éšç€è¾“å…¥é•¿åº¦å¢é•¿ï¼ŒTransformer çš„ self-attention å¯¼è‡´ KV cacheï¼ˆattention çš„ key/value çŠ¶æ€ï¼‰å ç”¨å†…å­˜çº¿æ€§å¢é•¿ï¼Œæˆä¸ºæ¨ç†é˜¶æ®µçš„ä¸»è¦ç“¶é¢ˆã€‚å·²æœ‰å·¥ä½œé€šè¿‡ token eviction / å±‚çº§ç¼“å­˜å‹ç¼©æ¥ç¼“è§£ï¼Œä½†å‡ ä¹æ²¡æœ‰ç ”ç©¶åœ¨â€œå¤´ï¼ˆheadï¼‰çº§åˆ«â€ä¸Šå¯¹ KV cache å¤§å°è¿›è¡Œå·®å¼‚åŒ–åˆ†é…ã€‚ä½œè€…è§‚å¯Ÿåˆ° attention heads åœ¨åŠŸèƒ½ä¸Šé«˜åº¦å¼‚è´¨ï¼ˆå¦‚ retrieval headsã€reasoning heads ç­‰ï¼‰ï¼Œå› æ­¤æå‡ºåŸºäºå¤´é‡è¦æ€§çš„ head-level KV cache å‹ç¼©æ–¹æ³•ï¼ˆHeadKVï¼‰ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºç»“åˆæ£€ç´¢ä¸æ¨ç†èƒ½åŠ›è¯„ä¼°çš„ HeadKV-R2ã€‚\n2. Relative Work 2.1 Attention heads å›é¡¾äº†å¯¹å¤šå¤´æ³¨æ„åŠ›ä¸­ head åŠŸèƒ½çš„ç ”ç©¶ï¼ˆVoita et al., Olsson et al., Wu et al., Zheng et al. ç­‰ï¼‰ï¼Œå¹¶æŒ‡å‡ºä¸åŒ head åœ¨è¯æ³•ã€ç»“æ„ã€å¤åˆ¶ï¼ˆinductionï¼‰ã€æ£€ç´¢ç­‰æ–¹é¢æ‰®æ¼”ä¸åŒè§’è‰²ã€‚è¿™äº›è§‚å¯Ÿä¸ºæŒ‰ head åˆ†é… KV cache æä¾›ç†è®ºåŸºç¡€ã€‚\n2.2 KV cache compression StreamingLLMï¼ˆattention sinkï¼‰ã€Heavy-Hitterï¼PyramidKVã€SnapKVã€Ada-KV ç­‰ï¼›å¹¶æŒ‡å‡ºç°æœ‰æ–¹æ³•é€šå¸¸ä»¥ layer ä¸ºå•ä½åˆ†é…é¢„ç®—æˆ–åœ¨ layer å†…éƒ¨åŠ¨æ€åˆ†é…ï¼Œä½†å¹¶æœªå®Œå…¨æ‘†è„± layer çº¦æŸï¼ˆå› æ­¤éš¾ä»¥å®ç°çœŸæ­£çš„ head-level å‹ç¼©ï¼‰ã€‚æœ¬å·¥ä½œé€‰æ‹©å®Œå…¨åŸºäº head çš„é‡è¦æ€§åˆ†å¸ƒç‹¬ç«‹åˆ†é…ç¼“å­˜é¢„ç®—ï¼Œä»¥æœŸæ›´ç²¾å‡†ä¿ç•™å…³é”®ä¿¡æ¯ã€‚\n3. Method 3.1 Head-Level Importance Score Estimation ç›®æ ‡ï¼šä¸ºæ¯ä¸ª attention head è®¡ç®—ä¸€ä¸ªåæ˜ å…¶åœ¨â€œæ£€ç´¢ + æ¨ç†ï¼ˆretrieval + reasoningï¼‰â€ä»»åŠ¡ä¸­é‡è¦æ€§çš„åˆ†æ•° $S_h$ï¼Œç”¨äºåç»­æŒ‰æ¯”ä¾‹åˆ†é… KV cacheã€‚\nåŠ¨æœºç»†åŒ–ï¼š\nç›´æ¥ä½¿ç”¨ Wu et al. (2024) çš„ Needle-in-a-Haystackï¼ˆç²¾ç¡®åŒ¹é…æ£€ç´¢ï¼‰æµ‹è¯•ä¼šäº§ç”Ÿæä¸ºç¨€ç–çš„åˆ†å¸ƒï¼ˆâ‰ˆ70% å¤´å¾—åˆ†ä¸º 0ï¼‰ï¼Œå› ä¸ºå…¶ä¾èµ–äº exact-match çš„ argmax è§„åˆ™ï¼Œè¿‡äºè‹›åˆ»ï¼Œä¸åˆ©äºåˆ†å¸ƒå¼é¢„ç®—åˆ†é…ã€‚ ä¸ºäº†åŒæ—¶æ•æ‰ æ£€ç´¢èƒ½åŠ›ï¼ˆèƒ½æŠŠç­”æ¡ˆç‰‡æ®µ k ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ï¼‰å’Œ æ¨ç†èƒ½åŠ›ï¼ˆéœ€è¦æŒ‰ç»™å®šæ¨ç†æ­¥éª¤é€‰æ‹©æ­£ç¡®ç­”æ¡ˆï¼‰ï¼Œä½œè€…å°† needle æ”¹é€ æˆä¸‰æ®µå¼ $k=(r,c_1,c_2)$ï¼šå…¶ä¸­ $r$ æ˜¯æ˜¾å¼çš„ reasoning stepï¼Œ$c_1$ æ˜¯è¯±å¯¼çš„é”™è¯¯ç­”æ¡ˆï¼Œ$c_2$ æ˜¯æ­£ç¡®ç­”æ¡ˆï¼›æ¨¡å‹å¿…é¡»å€ŸåŠ© $r$ æ¥æ¨ç†å¹¶è¾“å‡º $c_2$ã€‚è¿™ç§æ„é€ èƒ½æ¿€æ´»æ—¢ç”¨äºå®šä½ä¹Ÿç”¨äºé€»è¾‘æ¨ç†çš„å¤´ã€‚ å…¬å¼ä¸è®¡ç®—æ­¥éª¤ï¼š\nè®ºæ–‡ç»™å‡ºä¸¤ç±»è¯„åˆ†ï¼šåŸå§‹ Retrievalï¼ˆEq.1ï¼‰ä¸æ–°çš„ Retrieval-Reasoningï¼ˆEq.2ï¼‰ã€‚\nRetrievalï¼ˆWu et al.ï¼‰ $$ S_h = \\sum_{t=1}^{N} N_t, \\quad N_t = \\begin{cases} \\dfrac{1}{N}, \u0026 \\text{if } \\arg\\max(a^t_h) \\in k [2mm] 0, \u0026 \\text{otherwise} \\end{cases} \\tag{1} $$\nå…¶ä¸­ $a^t_h$ æ˜¯ head $h$ åœ¨ decoding step $t$ å¯¹åˆå¹¶è¾“å…¥ï¼ˆneedle + haystackï¼‰çš„ attention åˆ†æ•°å‘é‡ï¼Œ$N$ æ˜¯ needle é•¿åº¦ã€‚æ­¤æ³•ä¾èµ– argmax æ˜¯å¦è½åœ¨ needle ä¸Šï¼ˆå³æ˜¯å¦ exact matchï¼‰ã€‚\nRetrieval-Reasoningï¼ˆä½œè€…æå‡ºï¼‰ ä¸ºäº†ä¸ä¸¢å¤± token ä»¥å¤–çš„è´¡çŒ®ï¼Œä½œè€…æŠŠâ€œåªçœ‹ argmaxâ€æ‰©å±•ä¸ºå¯¹ top-i attention çš„åŠ æƒæ±‚å’Œï¼Œå¹¶ä»¥æ­£ç¡®ç­”æ¡ˆ $c_2$ çš„æ‰€æœ‰ tokens ä¸ºå…³æ³¨å¯¹è±¡ï¼š\n$$ S_h = \\sum_{t=1}^N \\sum_{i=1}^N M^t_i, \\quad M^t_i = \\begin{cases} \\dfrac{a_i}{N}, \u0026 \\text{if } \\text{top-i}(a^t_h)\\in c_2 [1mm] 0, \u0026 \\text{otherwise} \\end{cases} \\tag{2} $$\nå…¶ä¸­ $a_i$ æ˜¯ head $h$ åœ¨ step $t$ çš„ç¬¬ $i$ å¤§ attention å€¼ï¼›$\\text{top-i}(a^t_h)$ æ˜¯å¯¹åº”çš„ tokenï¼ˆç¬¬ $i$ é«˜ attention æŒ‡å‘çš„ tokenï¼‰ã€‚ ç›´è§‚ä¸Šï¼Œè‹¥ä¸€ä¸ª head åœ¨å¤šä¸ª decoding step å¯¹æ­£ç¡®ç­”æ¡ˆçš„ token ç»™å‡ºé«˜ attentionï¼Œåˆ™è¯¥ head å¾—åˆ†è¾ƒé«˜ï¼›é€šè¿‡å¯¹ $a_i$ çš„åŠ æƒï¼Œè¯„åˆ†æ›´å¹³æ»‘ä¸”èƒ½åæ˜ â€œéƒ¨åˆ†æ³¨æ„åŠ›â€è´¡çŒ®ã€‚\nå®ç°ç»†èŠ‚ï¼ˆè¡¥å……ï¼‰ï¼š\nåœ¨å®é™…å®ç°ä¸­ï¼Œä½œè€…å°† needle æ’å…¥ä¸åŒä½ç½® $p_i$ï¼ˆä¸åŒçš„ haystack ä½ç½®ï¼‰ä»¥ä¿è¯åˆ†å¸ƒç¨³å¥ã€‚å¯¹æ¯ä¸ªæ’å…¥ä½ç½®è·‘è‹¥å¹²æ ·æœ¬ï¼Œå†å¯¹åŒä¸€ head çš„å¾—åˆ†å–å¹³å‡ï¼ˆè®ºæ–‡ pseudo-code ä¸­æœ‰è¯»å–å¹¶å¹³å‡çš„å®ç°ï¼‰ã€‚ å¯¹ $S_h$ åš $L_1$-å½’ä¸€åŒ–ï¼ˆsum åˆ° 1ï¼‰ï¼Œå¾—åˆ° head é‡è¦æ€§åˆ†å¸ƒå‘é‡ $\\mathbf{S}$ï¼Œä»¥ä¾¿ç”¨äºæŒ‰æ¯”ä¾‹åˆ†é…åŠ¨æ€æ± é¢„ç®—ï¼ˆè§ 3.2ï¼‰ã€‚ è®¾è®¡å– top-i çš„ä¸Šé™ $i_\\text{max}$ï¼Œä»¥åŠæ˜¯å¦å¯¹ä¸åŒ decoding step èµ‹ä¸åŒæƒé‡ï¼Œè¿™äº›åœ¨è®ºæ–‡å®ç°ä¸­é€šè¿‡å®éªŒé€‰æ‹©ï¼ˆé»˜è®¤æ‰©å¤§äº† i çš„æ•°é‡ï¼Œç›¸æ¯” Wu et al. æ›´å®½æ¾ï¼‰ã€‚è¯¦è§é™„å½• pseudo-codeã€‚ ç›´è§‚æ€»ç»“ï¼šR2ï¼ˆRetrieval-Reasoningï¼‰è¯„åˆ†æŠŠ head å¯¹äº**æ•´æ®µæ­£ç¡®ç­”æ¡ˆï¼ˆè€Œéå• tokenï¼‰**çš„æŒç»­æ³¨æ„åŠ›éƒ½è€ƒè™‘è¿›æ¥ï¼Œå› æ­¤è¯„åˆ†å¯†åº¦æ›´é«˜ã€æ›´èƒ½åŒºåˆ†â€œè´Ÿè´£æ£€ç´¢â€ä¸â€œè´Ÿè´£æ¨ç†â€çš„å¤´ï¼Œå¹¶ä¸”æ›´é€‚åˆé©±åŠ¨èµ„æºæœ‰é™æ—¶çš„é¢„ç®—åˆ†é…ã€‚è®ºæ–‡å¯è§†åŒ–ä¹Ÿæ˜¾ç¤º R2 åˆ†å¸ƒæ›´ denseï¼Œè€ŒåŸå§‹ retrieval åˆ†å¸ƒè¾ƒ sparseã€‚\n3.2 Head-Level KV Cache Allocation æ ¸å¿ƒæ€æƒ³ï¼šåŸºäºå¾—åˆ°çš„ head é‡è¦æ€§åˆ†å¸ƒ ${S_h}$ å¯¹æ¯ä¸ª head åˆ†é…ä¸åŒçš„ KV cache å¤§å° $b_h$ï¼ŒåŒæ—¶ä¿ç•™ä¸€ä¸ªã€Œå…±äº«åŠ¨æ€æ± ã€ $B$ ç”¨äºæŠŠé¢„ç®—ä»å¼±å¤´æ”¶é›†å¹¶æŒ‰æƒé‡ç»™å¼ºå¤´ã€‚\nå…¬å¼ï¼š\n$$ B = \\frac{b}{\\beta} \\cdot L \\cdot H, \\qquad b_h = \\Big(b - \\frac{b}{\\beta}\\Big) + S_h \\cdot B \\tag{4} $$\nè§£é‡Šï¼š\n$b$ï¼šæ¯ä¸ª head çš„åˆå§‹å›ºå®šé¢„ç®—ï¼ˆbaseline budgetï¼‰ï¼› $\\beta$ï¼šè¶…å‚æ•°æ§åˆ¶å…±äº«æ± å¤§å°ï¼›è¾ƒå°çš„ $\\beta$ â‡’ æ›´å¤§çš„å…±äº«æ± ï¼ˆæ›´å¤šé¢„ç®—å¯è¢«é‡æ–°åˆ†é…ï¼‰ï¼› $L$ã€$H$ï¼šæ¨¡å‹çš„å±‚æ•°ä¸æ¯å±‚ head æ•°ï¼ˆå› æ­¤ $L\\times H$ æ˜¯æ€» head æ•°ï¼‰ï¼› æœ€ç»ˆ $b_h$ ç”± â€œä¿è¯çš„æœ€å°é¢„ç®—â€ $\\big(b - b/\\beta\\big)$ ä¸æŒ‰é‡è¦æ€§åˆ†é…çš„åŠ¨æ€é¢„ç®— $S_h \\cdot B$ ä¹‹å’Œæ„æˆã€‚ å®ç°ç»†èŠ‚ï¼ˆPseudo codeï¼‰ï¼š\nå…ˆä¿ç•™æœ€å $\\alpha$ ä¸ª instruction tokenï¼ˆlocal windowï¼‰ï¼Œè¿™äº› token åœ¨å½¢æˆåŠ¨æ€æ± å‰å¿…é¡»ä¿è¯å…¥æ± å‰è¢«ä¿ç•™ï¼Œç”¨äºæŒ‡å¯¼ selectionï¼ˆè§ 3.3ï¼‰ã€‚è®ºæ–‡é»˜è®¤ $\\alpha=8$ã€‚\nè®ºæ–‡å®ç°ï¼ˆpseudo-codeï¼‰ä¸­çš„æ­¥éª¤ï¼š\nè¯»å–å¹¶å¹³å‡ä¿å­˜å¥½çš„ head å¾—åˆ†åˆ—è¡¨ï¼Œå½’ä¸€åŒ–å¾—åˆ° $\\mathbf{S}$ã€‚ è®¡ç®— $$ \\text{total_pool_capacity} = \\frac{\\text{base_capacity}}{\\beta} \\cdot \\text{num_hidden_layers} \\cdot \\text{num_attention_heads}, \\quad \\text{min_num} = \\text{base_capacity} - \\frac{\\text{base_capacity}}{\\beta} $$ 3. $$ \\text{head_capacity} = \\text{round}(\\text{total_attention} \\cdot \\text{total_pool_capacity} + \\text{min_num}) $$ ï¼ˆå‘æœ€è¿‘æ•´æ•°å–æ•´å¾—åˆ°æ¯ä¸ª head çš„æœ€ç»ˆæ¡ç›®æ•°ï¼Œç”¨äº gather æ“ä½œï¼‰ã€‚\n3.3 KV Cache Selection ç›®æ ‡ï¼šåœ¨ç¡®å®šæ¯ä¸ª head ä¿ç•™çš„æ¡ç›®æ•° (b_h) åï¼Œå¦‚ä½•ä»è¯¥ head çš„å†å² key/value åˆ—è¡¨ä¸­é€‰å–å…·ä½“çš„ç´¢å¼•ï¼ˆtokensï¼‰ä»¥æ„æˆ compressed KV cacheã€‚\næ–¹æ³•ï¼ˆåŸºäº SnapKVï¼‰ï¼š\nä¿ç•™æœ€å (\\alpha) ä¸ª instruction tokensï¼ˆlocal observation windowï¼‰ï¼› è®¡ç®— local windowï¼ˆqueryï¼‰åˆ°æ¯ä¸ªå€™é€‰ tokenï¼ˆkeyï¼‰çš„ attention scoreï¼ˆé€šå¸¸ä¸ºç‚¹ç§¯æˆ– softmax å½’ä¸€åŒ–åçš„åˆ†æ•°ï¼‰ï¼› å¯¹æ¯ä¸ª headï¼Œå°†è¿™äº› attention åˆ†æ•°è¿›è¡Œ pooling/èšåˆï¼ˆè®ºæ–‡ä¸­æåˆ° pooling å±‚ç”¨äºèšåˆæ¥è‡ª local window çš„ attentionï¼‰ï¼Œå¹¶å°†å¾—åˆ†æ’åºï¼› é€‰å–å¾—åˆ†æœ€é«˜çš„å‰ (b_h) ä¸ª token ç´¢å¼•ä½œä¸ºè¯¥ head çš„ç¼“å­˜æ¡ç›®ã€‚æœ€åå°†è¿™äº›è¢«é€‰å‡ºçš„ key/value ä¸ local window çš„æœ€è¿‘ tokens åˆå¹¶ï¼ˆconcatenateï¼‰ï¼Œæ„æˆæœ€ç»ˆæ¯ä¸ª head çš„ compressed KVã€‚ Pseudo-PyTorch æ ·ä¾‹ï¼ˆè®ºæ–‡ Listing æ”¹å†™å¹¶æ³¨é‡Šè¦ç‚¹ï¼‰ï¼š\n# ä¼ªä»£ç è¦ç‚¹ï¼ˆåŸºäºè®ºæ–‡ Listingï¼‰ # å‡è®¾ï¼šorigin_heads_key_states: [num_heads, batch, seq_len, head_dim] # origin_heads_value_states: ç±»ä¼¼ # head_capacity[layer_idx][head_idx] å·²ç”± obtain_head_budget å¾—åˆ° heads_key_states = [] heads_value_states = [] # 1. è®¡ç®— local window -\u003e æ‰€æœ‰ tokens çš„ attention scoreï¼ˆåŒ SnapKVï¼‰ attn_score = calc_attn_score(query_states, key_states) # shape: [num_heads, seq_len] # 2. æ’åºå¾—åˆ°ç´¢å¼•ï¼ˆæŒ‰å¾—åˆ†é™åºï¼‰ _, indices = attn_score.sort(dim=-1, descending=True) # 3. å¯¹æ¯ä¸ª head æŒ‰ head_capacity å– top-k ç´¢å¼•å¹¶ gather key/value for head_idx in range(num_heads): k = head_capacity[layer_idx][head_idx] # è¯¥ head æœ€ç»ˆä¿ç•™æ•° cache_index = indices[head_idx, :k] # å– top-k ç´¢å¼• # æ‰©å±•ç´¢å¼•åˆ° head_dim ç”¨äº gather cache_index = cache_index.view(1,1,-1,1).expand(-1,-1,-1,head_dim) top_Kcache = origin_heads_key_states[head_idx].gather(dim=2, index=cache_index) top_Vcache = origin_heads_value_states[head_idx].gather(dim=2, index=cache_index) # ä¸ local window çš„ last self.window_size tokens åˆå¹¶ selected_k = torch.cat([top_Kcache, origin_heads_key_states[head_idx][:,:, -self.window_size:, :]], dim=2) selected_v = torch.cat([top_Vcache, origin_heads_value_states[head_idx][:,:, -self.window_size:, :]], dim=2) heads_key_states.append(selected_k.view(-1, head_dim)) heads_value_states.append(selected_v.view(-1, head_dim)) # æœ€ç»ˆåˆå¹¶æ‰€æœ‰ head çš„ key/value heads_key_states = torch.cat(heads_key_states, dim=0) heads_value_states = torch.cat(heads_value_states, dim=0) è¯¥æµç¨‹åœ¨è®ºæ–‡å®ç°ï¼ˆListingï¼‰ä¸­ç»™å‡ºï¼Œå®é™…å®ç°è¿˜åŒ…å«ä¿å­˜ä¸åŠ è½½ importance distributionã€layer-by-layer çš„å¤„ç†ä¸å¼ é‡å¯¹é½ç»†èŠ‚ã€‚\nè¡¥å……è¯´æ˜ï¼š\nåˆå¹¶ local window çš„ç›®çš„æ˜¯ä¿è¯æœ€è¿‘çš„æŒ‡ä»¤ä¿¡æ¯ï¼ˆinstruction tokensï¼‰ä¸€å®šè¢«ä¿ç•™ï¼Œè¿™å¯¹ç”Ÿæˆè´¨é‡å’Œ selection æŒ‡å¯¼å¾ˆé‡è¦ï¼› selection ä½¿ç”¨ attention pooling è€Œä¸æ˜¯ç®€å•çš„ token TF/IDF ç­‰å¯å‘å¼ï¼Œæ˜¯å› ä¸º attention èƒ½åæ˜ æ¨¡å‹å½“å‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„å†…éƒ¨ä¿¡å·ï¼ˆå³â€œæ¨¡å‹çŸ¥é“è‡ªå·±åœ¨çœ‹ä»€ä¹ˆâ€ï¼‰ï¼Œè¿™ä¹Ÿæ˜¯ SnapKV çš„æ ¸å¿ƒæ€æƒ³ã€‚ 4. Experiments and Analysis 4.1 Experiment settings Backbone Modelsï¼šLlama-3-8B-Instructã€Mistral-7B-Instructã€‚\nBenchmarks / Datasetsï¼šLongBenchï¼ˆSingle-Doc QAã€Multi-Doc QA ç±»åˆ«ï¼‰ä¸ LooGLEï¼ˆLong Dependency QA ç±»åˆ«ï¼‰ã€‚è®ºæ–‡é™„å½•ç»™å‡ºå„æ•°æ®é›†çš„å…·ä½“æ ·æœ¬ä¸å¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦ç­‰ï¼ˆAppendix Table 5ï¼‰ã€‚\nBaselinesï¼š\nSnapKVï¼šä»¥æœ€å (\\alpha) ä¸ª tokens æŒ‡å¯¼ selectionï¼ˆattention poolingï¼‰ã€‚ PyramidKVï¼šæŒ‰å±‚é‡‘å­—å¡”åˆ†é…ï¼Œæ›´ä½å±‚ç»™æ›´å¤šç¼“å­˜ã€‚ Ada-KV / Ada-SnapKVï¼šåœ¨å±‚å†…åŸºäº concentration åŠ¨æ€åˆ†é…ï¼ˆè®ºæ–‡é‡‡ç”¨ Ada-SnapKV ä½œä¸ºæœ€å¼º baselineï¼‰ã€‚ ç»Ÿä¸€è®¾ç½®ï¼šlocal window size (\\alpha = 8)ï¼›è¯„ä¼° KV sizeï¼ˆä¿ç•™æ¡ç›®ï¼‰é›†åˆï¼š({64,128,256,512,1024})ï¼›(\\beta) åœ¨ ({1.005,1.01,1.1,1.2,1.5,2,5,10}) å–æœ€ä¼˜æŠ¥å‘Šï¼›é€‰æ‹© SnapKV ä½œä¸º per-head selection æ–¹æ³•ï¼ˆå³ HeadKV åœ¨åˆ†é…ä¹‹åç”¨ SnapKV åšå…·ä½“æ¡ç›®æŒ‘é€‰ï¼‰ã€‚\n4.2 Main results æ€»ä½“ç»“è®ºï¼šHead-level åˆ†é…ï¼ˆHeadKVï¼‰åœ¨å„ç§ KV size ä¸‹æ™®éä¼˜äº layer-level baselinesï¼Œå°¤å…¶åœ¨ä½èµ„æºï¼ˆKV size=64 æˆ– 128ï¼‰ä¸‹æ”¶ç›Šæœ€æ˜¾è‘—ï¼›è€ŒåŸºäº Retrieval-Reasoningï¼ˆHeadKV-R2ï¼‰çš„åˆ†å¸ƒè¿›ä¸€æ­¥ä¼˜äºåªåŸºäºæ£€ç´¢çš„åˆ†å¸ƒï¼ˆHeadKV-Rï¼‰ã€‚åœ¨æŸäº›è®¾ç½®ä¸‹ï¼ˆLlama-3-8B, KV=1024ï¼‰ï¼ŒHeadKV-R2 çš„å¹³å‡åˆ†ç”šè‡³ç•¥è¶… FullKVï¼ˆ32.95 vs 32.90ï¼‰ï¼Œè¯´æ˜åˆç†å‹ç¼©åœ¨å™ªå£°ï¼å†—ä½™ä¿¡æ¯è¿‡å¤šæ—¶è¿˜èƒ½æŠ‘åˆ¶è´Ÿé¢å½±å“ã€‚è¡¨ 1 ç»™å‡ºå®Œæ•´æ•°å€¼ã€‚ è¡¥å……è§£é‡Š / ç›´è§‚è§£è¯»ï¼š\nlayer-level æ–¹æ³•ï¼ˆSnapKVã€PyramidKVï¼‰å¯¹æ‰€æœ‰å¤´ä¸€è§†åŒä»æˆ–æŒ‰å±‚ç²—ç²’åº¦åˆ†é…ï¼Œæ— æ³•åœ¨â€œå“ªäº› head çœŸæ­£è´Ÿè´£æ‰¾ç­”æ¡ˆæˆ–æ¨ç†â€ä¸Šåšå¾®è°ƒï¼Œå› æ­¤åœ¨ head æ•°ç›®å¤šã€åŠŸèƒ½å¼‚è´¨æ€§å¤§çš„æ¨¡å‹ä¸Šä¼šæµªè´¹å®è´µçš„ç¼“å­˜èµ„æºã€‚HeadKV å°†é¢„ç®—é›†ä¸­åˆ°å°‘æ•°å…³é”®å¤´ï¼Œä»è€Œåœ¨æå°çš„ç¼“å­˜é¢„ç®—ä¸‹ä¿å­˜å…³é”®ä¿¡æ¯ï¼ˆè®ºæ–‡æå‡ºåœ¨ 1.5% KV ä¿ç•™ä¸‹ä»èƒ½è¾¾åˆ° FullKV çš„ 97% æ€§èƒ½ï¼‰ã€‚ 4.3 Retrieval-Reasoning Heads Ablationï¼šä½œè€…æ¯”è¾ƒäº†ä¸‰ç§ head åˆ†å¸ƒï¼šåŸå§‹ Retrievalï¼ˆWu et al. çš„ exact-matchï¼›HeadKV-Rï¼‰ã€Enhanced-Retrievalï¼ˆä¿æŒ retrieval ç¤ºä¾‹ä½†é‡‡ç”¨æ–°çš„å¾—åˆ†ä¼°è®¡æ–¹æ³•ï¼›HeadKV-ERï¼‰ã€Retrieval-Reasoningï¼ˆä½œè€…æå‡ºçš„ R2ï¼›HeadKV-R2ï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼šHeadKV-ER è¾ƒ HeadKV-R æœ‰å°å¹…æå‡ï¼ˆå› ä¸ºæ›´å…³æ³¨æ•´ä¸ª needle è€Œé argmaxï¼‰ï¼Œä½†ä»ä¸å¦‚ HeadKV-R2ï¼ˆå› ä¸º ER ä»ç¼ºå°‘æ¨ç†æ­¥éª¤çš„æ¿€æ´»ä¿¡å·ï¼‰ã€‚è¡¨ 2 ç»™å‡ºå…·ä½“æ•°å€¼ã€‚ æ·±å…¥è¡¥å……ï¼š\nR2 åˆ†å¸ƒçš„å¯†åº¦æ›´å¤§ã€æœ‰æ›´å¥½çš„åŒºåˆ†åº¦ï¼ˆless zero massï¼‰ï¼Œé€‚åˆåœ¨åˆ†é…å…±äº«æ± æ—¶è¿›è¡Œç¨³å®šçš„æ¯”ä¾‹åˆ†é…ï¼›å®éªŒæ˜¾ç¤º R2 åœ¨å¤šä¸ªä»»åŠ¡ä¸Šéƒ½å¸¦æ¥ä¸€è‡´æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ multi-doc / long dependency çš„ QA åœºæ™¯ï¼ˆè¿™äº›åœºæ™¯åŒæ—¶éœ€è¦æ£€ç´¢ä¸å¤šæ­¥æ¨ç†ï¼‰ã€‚ 4.4 Long-context retrieval and reasoning ä½œè€…ä½¿ç”¨ä¸¤ç±»ä¸“é—¨è®¾è®¡çš„ stress-testsï¼š\nNeedle-in-a-Haystackï¼ˆæ£€ç´¢æµ‹è¯•ï¼‰ï¼šåœ¨æµ·é‡æ— å…³æ–‡æœ¬ä¸­æ’å…¥ä¸€ä¸ª needleï¼ˆç­”æ¡ˆç‰‡æ®µï¼‰ï¼Œæ£€æµ‹æ˜¯å¦èƒ½æ£€ç´¢å¹¶ç›´æ¥ paste å‡ºç­”æ¡ˆï¼ˆåå‘çº¯æ£€ç´¢èƒ½åŠ›ï¼‰ã€‚ç»“æœä¸­ HeadKVï¼ˆå°¤å…¶ HeadKV-R2ï¼‰åœ¨ KV=128 æ—¶æ¯”å…¶ä»–å‹ç¼©æ–¹æ³•è¡¨ç°æ›´å¥½ï¼ˆå›¾ 5ï¼‰ã€‚\nReasoning-in-a-Haystackï¼ˆæ¨ç†æµ‹è¯•ï¼‰ï¼šåŸºäº bAbI é£æ ¼çš„ reasoning é—®é¢˜ï¼Œå°† reasoning-needles æ’å…¥åˆ°å¤§ haystack ä¸­ï¼Œæ¨¡å‹éœ€å…ˆæ£€ç´¢åˆ°ç›¸å…³ needlesï¼Œå†åŸºäºå®ƒä»¬è¿›è¡Œå¤šæ­¥æ¨ç†ï¼ˆåå‘æ£€ç´¢ï¼‹æ¨ç†ï¼‰ã€‚åœ¨è¿™ä¸ªæµ‹è¯•ä¸Šï¼ŒHeadKV-R2 çš„ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ï¼ˆTable 3ï¼‰ï¼Œè¯´æ˜ R2 çš„è¯„åˆ†ç¡®å®æ•æ‰åˆ°äº†æ¨ç†ç›¸å…³çš„ headã€‚\nå®šé‡è¦ç‚¹ï¼ˆæ‘˜è‡ªè®ºæ–‡è¡¨æ ¼ä¸å›¾ï¼‰ï¼š\nåœ¨ Llama-3-8B, KV=128 çš„ Reasoning-in-a-Haystack å¹³å‡åˆ†ï¼šFullKV çº¦ 57.04ï¼ŒHeadKV-R2 çº¦ 56.84ï¼ˆæ¥è¿‘ FullKVï¼‰ï¼Œè€Œ SnapKVã€PyramidKV ç­‰æ˜æ˜¾è½åï¼ˆè§ Table 3ï¼‰ã€‚è¿™è¡¨æ˜åœ¨å—é™ KV ä¸‹ HeadKV-R2 èƒ½æ˜¾è‘—ä¿ç•™ reasoning èƒ½åŠ›ã€‚ 4.5 Memory \u0026 Latency è®¾ç½®ä¸ç»“è®ºï¼š\nä½¿ç”¨ Mistral-7B-Instructã€æœ€å¤§åºåˆ—é•¿åº¦ 32Kã€FlashAttention å®ç°ï¼›è¯„ä¼° decoding latencyï¼ˆåŒ…å« prefill ä¸ decodingï¼‰ä¸ peak memoryï¼ˆåœ¨ä¸åŒ context len ä¸ generation len ä¸‹ï¼‰ã€‚è®ºæ–‡å›¾ 6 æ˜¾ç¤º HeadKV åœ¨è§£ç å»¶è¿Ÿä¸Šä¸å…¶ä»–å‹ç¼©æ–¹æ³•åŸºæœ¬æŒå¹³ï¼ŒåŒæ—¶åœ¨ peak memory ä¸Šè¾ƒ FullKV æœ‰æ˜¾è‘—é™ä½ã€‚æ¢è¨€ä¹‹ï¼šHeadKV åœ¨ä¸å¢åŠ è¿è¡Œæ—¶å¼€é”€çš„å‰æä¸‹ï¼Œè¾¾åˆ°äº†æ›´å¥½çš„æ€§èƒ½/è®°å¿†ç‡æŠ˜ä¸­ã€‚ è¡¥å……è¯´æ˜ï¼š\ndecoding latency çš„æ›²çº¿åœ¨ generation length=1 æ—¶æ¥è¿‘ï¼Œè¯´æ˜ prefill çš„é¢å¤–å¼€é”€ï¼ˆåŒ…æ‹¬è®¡ç®— importance scores çš„ç¦»çº¿å¼€é”€ï¼‰è¢«è®¾è®¡ä¸ºåˆå§‹åŒ–é˜¶æ®µæˆ–ä¸€æ¬¡æ€§ç¦»çº¿è®¡ç®—ï¼ˆè®ºæ–‡ä¸­ importance distribution ä¸ºé™æ€å¹¶åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶è½½å…¥ï¼‰ï¼Œå› æ­¤å¯¹åœ¨çº¿æ¨ç†å¼€é”€å½±å“å¾®å°ã€‚Pseudo-code æ˜ç¡®æŒ‡å‡º obtain_head_budget åœ¨åˆå§‹åŒ–æ—¶è¿è¡Œä¸€æ¬¡ã€‚ G. Hyper-parameter analysis è®ºæ–‡é€šè¿‡æ‰«æ (\\beta) å€¼é›†åˆå±•ç¤ºäº† (\\beta) å¯¹æœ€ç»ˆå¹³å‡åˆ†çš„å½±å“ï¼ˆFigure 10/11ï¼‰ã€‚æ€»ä½“ç»“è®ºï¼šè¾ƒå° (\\beta)ï¼ˆæ›´å¤§çš„åŠ¨æ€æ± ï¼‰èƒ½è®© HeadKV-R2 è¿›ä¸€æ­¥è·ç›Šï¼Œè¯´æ˜ R2 åˆ†å¸ƒåœ¨â€œæŠŠæ›´å¤šé¢„ç®—åŠ¨æ€åˆ†é…ç»™é‡è¦å¤´â€æ—¶æ›´å¯é ã€‚è®ºæ–‡åŒæ—¶ä¿æŒ (\\alpha) ä¸å…¶ä»–å‚æ•°ä¸ PyramidKV çš„å®ç°ä¸€è‡´ä»¥ä¿è¯å…¬å¹³æ¯”è¾ƒã€‚ 5. Conclusion \u0026 Future Work è®ºæ–‡ç»“è®ºï¼šæå‡º HeadKV-R2ï¼ˆhead-level KV cache å‹ç¼© + retrieval-reasoning é‡è¦æ€§ä¼°è®¡ï¼‰ï¼Œåœ¨ LongBenchã€LooGLE ç­‰å¤šæ•°æ®é›†ä¸ä¸¤ç§ backbone æ¨¡å‹ä¸Šè¯æ˜äº†åœ¨æé™ç¼“å­˜é¢„ç®—ä¸‹èƒ½ä¿å­˜å¤§éƒ¨åˆ†æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒä½å†…å­˜ä¸å»¶è¿Ÿå¼€é”€ã€‚\n",
  "wordCount" : "940",
  "inLanguage": "zh-cn",
  "datePublished": "2025-10-31T00:00:00Z",
  "dateModified": "2025-10-31T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jjl357.github.io/blog/posts/not-all-heads-matter---a-head-level-kv-cache-compression-method-with-integrated-retrieval-and-reasoning---iclr25/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JJ's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jjl357.github.io/blog/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
  <nav class="nav">
    <div class="logo">
      
      <a href="https://jjl357.github.io/" accesskey="h" title="ğŸ¥› â˜• ğŸµ (Alt + H)">
        <span>ğŸ¥› â˜• ğŸµ</span>
        
      </a>

      <div class="logo-switches">
        <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
          <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      </div>
    </div>
    <ul id="menu">
    </ul>
  </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      NOT ALL HEADS MATTER: A HEAD-LEVEL KV CACHE COMPRESSION METHOD WITH INTEGRATED RETRIEVAL AND REASONING
    </h1>
    <div class="post-meta"><span title='2025-10-31 00:00:00 +0000 UTC'>October 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="https://jjl357.github.io/blog/image/HeadKV/title.png"></p>
<p><strong>Conference:</strong> ICLR'25
<strong>Github:</strong> <a href="https://github.com/FYYFU/HeadKV">https://github.com/FYYFU/HeadKV</a>.</p>
<hr>
<h2 id="my-thoughts">My Thoughts<a hidden class="anchor" aria-hidden="true" href="#my-thoughts">#</a></h2>
<p>è¿™ç¯‡å·¥ä½œå’Œ DuoAttention çš„å…³æ³¨ç‚¹ç±»ä¼¼ï¼Œéƒ½æ˜¯å…³æ³¨ä¸åŒ attention head å¯¹æ¨¡å‹ä¸åŒèƒ½åŠ›çš„è´¡çŒ®ä¸åŒï¼Œè¿™ç¯‡å·¥ä½œæ›´å…³æ³¨ attention head å¯¹æ¨¡å‹ <strong>Retrieval</strong> ä¸ <strong>Reasoning</strong> çš„ importance score, æ¥å®ç° KV Cache çš„ nonuniform budget allocation ã€‚</p>
<h2 id="1-motivation">1. Motivation<a hidden class="anchor" aria-hidden="true" href="#1-motivation">#</a></h2>
<p>ç°ä»£ LLM è¶Šæ¥è¶Šæ”¯æŒæé•¿ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ GPT-4ã€Llama-3ã€Qwen-2ã€Claude ç­‰ï¼‰ï¼Œä½†éšç€è¾“å…¥é•¿åº¦å¢é•¿ï¼ŒTransformer çš„ self-attention å¯¼è‡´ KV cacheï¼ˆattention çš„ key/value çŠ¶æ€ï¼‰å ç”¨å†…å­˜çº¿æ€§å¢é•¿ï¼Œæˆä¸ºæ¨ç†é˜¶æ®µçš„ä¸»è¦ç“¶é¢ˆã€‚å·²æœ‰å·¥ä½œé€šè¿‡ token eviction / å±‚çº§ç¼“å­˜å‹ç¼©æ¥ç¼“è§£ï¼Œä½†<strong>å‡ ä¹æ²¡æœ‰ç ”ç©¶åœ¨â€œå¤´ï¼ˆheadï¼‰çº§åˆ«â€ä¸Šå¯¹ KV cache å¤§å°è¿›è¡Œå·®å¼‚åŒ–åˆ†é…</strong>ã€‚ä½œè€…è§‚å¯Ÿåˆ° attention heads åœ¨åŠŸèƒ½ä¸Šé«˜åº¦å¼‚è´¨ï¼ˆå¦‚ retrieval headsã€reasoning heads ç­‰ï¼‰ï¼Œå› æ­¤æå‡ºåŸºäºå¤´é‡è¦æ€§çš„ head-level KV cache å‹ç¼©æ–¹æ³•ï¼ˆHeadKVï¼‰ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºç»“åˆæ£€ç´¢ä¸æ¨ç†èƒ½åŠ›è¯„ä¼°çš„ HeadKV-R2ã€‚</p>
<hr>
<h2 id="2-relative-work">2. Relative Work<a hidden class="anchor" aria-hidden="true" href="#2-relative-work">#</a></h2>
<h3 id="21-attention-heads">2.1 Attention heads<a hidden class="anchor" aria-hidden="true" href="#21-attention-heads">#</a></h3>
<p>å›é¡¾äº†å¯¹å¤šå¤´æ³¨æ„åŠ›ä¸­ head åŠŸèƒ½çš„ç ”ç©¶ï¼ˆVoita et al., Olsson et al., Wu et al., Zheng et al. ç­‰ï¼‰ï¼Œå¹¶æŒ‡å‡ºä¸åŒ head åœ¨è¯æ³•ã€ç»“æ„ã€å¤åˆ¶ï¼ˆinductionï¼‰ã€æ£€ç´¢ç­‰æ–¹é¢æ‰®æ¼”ä¸åŒè§’è‰²ã€‚è¿™äº›è§‚å¯Ÿä¸ºæŒ‰ head åˆ†é… KV cache æä¾›ç†è®ºåŸºç¡€ã€‚</p>
<h3 id="22-kv-cache-compression">2.2 KV cache compression<a hidden class="anchor" aria-hidden="true" href="#22-kv-cache-compression">#</a></h3>
<p>StreamingLLMï¼ˆattention sinkï¼‰ã€Heavy-Hitterï¼PyramidKVã€SnapKVã€Ada-KV ç­‰ï¼›å¹¶æŒ‡å‡ºç°æœ‰æ–¹æ³•é€šå¸¸ä»¥ layer ä¸ºå•ä½åˆ†é…é¢„ç®—æˆ–åœ¨ layer å†…éƒ¨åŠ¨æ€åˆ†é…ï¼Œä½†å¹¶æœªå®Œå…¨æ‘†è„± layer çº¦æŸï¼ˆå› æ­¤éš¾ä»¥å®ç°çœŸæ­£çš„ head-level å‹ç¼©ï¼‰ã€‚æœ¬å·¥ä½œé€‰æ‹©<strong>å®Œå…¨åŸºäº head çš„é‡è¦æ€§åˆ†å¸ƒç‹¬ç«‹åˆ†é…ç¼“å­˜é¢„ç®—</strong>ï¼Œä»¥æœŸæ›´ç²¾å‡†ä¿ç•™å…³é”®ä¿¡æ¯ã€‚</p>
<hr>
<h2 id="3-method">3. Method<a hidden class="anchor" aria-hidden="true" href="#3-method">#</a></h2>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/HeadKV/figure1.png"></p>
<h3 id="31-head-level-importance-score-estimation">3.1 Head-Level Importance Score Estimation<a hidden class="anchor" aria-hidden="true" href="#31-head-level-importance-score-estimation">#</a></h3>
<p><strong>ç›®æ ‡</strong>ï¼šä¸ºæ¯ä¸ª attention head è®¡ç®—ä¸€ä¸ªåæ˜ å…¶åœ¨â€œæ£€ç´¢ + æ¨ç†ï¼ˆretrieval + reasoningï¼‰â€ä»»åŠ¡ä¸­é‡è¦æ€§çš„åˆ†æ•° $S_h$ï¼Œç”¨äºåç»­æŒ‰æ¯”ä¾‹åˆ†é… KV cacheã€‚</p>
<p><strong>åŠ¨æœºç»†åŒ–</strong>ï¼š</p>
<ul>
<li>ç›´æ¥ä½¿ç”¨ Wu et al. (2024) çš„ Needle-in-a-Haystackï¼ˆç²¾ç¡®åŒ¹é…æ£€ç´¢ï¼‰æµ‹è¯•ä¼šäº§ç”Ÿæä¸ºç¨€ç–çš„åˆ†å¸ƒï¼ˆâ‰ˆ70% å¤´å¾—åˆ†ä¸º 0ï¼‰ï¼Œå› ä¸ºå…¶ä¾èµ–äº exact-match çš„ argmax è§„åˆ™ï¼Œè¿‡äºè‹›åˆ»ï¼Œä¸åˆ©äºåˆ†å¸ƒå¼é¢„ç®—åˆ†é…ã€‚</li>
<li>ä¸ºäº†åŒæ—¶æ•æ‰ <strong>æ£€ç´¢èƒ½åŠ›</strong>ï¼ˆèƒ½æŠŠç­”æ¡ˆç‰‡æ®µ <code>k</code> ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ï¼‰å’Œ <strong>æ¨ç†èƒ½åŠ›</strong>ï¼ˆéœ€è¦æŒ‰ç»™å®šæ¨ç†æ­¥éª¤é€‰æ‹©æ­£ç¡®ç­”æ¡ˆï¼‰ï¼Œä½œè€…å°† needle æ”¹é€ æˆä¸‰æ®µå¼ $k=(r,c_1,c_2)$ï¼šå…¶ä¸­ $r$ æ˜¯æ˜¾å¼çš„ reasoning stepï¼Œ$c_1$ æ˜¯è¯±å¯¼çš„é”™è¯¯ç­”æ¡ˆï¼Œ$c_2$ æ˜¯æ­£ç¡®ç­”æ¡ˆï¼›æ¨¡å‹å¿…é¡»å€ŸåŠ© $r$ æ¥æ¨ç†å¹¶è¾“å‡º $c_2$ã€‚è¿™ç§æ„é€ èƒ½æ¿€æ´»æ—¢ç”¨äºå®šä½ä¹Ÿç”¨äºé€»è¾‘æ¨ç†çš„å¤´ã€‚</li>
</ul>
<hr>
<p><strong>å…¬å¼ä¸è®¡ç®—æ­¥éª¤ï¼š</strong></p>
<p>è®ºæ–‡ç»™å‡ºä¸¤ç±»è¯„åˆ†ï¼šåŸå§‹ Retrievalï¼ˆEq.1ï¼‰ä¸æ–°çš„ Retrieval-Reasoningï¼ˆEq.2ï¼‰ã€‚</p>
<h4 id="retrievalwu-et-al">Retrievalï¼ˆWu et al.ï¼‰<a hidden class="anchor" aria-hidden="true" href="#retrievalwu-et-al">#</a></h4>
<p>$$
S_h = \sum_{t=1}^{N} N_t, \quad
N_t =
\begin{cases}
\dfrac{1}{N}, &amp; \text{if } \arg\max(a^t_h) \in k [2mm]
0, &amp; \text{otherwise}
\end{cases}
\tag{1}
$$</p>
<p>å…¶ä¸­ $a^t_h$ æ˜¯ head $h$ åœ¨ decoding step $t$ å¯¹åˆå¹¶è¾“å…¥ï¼ˆneedle + haystackï¼‰çš„ attention åˆ†æ•°å‘é‡ï¼Œ$N$ æ˜¯ needle é•¿åº¦ã€‚æ­¤æ³•ä¾èµ– argmax æ˜¯å¦è½åœ¨ needle ä¸Šï¼ˆå³æ˜¯å¦ exact matchï¼‰ã€‚</p>
<hr>
<h4 id="retrieval-reasoningä½œè€…æå‡º">Retrieval-Reasoningï¼ˆä½œè€…æå‡ºï¼‰<a hidden class="anchor" aria-hidden="true" href="#retrieval-reasoningä½œè€…æå‡º">#</a></h4>
<p>ä¸ºäº†ä¸ä¸¢å¤± token ä»¥å¤–çš„è´¡çŒ®ï¼Œä½œè€…æŠŠâ€œåªçœ‹ argmaxâ€æ‰©å±•ä¸ºå¯¹ top-i attention çš„åŠ æƒæ±‚å’Œï¼Œå¹¶ä»¥æ­£ç¡®ç­”æ¡ˆ $c_2$ çš„æ‰€æœ‰ tokens ä¸ºå…³æ³¨å¯¹è±¡ï¼š</p>
<p>$$
S_h = \sum_{t=1}^N \sum_{i=1}^N M^t_i, \quad
M^t_i =
\begin{cases}
\dfrac{a_i}{N}, &amp; \text{if } \text{top-i}(a^t_h)\in c_2 [1mm]
0, &amp; \text{otherwise}
\end{cases}
\tag{2}
$$</p>
<p>å…¶ä¸­ $a_i$ æ˜¯ head $h$ åœ¨ step $t$ çš„ç¬¬ $i$ å¤§ attention å€¼ï¼›$\text{top-i}(a^t_h)$ æ˜¯å¯¹åº”çš„ tokenï¼ˆç¬¬ $i$ é«˜ attention æŒ‡å‘çš„ tokenï¼‰ã€‚
ç›´è§‚ä¸Šï¼Œè‹¥ä¸€ä¸ª head åœ¨å¤šä¸ª decoding step å¯¹æ­£ç¡®ç­”æ¡ˆçš„ token ç»™å‡ºé«˜ attentionï¼Œåˆ™è¯¥ head å¾—åˆ†è¾ƒé«˜ï¼›é€šè¿‡å¯¹ $a_i$ çš„åŠ æƒï¼Œè¯„åˆ†æ›´å¹³æ»‘ä¸”èƒ½åæ˜ â€œéƒ¨åˆ†æ³¨æ„åŠ›â€è´¡çŒ®ã€‚</p>
<hr>
<p><strong>å®ç°ç»†èŠ‚ï¼ˆè¡¥å……ï¼‰</strong>ï¼š</p>
<ul>
<li>åœ¨å®é™…å®ç°ä¸­ï¼Œä½œè€…å°† needle æ’å…¥ä¸åŒä½ç½® $p_i$ï¼ˆä¸åŒçš„ haystack ä½ç½®ï¼‰ä»¥ä¿è¯åˆ†å¸ƒç¨³å¥ã€‚å¯¹æ¯ä¸ªæ’å…¥ä½ç½®è·‘è‹¥å¹²æ ·æœ¬ï¼Œå†å¯¹åŒä¸€ head çš„å¾—åˆ†å–å¹³å‡ï¼ˆè®ºæ–‡ pseudo-code ä¸­æœ‰è¯»å–å¹¶å¹³å‡çš„å®ç°ï¼‰ã€‚</li>
<li>å¯¹ $S_h$ åš $L_1$-å½’ä¸€åŒ–ï¼ˆsum åˆ° 1ï¼‰ï¼Œå¾—åˆ° head é‡è¦æ€§åˆ†å¸ƒå‘é‡ $\mathbf{S}$ï¼Œä»¥ä¾¿ç”¨äºæŒ‰æ¯”ä¾‹åˆ†é…åŠ¨æ€æ± é¢„ç®—ï¼ˆè§ 3.2ï¼‰ã€‚</li>
<li>è®¾è®¡å– top-i çš„ä¸Šé™ $i_\text{max}$ï¼Œä»¥åŠæ˜¯å¦å¯¹ä¸åŒ decoding step èµ‹ä¸åŒæƒé‡ï¼Œè¿™äº›åœ¨è®ºæ–‡å®ç°ä¸­é€šè¿‡å®éªŒé€‰æ‹©ï¼ˆé»˜è®¤æ‰©å¤§äº† i çš„æ•°é‡ï¼Œç›¸æ¯” Wu et al. æ›´å®½æ¾ï¼‰ã€‚è¯¦è§é™„å½• pseudo-codeã€‚</li>
</ul>
<p><strong>ç›´è§‚æ€»ç»“</strong>ï¼šR2ï¼ˆRetrieval-Reasoningï¼‰è¯„åˆ†æŠŠ head å¯¹äº**æ•´æ®µæ­£ç¡®ç­”æ¡ˆï¼ˆè€Œéå• tokenï¼‰**çš„æŒç»­æ³¨æ„åŠ›éƒ½è€ƒè™‘è¿›æ¥ï¼Œå› æ­¤è¯„åˆ†å¯†åº¦æ›´é«˜ã€æ›´èƒ½åŒºåˆ†â€œè´Ÿè´£æ£€ç´¢â€ä¸â€œè´Ÿè´£æ¨ç†â€çš„å¤´ï¼Œå¹¶ä¸”æ›´é€‚åˆé©±åŠ¨èµ„æºæœ‰é™æ—¶çš„é¢„ç®—åˆ†é…ã€‚è®ºæ–‡å¯è§†åŒ–ä¹Ÿæ˜¾ç¤º R2 åˆ†å¸ƒæ›´ denseï¼Œè€ŒåŸå§‹ retrieval åˆ†å¸ƒè¾ƒ sparseã€‚</p>
<hr>
<h3 id="32-head-level-kv-cache-allocation">3.2 Head-Level KV Cache Allocation<a hidden class="anchor" aria-hidden="true" href="#32-head-level-kv-cache-allocation">#</a></h3>
<p><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼šåŸºäºå¾—åˆ°çš„ head é‡è¦æ€§åˆ†å¸ƒ ${S_h}$ å¯¹æ¯ä¸ª head åˆ†é…ä¸åŒçš„ KV cache å¤§å° $b_h$ï¼ŒåŒæ—¶ä¿ç•™ä¸€ä¸ªã€Œå…±äº«åŠ¨æ€æ± ã€ $B$ ç”¨äºæŠŠé¢„ç®—ä»å¼±å¤´æ”¶é›†å¹¶æŒ‰æƒé‡ç»™å¼ºå¤´ã€‚</p>
<hr>
<p><strong>å…¬å¼ï¼š</strong></p>
<p>$$
B = \frac{b}{\beta} \cdot L \cdot H, \qquad
b_h = \Big(b - \frac{b}{\beta}\Big) + S_h \cdot B
\tag{4}
$$</p>
<p>è§£é‡Šï¼š</p>
<ul>
<li>$b$ï¼šæ¯ä¸ª head çš„åˆå§‹å›ºå®šé¢„ç®—ï¼ˆbaseline budgetï¼‰ï¼›</li>
<li>$\beta$ï¼šè¶…å‚æ•°æ§åˆ¶å…±äº«æ± å¤§å°ï¼›è¾ƒå°çš„ $\beta$ â‡’ æ›´å¤§çš„å…±äº«æ± ï¼ˆæ›´å¤šé¢„ç®—å¯è¢«é‡æ–°åˆ†é…ï¼‰ï¼›</li>
<li>$L$ã€$H$ï¼šæ¨¡å‹çš„å±‚æ•°ä¸æ¯å±‚ head æ•°ï¼ˆå› æ­¤ $L\times H$ æ˜¯æ€» head æ•°ï¼‰ï¼›</li>
<li>æœ€ç»ˆ $b_h$ ç”± â€œä¿è¯çš„æœ€å°é¢„ç®—â€ $\big(b - b/\beta\big)$ ä¸æŒ‰é‡è¦æ€§åˆ†é…çš„åŠ¨æ€é¢„ç®— $S_h \cdot B$ ä¹‹å’Œæ„æˆã€‚</li>
</ul>
<hr>
<p><strong>å®ç°ç»†èŠ‚ï¼ˆPseudo codeï¼‰</strong>ï¼š</p>
<ul>
<li>
<p>å…ˆä¿ç•™æœ€å $\alpha$ ä¸ª instruction tokenï¼ˆlocal windowï¼‰ï¼Œè¿™äº› token åœ¨å½¢æˆåŠ¨æ€æ± å‰å¿…é¡»ä¿è¯å…¥æ± å‰è¢«ä¿ç•™ï¼Œç”¨äºæŒ‡å¯¼ selectionï¼ˆè§ 3.3ï¼‰ã€‚è®ºæ–‡é»˜è®¤ $\alpha=8$ã€‚</p>
</li>
<li>
<p>è®ºæ–‡å®ç°ï¼ˆpseudo-codeï¼‰ä¸­çš„æ­¥éª¤ï¼š</p>
<ol>
<li>è¯»å–å¹¶å¹³å‡ä¿å­˜å¥½çš„ head å¾—åˆ†åˆ—è¡¨ï¼Œå½’ä¸€åŒ–å¾—åˆ° $\mathbf{S}$ã€‚</li>
<li>è®¡ç®—
$$
\text{total_pool_capacity} = \frac{\text{base_capacity}}{\beta} \cdot \text{num_hidden_layers} \cdot \text{num_attention_heads},
\quad
\text{min_num} = \text{base_capacity} - \frac{\text{base_capacity}}{\beta}
$$
3.</li>
</ol>
<p>$$
\text{head_capacity} = \text{round}(\text{total_attention} \cdot \text{total_pool_capacity} + \text{min_num})
$$
ï¼ˆå‘æœ€è¿‘æ•´æ•°å–æ•´å¾—åˆ°æ¯ä¸ª head çš„æœ€ç»ˆæ¡ç›®æ•°ï¼Œç”¨äº gather æ“ä½œï¼‰ã€‚</p>
</li>
</ul>
<hr>
<h3 id="33-kv-cache-selection">3.3 KV Cache Selection<a hidden class="anchor" aria-hidden="true" href="#33-kv-cache-selection">#</a></h3>
<p><strong>ç›®æ ‡</strong>ï¼šåœ¨ç¡®å®šæ¯ä¸ª head ä¿ç•™çš„æ¡ç›®æ•° (b_h) åï¼Œå¦‚ä½•ä»è¯¥ head çš„å†å² key/value åˆ—è¡¨ä¸­é€‰å–å…·ä½“çš„ç´¢å¼•ï¼ˆtokensï¼‰ä»¥æ„æˆ compressed KV cacheã€‚</p>
<p><strong>æ–¹æ³•ï¼ˆåŸºäº SnapKVï¼‰</strong>ï¼š</p>
<ul>
<li>ä¿ç•™æœ€å (\alpha) ä¸ª instruction tokensï¼ˆlocal observation windowï¼‰ï¼›</li>
<li>è®¡ç®— local windowï¼ˆqueryï¼‰åˆ°æ¯ä¸ªå€™é€‰ tokenï¼ˆkeyï¼‰çš„ attention scoreï¼ˆé€šå¸¸ä¸ºç‚¹ç§¯æˆ– softmax å½’ä¸€åŒ–åçš„åˆ†æ•°ï¼‰ï¼›</li>
<li>å¯¹æ¯ä¸ª headï¼Œå°†è¿™äº› attention åˆ†æ•°è¿›è¡Œ pooling/èšåˆï¼ˆè®ºæ–‡ä¸­æåˆ° pooling å±‚ç”¨äºèšåˆæ¥è‡ª local window çš„ attentionï¼‰ï¼Œå¹¶å°†å¾—åˆ†æ’åºï¼›</li>
<li>é€‰å–å¾—åˆ†æœ€é«˜çš„å‰ (b_h) ä¸ª token ç´¢å¼•ä½œä¸ºè¯¥ head çš„ç¼“å­˜æ¡ç›®ã€‚æœ€åå°†è¿™äº›è¢«é€‰å‡ºçš„ key/value ä¸ local window çš„æœ€è¿‘ tokens åˆå¹¶ï¼ˆconcatenateï¼‰ï¼Œæ„æˆæœ€ç»ˆæ¯ä¸ª head çš„ compressed KVã€‚</li>
</ul>
<p><strong>Pseudo-PyTorch æ ·ä¾‹ï¼ˆè®ºæ–‡ Listing æ”¹å†™å¹¶æ³¨é‡Šè¦ç‚¹ï¼‰</strong>ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># ä¼ªä»£ç è¦ç‚¹ï¼ˆåŸºäºè®ºæ–‡ Listingï¼‰</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å‡è®¾ï¼šorigin_heads_key_states: [num_heads, batch, seq_len, head_dim]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         origin_heads_value_states: ç±»ä¼¼</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#         head_capacity[layer_idx][head_idx] å·²ç”± obtain_head_budget å¾—åˆ°</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>heads_key_states <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>heads_value_states <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. è®¡ç®— local window -&gt; æ‰€æœ‰ tokens çš„ attention scoreï¼ˆåŒ SnapKVï¼‰</span>
</span></span><span style="display:flex;"><span>attn_score <span style="color:#f92672">=</span> calc_attn_score(query_states, key_states)  <span style="color:#75715e"># shape: [num_heads, seq_len]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. æ’åºå¾—åˆ°ç´¢å¼•ï¼ˆæŒ‰å¾—åˆ†é™åºï¼‰</span>
</span></span><span style="display:flex;"><span>_, indices <span style="color:#f92672">=</span> attn_score<span style="color:#f92672">.</span>sort(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, descending<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. å¯¹æ¯ä¸ª head æŒ‰ head_capacity å– top-k ç´¢å¼•å¹¶ gather key/value</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> head_idx <span style="color:#f92672">in</span> range(num_heads):
</span></span><span style="display:flex;"><span>    k <span style="color:#f92672">=</span> head_capacity[layer_idx][head_idx]  <span style="color:#75715e"># è¯¥ head æœ€ç»ˆä¿ç•™æ•°</span>
</span></span><span style="display:flex;"><span>    cache_index <span style="color:#f92672">=</span> indices[head_idx, :k]     <span style="color:#75715e"># å– top-k ç´¢å¼•</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># æ‰©å±•ç´¢å¼•åˆ° head_dim ç”¨äº gather</span>
</span></span><span style="display:flex;"><span>    cache_index <span style="color:#f92672">=</span> cache_index<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>expand(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,head_dim)
</span></span><span style="display:flex;"><span>    top_Kcache <span style="color:#f92672">=</span> origin_heads_key_states[head_idx]<span style="color:#f92672">.</span>gather(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, index<span style="color:#f92672">=</span>cache_index)
</span></span><span style="display:flex;"><span>    top_Vcache <span style="color:#f92672">=</span> origin_heads_value_states[head_idx]<span style="color:#f92672">.</span>gather(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, index<span style="color:#f92672">=</span>cache_index)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># ä¸ local window çš„ last self.window_size tokens åˆå¹¶</span>
</span></span><span style="display:flex;"><span>    selected_k <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([top_Kcache, origin_heads_key_states[head_idx][:,:, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>window_size:, :]], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    selected_v <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([top_Vcache, origin_heads_value_states[head_idx][:,:, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>window_size:, :]], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    heads_key_states<span style="color:#f92672">.</span>append(selected_k<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, head_dim))
</span></span><span style="display:flex;"><span>    heads_value_states<span style="color:#f92672">.</span>append(selected_v<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, head_dim))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># æœ€ç»ˆåˆå¹¶æ‰€æœ‰ head çš„ key/value</span>
</span></span><span style="display:flex;"><span>heads_key_states <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(heads_key_states, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>heads_value_states <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(heads_value_states, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>è¯¥æµç¨‹åœ¨è®ºæ–‡å®ç°ï¼ˆListingï¼‰ä¸­ç»™å‡ºï¼Œå®é™…å®ç°è¿˜åŒ…å«ä¿å­˜ä¸åŠ è½½ importance distributionã€layer-by-layer çš„å¤„ç†ä¸å¼ é‡å¯¹é½ç»†èŠ‚ã€‚</p>
<p><strong>è¡¥å……è¯´æ˜</strong>ï¼š</p>
<ul>
<li>åˆå¹¶ local window çš„ç›®çš„æ˜¯ä¿è¯æœ€è¿‘çš„æŒ‡ä»¤ä¿¡æ¯ï¼ˆinstruction tokensï¼‰ä¸€å®šè¢«ä¿ç•™ï¼Œè¿™å¯¹ç”Ÿæˆè´¨é‡å’Œ selection æŒ‡å¯¼å¾ˆé‡è¦ï¼›</li>
<li>selection ä½¿ç”¨ attention pooling è€Œä¸æ˜¯ç®€å•çš„ token TF/IDF ç­‰å¯å‘å¼ï¼Œæ˜¯å› ä¸º attention èƒ½åæ˜ æ¨¡å‹å½“å‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„å†…éƒ¨ä¿¡å·ï¼ˆå³â€œæ¨¡å‹çŸ¥é“è‡ªå·±åœ¨çœ‹ä»€ä¹ˆâ€ï¼‰ï¼Œè¿™ä¹Ÿæ˜¯ SnapKV çš„æ ¸å¿ƒæ€æƒ³ã€‚</li>
</ul>
<hr>
<h2 id="4-experiments-and-analysis">4. Experiments and Analysis<a hidden class="anchor" aria-hidden="true" href="#4-experiments-and-analysis">#</a></h2>
<h3 id="41-experiment-settings">4.1 Experiment settings<a hidden class="anchor" aria-hidden="true" href="#41-experiment-settings">#</a></h3>
<ul>
<li>
<p><strong>Backbone Models</strong>ï¼šLlama-3-8B-Instructã€Mistral-7B-Instructã€‚</p>
</li>
<li>
<p><strong>Benchmarks / Datasets</strong>ï¼šLongBenchï¼ˆSingle-Doc QAã€Multi-Doc QA ç±»åˆ«ï¼‰ä¸ LooGLEï¼ˆLong Dependency QA ç±»åˆ«ï¼‰ã€‚è®ºæ–‡é™„å½•ç»™å‡ºå„æ•°æ®é›†çš„å…·ä½“æ ·æœ¬ä¸å¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦ç­‰ï¼ˆAppendix Table 5ï¼‰ã€‚</p>
</li>
<li>
<p><strong>Baselines</strong>ï¼š</p>
<ol>
<li><strong>SnapKV</strong>ï¼šä»¥æœ€å (\alpha) ä¸ª tokens æŒ‡å¯¼ selectionï¼ˆattention poolingï¼‰ã€‚</li>
<li><strong>PyramidKV</strong>ï¼šæŒ‰å±‚é‡‘å­—å¡”åˆ†é…ï¼Œæ›´ä½å±‚ç»™æ›´å¤šç¼“å­˜ã€‚</li>
<li><strong>Ada-KV / Ada-SnapKV</strong>ï¼šåœ¨å±‚å†…åŸºäº concentration åŠ¨æ€åˆ†é…ï¼ˆè®ºæ–‡é‡‡ç”¨ Ada-SnapKV ä½œä¸ºæœ€å¼º baselineï¼‰ã€‚</li>
</ol>
</li>
<li>
<p><strong>ç»Ÿä¸€è®¾ç½®</strong>ï¼šlocal window size (\alpha = 8)ï¼›è¯„ä¼° KV sizeï¼ˆä¿ç•™æ¡ç›®ï¼‰é›†åˆï¼š({64,128,256,512,1024})ï¼›(\beta) åœ¨ ({1.005,1.01,1.1,1.2,1.5,2,5,10}) å–æœ€ä¼˜æŠ¥å‘Šï¼›é€‰æ‹© SnapKV ä½œä¸º per-head selection æ–¹æ³•ï¼ˆå³ HeadKV åœ¨åˆ†é…ä¹‹åç”¨ SnapKV åšå…·ä½“æ¡ç›®æŒ‘é€‰ï¼‰ã€‚</p>
</li>
</ul>
<hr>
<h3 id="42-main-results">4.2 Main results<a hidden class="anchor" aria-hidden="true" href="#42-main-results">#</a></h3>
<ul>
<li><strong>æ€»ä½“ç»“è®º</strong>ï¼šHead-level åˆ†é…ï¼ˆHeadKVï¼‰åœ¨å„ç§ KV size ä¸‹æ™®éä¼˜äº layer-level baselinesï¼Œå°¤å…¶åœ¨ä½èµ„æºï¼ˆKV size=64 æˆ– 128ï¼‰ä¸‹æ”¶ç›Šæœ€æ˜¾è‘—ï¼›è€ŒåŸºäº Retrieval-Reasoningï¼ˆHeadKV-R2ï¼‰çš„åˆ†å¸ƒè¿›ä¸€æ­¥ä¼˜äºåªåŸºäºæ£€ç´¢çš„åˆ†å¸ƒï¼ˆHeadKV-Rï¼‰ã€‚åœ¨æŸäº›è®¾ç½®ä¸‹ï¼ˆLlama-3-8B, KV=1024ï¼‰ï¼ŒHeadKV-R2 çš„å¹³å‡åˆ†ç”šè‡³ç•¥è¶… FullKVï¼ˆ32.95 vs 32.90ï¼‰ï¼Œè¯´æ˜åˆç†å‹ç¼©åœ¨å™ªå£°ï¼å†—ä½™ä¿¡æ¯è¿‡å¤šæ—¶è¿˜èƒ½æŠ‘åˆ¶è´Ÿé¢å½±å“ã€‚è¡¨ 1 ç»™å‡ºå®Œæ•´æ•°å€¼ã€‚</li>
</ul>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/HeadKV/table1.png"></p>
<p><strong>è¡¥å……è§£é‡Š / ç›´è§‚è§£è¯»</strong>ï¼š</p>
<ul>
<li>layer-level æ–¹æ³•ï¼ˆSnapKVã€PyramidKVï¼‰å¯¹æ‰€æœ‰å¤´ä¸€è§†åŒä»æˆ–æŒ‰å±‚ç²—ç²’åº¦åˆ†é…ï¼Œæ— æ³•åœ¨â€œå“ªäº› head çœŸæ­£è´Ÿè´£æ‰¾ç­”æ¡ˆæˆ–æ¨ç†â€ä¸Šåšå¾®è°ƒï¼Œå› æ­¤åœ¨ head æ•°ç›®å¤šã€åŠŸèƒ½å¼‚è´¨æ€§å¤§çš„æ¨¡å‹ä¸Šä¼šæµªè´¹å®è´µçš„ç¼“å­˜èµ„æºã€‚HeadKV å°†é¢„ç®—é›†ä¸­åˆ°å°‘æ•°å…³é”®å¤´ï¼Œä»è€Œåœ¨æå°çš„ç¼“å­˜é¢„ç®—ä¸‹ä¿å­˜å…³é”®ä¿¡æ¯ï¼ˆè®ºæ–‡æå‡ºåœ¨ 1.5% KV ä¿ç•™ä¸‹ä»èƒ½è¾¾åˆ° FullKV çš„ 97% æ€§èƒ½ï¼‰ã€‚</li>
</ul>
<hr>
<h3 id="43-retrieval-reasoning-heads">4.3 Retrieval-Reasoning Heads<a hidden class="anchor" aria-hidden="true" href="#43-retrieval-reasoning-heads">#</a></h3>
<ul>
<li><strong>Ablation</strong>ï¼šä½œè€…æ¯”è¾ƒäº†ä¸‰ç§ head åˆ†å¸ƒï¼šåŸå§‹ Retrievalï¼ˆWu et al. çš„ exact-matchï¼›HeadKV-Rï¼‰ã€Enhanced-Retrievalï¼ˆä¿æŒ retrieval ç¤ºä¾‹ä½†é‡‡ç”¨æ–°çš„å¾—åˆ†ä¼°è®¡æ–¹æ³•ï¼›HeadKV-ERï¼‰ã€Retrieval-Reasoningï¼ˆä½œè€…æå‡ºçš„ R2ï¼›HeadKV-R2ï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼šHeadKV-ER è¾ƒ HeadKV-R æœ‰å°å¹…æå‡ï¼ˆå› ä¸ºæ›´å…³æ³¨æ•´ä¸ª needle è€Œé argmaxï¼‰ï¼Œä½†ä»ä¸å¦‚ HeadKV-R2ï¼ˆå› ä¸º ER ä»ç¼ºå°‘æ¨ç†æ­¥éª¤çš„æ¿€æ´»ä¿¡å·ï¼‰ã€‚è¡¨ 2 ç»™å‡ºå…·ä½“æ•°å€¼ã€‚</li>
</ul>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/HeadKV/figure4.png"></p>
<p><strong>æ·±å…¥è¡¥å……</strong>ï¼š</p>
<ul>
<li>R2 åˆ†å¸ƒçš„å¯†åº¦æ›´å¤§ã€æœ‰æ›´å¥½çš„åŒºåˆ†åº¦ï¼ˆless zero massï¼‰ï¼Œé€‚åˆåœ¨åˆ†é…å…±äº«æ± æ—¶è¿›è¡Œç¨³å®šçš„æ¯”ä¾‹åˆ†é…ï¼›å®éªŒæ˜¾ç¤º R2 åœ¨å¤šä¸ªä»»åŠ¡ä¸Šéƒ½å¸¦æ¥ä¸€è‡´æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ multi-doc / long dependency çš„ QA åœºæ™¯ï¼ˆè¿™äº›åœºæ™¯åŒæ—¶éœ€è¦æ£€ç´¢ä¸å¤šæ­¥æ¨ç†ï¼‰ã€‚</li>
</ul>
<hr>
<h3 id="44-long-context-retrieval-and-reasoning">4.4 Long-context retrieval and reasoning<a hidden class="anchor" aria-hidden="true" href="#44-long-context-retrieval-and-reasoning">#</a></h3>
<p>ä½œè€…ä½¿ç”¨ä¸¤ç±»ä¸“é—¨è®¾è®¡çš„ stress-testsï¼š</p>
<ol>
<li>
<p><strong>Needle-in-a-Haystackï¼ˆæ£€ç´¢æµ‹è¯•ï¼‰</strong>ï¼šåœ¨æµ·é‡æ— å…³æ–‡æœ¬ä¸­æ’å…¥ä¸€ä¸ª needleï¼ˆç­”æ¡ˆç‰‡æ®µï¼‰ï¼Œæ£€æµ‹æ˜¯å¦èƒ½æ£€ç´¢å¹¶ç›´æ¥ paste å‡ºç­”æ¡ˆï¼ˆåå‘çº¯æ£€ç´¢èƒ½åŠ›ï¼‰ã€‚ç»“æœä¸­ HeadKVï¼ˆå°¤å…¶ HeadKV-R2ï¼‰åœ¨ KV=128 æ—¶æ¯”å…¶ä»–å‹ç¼©æ–¹æ³•è¡¨ç°æ›´å¥½ï¼ˆå›¾ 5ï¼‰ã€‚</p>
</li>
<li>
<p><strong>Reasoning-in-a-Haystackï¼ˆæ¨ç†æµ‹è¯•ï¼‰</strong>ï¼šåŸºäº bAbI é£æ ¼çš„ reasoning é—®é¢˜ï¼Œå°† reasoning-needles æ’å…¥åˆ°å¤§ haystack ä¸­ï¼Œæ¨¡å‹éœ€å…ˆæ£€ç´¢åˆ°ç›¸å…³ needlesï¼Œå†åŸºäºå®ƒä»¬è¿›è¡Œå¤šæ­¥æ¨ç†ï¼ˆåå‘æ£€ç´¢ï¼‹æ¨ç†ï¼‰ã€‚åœ¨è¿™ä¸ªæµ‹è¯•ä¸Šï¼ŒHeadKV-R2 çš„ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ï¼ˆTable 3ï¼‰ï¼Œè¯´æ˜ R2 çš„è¯„åˆ†ç¡®å®æ•æ‰åˆ°äº†æ¨ç†ç›¸å…³çš„ headã€‚</p>
</li>
</ol>
<p><strong>å®šé‡è¦ç‚¹</strong>ï¼ˆæ‘˜è‡ªè®ºæ–‡è¡¨æ ¼ä¸å›¾ï¼‰ï¼š</p>
<ul>
<li>åœ¨ Llama-3-8B, KV=128 çš„ Reasoning-in-a-Haystack å¹³å‡åˆ†ï¼šFullKV çº¦ 57.04ï¼ŒHeadKV-R2 çº¦ 56.84ï¼ˆæ¥è¿‘ FullKVï¼‰ï¼Œè€Œ SnapKVã€PyramidKV ç­‰æ˜æ˜¾è½åï¼ˆè§ Table 3ï¼‰ã€‚è¿™è¡¨æ˜åœ¨å—é™ KV ä¸‹ HeadKV-R2 èƒ½æ˜¾è‘—ä¿ç•™ reasoning èƒ½åŠ›ã€‚</li>
</ul>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/HeadKV/figure5.png"></p>
<hr>
<h3 id="45-memory--latency">4.5 Memory &amp; Latency<a hidden class="anchor" aria-hidden="true" href="#45-memory--latency">#</a></h3>
<p><strong>è®¾ç½®ä¸ç»“è®º</strong>ï¼š</p>
<ul>
<li>ä½¿ç”¨ Mistral-7B-Instructã€æœ€å¤§åºåˆ—é•¿åº¦ 32Kã€FlashAttention å®ç°ï¼›è¯„ä¼° decoding latencyï¼ˆåŒ…å« prefill ä¸ decodingï¼‰ä¸ peak memoryï¼ˆåœ¨ä¸åŒ context len ä¸ generation len ä¸‹ï¼‰ã€‚è®ºæ–‡å›¾ 6 æ˜¾ç¤º HeadKV åœ¨è§£ç å»¶è¿Ÿä¸Šä¸å…¶ä»–å‹ç¼©æ–¹æ³•åŸºæœ¬æŒå¹³ï¼ŒåŒæ—¶åœ¨ peak memory ä¸Šè¾ƒ FullKV æœ‰æ˜¾è‘—é™ä½ã€‚æ¢è¨€ä¹‹ï¼šHeadKV åœ¨ä¸å¢åŠ è¿è¡Œæ—¶å¼€é”€çš„å‰æä¸‹ï¼Œè¾¾åˆ°äº†æ›´å¥½çš„æ€§èƒ½/è®°å¿†ç‡æŠ˜ä¸­ã€‚</li>
</ul>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/HeadKV/figure6.png"></p>
<p><strong>è¡¥å……è¯´æ˜</strong>ï¼š</p>
<ul>
<li>decoding latency çš„æ›²çº¿åœ¨ generation length=1 æ—¶æ¥è¿‘ï¼Œè¯´æ˜ prefill çš„é¢å¤–å¼€é”€ï¼ˆåŒ…æ‹¬è®¡ç®— importance scores çš„ç¦»çº¿å¼€é”€ï¼‰è¢«è®¾è®¡ä¸ºåˆå§‹åŒ–é˜¶æ®µæˆ–ä¸€æ¬¡æ€§ç¦»çº¿è®¡ç®—ï¼ˆè®ºæ–‡ä¸­ importance distribution ä¸ºé™æ€å¹¶åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶è½½å…¥ï¼‰ï¼Œå› æ­¤å¯¹åœ¨çº¿æ¨ç†å¼€é”€å½±å“å¾®å°ã€‚Pseudo-code æ˜ç¡®æŒ‡å‡º obtain_head_budget åœ¨åˆå§‹åŒ–æ—¶è¿è¡Œä¸€æ¬¡ã€‚</li>
</ul>
<hr>
<h3 id="g-hyper-parameter-analysis">G. Hyper-parameter analysis<a hidden class="anchor" aria-hidden="true" href="#g-hyper-parameter-analysis">#</a></h3>
<ul>
<li>è®ºæ–‡é€šè¿‡æ‰«æ (\beta) å€¼é›†åˆå±•ç¤ºäº† (\beta) å¯¹æœ€ç»ˆå¹³å‡åˆ†çš„å½±å“ï¼ˆFigure 10/11ï¼‰ã€‚æ€»ä½“ç»“è®ºï¼šè¾ƒå° (\beta)ï¼ˆæ›´å¤§çš„åŠ¨æ€æ± ï¼‰èƒ½è®© HeadKV-R2 è¿›ä¸€æ­¥è·ç›Šï¼Œè¯´æ˜ R2 åˆ†å¸ƒåœ¨â€œæŠŠæ›´å¤šé¢„ç®—åŠ¨æ€åˆ†é…ç»™é‡è¦å¤´â€æ—¶æ›´å¯é ã€‚è®ºæ–‡åŒæ—¶ä¿æŒ (\alpha) ä¸å…¶ä»–å‚æ•°ä¸ PyramidKV çš„å®ç°ä¸€è‡´ä»¥ä¿è¯å…¬å¹³æ¯”è¾ƒã€‚</li>
</ul>
<hr>
<h2 id="5-conclusion--future-work">5. Conclusion &amp; Future Work<a hidden class="anchor" aria-hidden="true" href="#5-conclusion--future-work">#</a></h2>
<p><strong>è®ºæ–‡ç»“è®º</strong>ï¼šæå‡º HeadKV-R2ï¼ˆhead-level KV cache å‹ç¼© + retrieval-reasoning é‡è¦æ€§ä¼°è®¡ï¼‰ï¼Œåœ¨ LongBenchã€LooGLE ç­‰å¤šæ•°æ®é›†ä¸ä¸¤ç§ backbone æ¨¡å‹ä¸Šè¯æ˜äº†åœ¨æé™ç¼“å­˜é¢„ç®—ä¸‹èƒ½ä¿å­˜å¤§éƒ¨åˆ†æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒä½å†…å­˜ä¸å»¶è¿Ÿå¼€é”€ã€‚</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jjl357.github.io/blog/tags/paper-note/">Paper Note</a></li>
      <li><a href="https://jjl357.github.io/blog/tags/kv-cache/">KV Cache</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://jjl357.github.io/blog/">JJ&#39;s Blog</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
