<!DOCTYPE html>
<html lang="zh-cn" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models | JJ&#39;s Blog</title>
<meta name="keywords" content="KV Cache, Quantization, MLLM, Paper Note">
<meta name="description" content="
Conference: NeurIPS&#39;25 Spotlight
Github: https://github.com/SAI-Lab-NYU/QSVD

1. Motivation
Visionâ€“Language Models (VLMs) å¦‚ LLaVAã€BLIP2 ç­‰åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­” (VQA) ç­‰ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†è¿™äº›æ¨¡å‹éœ€è¦æå¤§çš„è®¡ç®—ä¸å­˜å‚¨å¼€é”€ï¼Œå°¤å…¶åœ¨æ¨ç†æ—¶ï¼š

KV Cache å ç”¨é«˜ï¼šæ³¨æ„åŠ›æœºåˆ¶ä¸­éœ€å­˜å‚¨ Keyã€Valueï¼Œæ¯å±‚ç¼“å­˜å¤§å°éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ã€‚
Q/K/V æŠ•å½±é‡å¤è®¡ç®—ï¼šä¸‰ç»„æƒé‡çŸ©é˜µç‹¬ç«‹è®¡ç®—ï¼Œé€ æˆç®—åŠ›æµªè´¹ã€‚
æ¨¡å‹é‡åŒ–å›°éš¾ï¼šæ¿€æ´»åˆ†å¸ƒå­˜åœ¨æç«¯ outliersï¼Œéš¾ä»¥ç¨³å®šè¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ã€‚

QSVD çš„ç›®æ ‡æ˜¯ç»Ÿä¸€åœ°å¯¹ Q/K/V æƒé‡çŸ©é˜µè¿›è¡Œä½ç§©è¿‘ä¼¼å¹¶ç»“åˆåè®­ç»ƒé‡åŒ– (PTQ)ï¼Œå®ç°ä»¥ä¸‹ä¸‰ç‚¹ï¼š

å‡å°‘å‚æ•°é‡ã€è®¡ç®—é‡ã€ç¼“å­˜å ç”¨ï¼›
ä¿æŒæ¨¡å‹æ€§èƒ½ï¼›
æ”¯æŒä½ç²¾åº¦ç¡¬ä»¶éƒ¨ç½²ã€‚



2. Related Work
2.1 SVD in Large Models
Singular Value Decomposition (SVD) æ˜¯ç»å…¸çš„çŸ©é˜µåˆ†è§£æ–¹æ³•ã€‚
å¯¹äºçŸ©é˜µ ( W \in \mathbb{R}^{m \times n} )ï¼Œå¯åˆ†è§£ä¸ºï¼š
$$
W = U \Sigma V^T
$$
å…¶ä¸­ï¼š

(U, V) ä¸ºæ­£äº¤çŸ©é˜µï¼›
(\Sigma) ä¸ºå¥‡å¼‚å€¼å¯¹è§’çŸ©é˜µï¼›
ä¿ç•™å‰ (r) ä¸ªå¥‡å¼‚å€¼å¯å¾—åˆ° rank-(r) è¿‘ä¼¼ï¼š

$$
W \approx U_r \Sigma_r V_r^T
$$">
<meta name="author" content="">
<link rel="canonical" href="https://jjl357.github.io/blog/posts/qsvd----efficient-low-rank-approximation-for-unified-query-key-value-weight-compression-in-low-precision-vision-language-models---neurips25-spotlight/">
<link crossorigin="anonymous" href="https://jjl357.github.io/blog/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jjl357.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jjl357.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jjl357.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jjl357.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://jjl357.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh-cn" href="https://jjl357.github.io/blog/posts/qsvd----efficient-low-rank-approximation-for-unified-query-key-value-weight-compression-in-low-precision-vision-language-models---neurips25-spotlight/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><script type="text/javascript"
        async
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<meta property="og:url" content="https://jjl357.github.io/blog/posts/qsvd----efficient-low-rank-approximation-for-unified-query-key-value-weight-compression-in-low-precision-vision-language-models---neurips25-spotlight/">
  <meta property="og:site_name" content="JJ&#39;s Blog">
  <meta property="og:title" content="QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models">
  <meta property="og:description" content="
Conference: NeurIPS&#39;25 Spotlight Github: https://github.com/SAI-Lab-NYU/QSVD
1. Motivation Visionâ€“Language Models (VLMs) å¦‚ LLaVAã€BLIP2 ç­‰åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­” (VQA) ç­‰ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†è¿™äº›æ¨¡å‹éœ€è¦æå¤§çš„è®¡ç®—ä¸å­˜å‚¨å¼€é”€ï¼Œå°¤å…¶åœ¨æ¨ç†æ—¶ï¼š
KV Cache å ç”¨é«˜ï¼šæ³¨æ„åŠ›æœºåˆ¶ä¸­éœ€å­˜å‚¨ Keyã€Valueï¼Œæ¯å±‚ç¼“å­˜å¤§å°éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ã€‚ Q/K/V æŠ•å½±é‡å¤è®¡ç®—ï¼šä¸‰ç»„æƒé‡çŸ©é˜µç‹¬ç«‹è®¡ç®—ï¼Œé€ æˆç®—åŠ›æµªè´¹ã€‚ æ¨¡å‹é‡åŒ–å›°éš¾ï¼šæ¿€æ´»åˆ†å¸ƒå­˜åœ¨æç«¯ outliersï¼Œéš¾ä»¥ç¨³å®šè¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ã€‚ QSVD çš„ç›®æ ‡æ˜¯ç»Ÿä¸€åœ°å¯¹ Q/K/V æƒé‡çŸ©é˜µè¿›è¡Œä½ç§©è¿‘ä¼¼å¹¶ç»“åˆåè®­ç»ƒé‡åŒ– (PTQ)ï¼Œå®ç°ä»¥ä¸‹ä¸‰ç‚¹ï¼š
å‡å°‘å‚æ•°é‡ã€è®¡ç®—é‡ã€ç¼“å­˜å ç”¨ï¼› ä¿æŒæ¨¡å‹æ€§èƒ½ï¼› æ”¯æŒä½ç²¾åº¦ç¡¬ä»¶éƒ¨ç½²ã€‚ 2. Related Work 2.1 SVD in Large Models Singular Value Decomposition (SVD) æ˜¯ç»å…¸çš„çŸ©é˜µåˆ†è§£æ–¹æ³•ã€‚ å¯¹äºçŸ©é˜µ ( W \in \mathbb{R}^{m \times n} )ï¼Œå¯åˆ†è§£ä¸ºï¼š
$$ W = U \Sigma V^T $$
å…¶ä¸­ï¼š
(U, V) ä¸ºæ­£äº¤çŸ©é˜µï¼› (\Sigma) ä¸ºå¥‡å¼‚å€¼å¯¹è§’çŸ©é˜µï¼› ä¿ç•™å‰ (r) ä¸ªå¥‡å¼‚å€¼å¯å¾—åˆ° rank-(r) è¿‘ä¼¼ï¼š $$ W \approx U_r \Sigma_r V_r^T $$">
  <meta property="og:locale" content="zh-cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-30T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-30T00:00:00+00:00">
    <meta property="article:tag" content="KV Cache">
    <meta property="article:tag" content="Quantization">
    <meta property="article:tag" content="MLLM">
    <meta property="article:tag" content="Paper Note">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models">
<meta name="twitter:description" content="
Conference: NeurIPS&#39;25 Spotlight
Github: https://github.com/SAI-Lab-NYU/QSVD

1. Motivation
Visionâ€“Language Models (VLMs) å¦‚ LLaVAã€BLIP2 ç­‰åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­” (VQA) ç­‰ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†è¿™äº›æ¨¡å‹éœ€è¦æå¤§çš„è®¡ç®—ä¸å­˜å‚¨å¼€é”€ï¼Œå°¤å…¶åœ¨æ¨ç†æ—¶ï¼š

KV Cache å ç”¨é«˜ï¼šæ³¨æ„åŠ›æœºåˆ¶ä¸­éœ€å­˜å‚¨ Keyã€Valueï¼Œæ¯å±‚ç¼“å­˜å¤§å°éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ã€‚
Q/K/V æŠ•å½±é‡å¤è®¡ç®—ï¼šä¸‰ç»„æƒé‡çŸ©é˜µç‹¬ç«‹è®¡ç®—ï¼Œé€ æˆç®—åŠ›æµªè´¹ã€‚
æ¨¡å‹é‡åŒ–å›°éš¾ï¼šæ¿€æ´»åˆ†å¸ƒå­˜åœ¨æç«¯ outliersï¼Œéš¾ä»¥ç¨³å®šè¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ã€‚

QSVD çš„ç›®æ ‡æ˜¯ç»Ÿä¸€åœ°å¯¹ Q/K/V æƒé‡çŸ©é˜µè¿›è¡Œä½ç§©è¿‘ä¼¼å¹¶ç»“åˆåè®­ç»ƒé‡åŒ– (PTQ)ï¼Œå®ç°ä»¥ä¸‹ä¸‰ç‚¹ï¼š

å‡å°‘å‚æ•°é‡ã€è®¡ç®—é‡ã€ç¼“å­˜å ç”¨ï¼›
ä¿æŒæ¨¡å‹æ€§èƒ½ï¼›
æ”¯æŒä½ç²¾åº¦ç¡¬ä»¶éƒ¨ç½²ã€‚



2. Related Work
2.1 SVD in Large Models
Singular Value Decomposition (SVD) æ˜¯ç»å…¸çš„çŸ©é˜µåˆ†è§£æ–¹æ³•ã€‚
å¯¹äºçŸ©é˜µ ( W \in \mathbb{R}^{m \times n} )ï¼Œå¯åˆ†è§£ä¸ºï¼š
$$
W = U \Sigma V^T
$$
å…¶ä¸­ï¼š

(U, V) ä¸ºæ­£äº¤çŸ©é˜µï¼›
(\Sigma) ä¸ºå¥‡å¼‚å€¼å¯¹è§’çŸ©é˜µï¼›
ä¿ç•™å‰ (r) ä¸ªå¥‡å¼‚å€¼å¯å¾—åˆ° rank-(r) è¿‘ä¼¼ï¼š

$$
W \approx U_r \Sigma_r V_r^T
$$">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jjl357.github.io/blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models",
      "item": "https://jjl357.github.io/blog/posts/qsvd----efficient-low-rank-approximation-for-unified-query-key-value-weight-compression-in-low-precision-vision-language-models---neurips25-spotlight/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models",
  "name": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models",
  "description": "\nConference: NeurIPS'25 Spotlight Github: https://github.com/SAI-Lab-NYU/QSVD\n1. Motivation Visionâ€“Language Models (VLMs) å¦‚ LLaVAã€BLIP2 ç­‰åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­” (VQA) ç­‰ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†è¿™äº›æ¨¡å‹éœ€è¦æå¤§çš„è®¡ç®—ä¸å­˜å‚¨å¼€é”€ï¼Œå°¤å…¶åœ¨æ¨ç†æ—¶ï¼š\nKV Cache å ç”¨é«˜ï¼šæ³¨æ„åŠ›æœºåˆ¶ä¸­éœ€å­˜å‚¨ Keyã€Valueï¼Œæ¯å±‚ç¼“å­˜å¤§å°éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ã€‚ Q/K/V æŠ•å½±é‡å¤è®¡ç®—ï¼šä¸‰ç»„æƒé‡çŸ©é˜µç‹¬ç«‹è®¡ç®—ï¼Œé€ æˆç®—åŠ›æµªè´¹ã€‚ æ¨¡å‹é‡åŒ–å›°éš¾ï¼šæ¿€æ´»åˆ†å¸ƒå­˜åœ¨æç«¯ outliersï¼Œéš¾ä»¥ç¨³å®šè¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ã€‚ QSVD çš„ç›®æ ‡æ˜¯ç»Ÿä¸€åœ°å¯¹ Q/K/V æƒé‡çŸ©é˜µè¿›è¡Œä½ç§©è¿‘ä¼¼å¹¶ç»“åˆåè®­ç»ƒé‡åŒ– (PTQ)ï¼Œå®ç°ä»¥ä¸‹ä¸‰ç‚¹ï¼š\nå‡å°‘å‚æ•°é‡ã€è®¡ç®—é‡ã€ç¼“å­˜å ç”¨ï¼› ä¿æŒæ¨¡å‹æ€§èƒ½ï¼› æ”¯æŒä½ç²¾åº¦ç¡¬ä»¶éƒ¨ç½²ã€‚ 2. Related Work 2.1 SVD in Large Models Singular Value Decomposition (SVD) æ˜¯ç»å…¸çš„çŸ©é˜µåˆ†è§£æ–¹æ³•ã€‚ å¯¹äºçŸ©é˜µ ( W \\in \\mathbb{R}^{m \\times n} )ï¼Œå¯åˆ†è§£ä¸ºï¼š\n$$ W = U \\Sigma V^T $$\nå…¶ä¸­ï¼š\n(U, V) ä¸ºæ­£äº¤çŸ©é˜µï¼› (\\Sigma) ä¸ºå¥‡å¼‚å€¼å¯¹è§’çŸ©é˜µï¼› ä¿ç•™å‰ (r) ä¸ªå¥‡å¼‚å€¼å¯å¾—åˆ° rank-(r) è¿‘ä¼¼ï¼š $$ W \\approx U_r \\Sigma_r V_r^T $$\n",
  "keywords": [
    "KV Cache", "Quantization", "MLLM", "Paper Note"
  ],
  "articleBody": "\nConference: NeurIPS'25 Spotlight Github: https://github.com/SAI-Lab-NYU/QSVD\n1. Motivation Visionâ€“Language Models (VLMs) å¦‚ LLaVAã€BLIP2 ç­‰åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­” (VQA) ç­‰ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†è¿™äº›æ¨¡å‹éœ€è¦æå¤§çš„è®¡ç®—ä¸å­˜å‚¨å¼€é”€ï¼Œå°¤å…¶åœ¨æ¨ç†æ—¶ï¼š\nKV Cache å ç”¨é«˜ï¼šæ³¨æ„åŠ›æœºåˆ¶ä¸­éœ€å­˜å‚¨ Keyã€Valueï¼Œæ¯å±‚ç¼“å­˜å¤§å°éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ã€‚ Q/K/V æŠ•å½±é‡å¤è®¡ç®—ï¼šä¸‰ç»„æƒé‡çŸ©é˜µç‹¬ç«‹è®¡ç®—ï¼Œé€ æˆç®—åŠ›æµªè´¹ã€‚ æ¨¡å‹é‡åŒ–å›°éš¾ï¼šæ¿€æ´»åˆ†å¸ƒå­˜åœ¨æç«¯ outliersï¼Œéš¾ä»¥ç¨³å®šè¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ã€‚ QSVD çš„ç›®æ ‡æ˜¯ç»Ÿä¸€åœ°å¯¹ Q/K/V æƒé‡çŸ©é˜µè¿›è¡Œä½ç§©è¿‘ä¼¼å¹¶ç»“åˆåè®­ç»ƒé‡åŒ– (PTQ)ï¼Œå®ç°ä»¥ä¸‹ä¸‰ç‚¹ï¼š\nå‡å°‘å‚æ•°é‡ã€è®¡ç®—é‡ã€ç¼“å­˜å ç”¨ï¼› ä¿æŒæ¨¡å‹æ€§èƒ½ï¼› æ”¯æŒä½ç²¾åº¦ç¡¬ä»¶éƒ¨ç½²ã€‚ 2. Related Work 2.1 SVD in Large Models Singular Value Decomposition (SVD) æ˜¯ç»å…¸çš„çŸ©é˜µåˆ†è§£æ–¹æ³•ã€‚ å¯¹äºçŸ©é˜µ ( W \\in \\mathbb{R}^{m \\times n} )ï¼Œå¯åˆ†è§£ä¸ºï¼š\n$$ W = U \\Sigma V^T $$\nå…¶ä¸­ï¼š\n(U, V) ä¸ºæ­£äº¤çŸ©é˜µï¼› (\\Sigma) ä¸ºå¥‡å¼‚å€¼å¯¹è§’çŸ©é˜µï¼› ä¿ç•™å‰ (r) ä¸ªå¥‡å¼‚å€¼å¯å¾—åˆ° rank-(r) è¿‘ä¼¼ï¼š $$ W \\approx U_r \\Sigma_r V_r^T $$\næˆ–å†™ä½œï¼š\n$$ W \\approx AB, \\quad A = U_r \\Sigma_r^{1/2}, , B = \\Sigma_r^{1/2} V_r^T $$\nSVD åœ¨å¤§æ¨¡å‹å‹ç¼©ä¸­çš„åº”ç”¨å¹¿æ³›ï¼Œä½†é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼š\nFWSVD åŸºäº Fisher ä¿¡æ¯ç¡®å®šé‡è¦å‚æ•°ï¼› ASVD è€ƒè™‘æ¿€æ´» outliersï¼› SVD-LLM / AdaSVD é€šè¿‡è¯¯å·®æ„ŸçŸ¥æˆªæ–­å‡å°‘æŸå¤±ï¼› Palu / SVD-LLM V2 è¿›ä¸€æ­¥ç»“åˆ KV-cache å‹ç¼©ï¼› DeepSeek / MLA å¼•å…¥ latent attentionï¼Œå°† attention ä¸­çš„ Key/Value æŠ•å½±åˆ°ä½ç§©ç©ºé—´ä»¥å‡å°‘è®¡ç®—ã€‚ QSVD çš„åˆ›æ–°åœ¨äºï¼š â†’ å°† Q/K/V æ‹¼æ¥ä¸ºç»Ÿä¸€çŸ©é˜µ è¿›è¡Œ SVDï¼Œå…±äº«ä¸‹æŠ•å½±çŸ©é˜µï¼Œå®ç° KV-cache çš„ç»Ÿä¸€å‹ç¼©ã€‚\n2.2 Quantization for Large Models Post-Training Quantization (PTQ) æ˜¯å®ç°å¤§æ¨¡å‹æ¨ç†é«˜æ•ˆåŒ–çš„å…³é”®æŠ€æœ¯ã€‚\nAffineQuant: é€šè¿‡å¯å­¦ä¹ ä»¿å°„å˜æ¢ä¼˜åŒ–ç¼©æ”¾å› å­ï¼› SmoothQuant: å°†æ¿€æ´» outliers è½¬ç§»åˆ°æƒé‡ï¼› QuaRot / DuQuant / SpinQuant: å¼•å…¥æ­£äº¤æ—‹è½¬çŸ©é˜µ ( H )ï¼Œå¹³æ»‘é€šé“åˆ†å¸ƒã€‚ æ ¸å¿ƒæ€æƒ³ï¼š è‹¥ ( Y = XW )ï¼Œå¯å†™ä½œï¼š\n$$ Y = (XH)(H^TW) $$\nå…¶ä¸­ (H) ä¸ºæ­£äº¤çŸ©é˜µï¼Œå¯ç¦»çº¿é¢„è®¡ç®—ï¼Œä»è€Œåœ¨ä¸æ”¹å˜è¾“å‡ºçš„å‰æä¸‹å¹³æ»‘æ¿€æ´»åˆ†å¸ƒã€‚\né’ˆå¯¹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œè¿‘å¹´ä¹Ÿæœ‰ç‰¹åŒ–çš„é‡åŒ–ç ”ç©¶ï¼š\nQSLAW: å¼•å…¥å¤šæ¨¡æ€ warmup + group-wise scalingï¼› Q-VLM: ç”¨æ¿€æ´»ç†µè¡¡é‡è·¨å±‚ä¾èµ–ï¼› MBQ: å¹³è¡¡è§†è§‰/æ–‡æœ¬æ¨¡æ€æ¢¯åº¦å·®å¼‚ã€‚ ä½†ä»¥å¾€å·¥ä½œå‡æœªæ¢ç´¢ â€œSVD ä¸é‡åŒ–è”åˆä¼˜åŒ–â€ çš„æ–¹æ¡ˆã€‚ QSVD æ˜¯é¦–ä¸ªæå‡ºä½ç§©è”åˆåˆ†è§£ + ä½æ¯”ç‰¹é‡åŒ–çš„é«˜æ•ˆ VLM æ¡†æ¶ã€‚\n3. Contributions | ä¸»è¦è´¡çŒ® Unified Joint SVD on Q/K/V\nå°† Q/K/V ä¸‰ä¸ªæƒé‡æ‹¼æ¥ä¸ºä¸€ä¸ªçŸ©é˜µè¿›è¡Œ SVDï¼› å¾—åˆ°å…±äº«çš„ä¸‹æŠ•å½±çŸ©é˜µ (W_{qkv}^d)ï¼Œæ˜¾è‘—å‡å°‘æƒé‡å‚æ•°ä¸ KV-cache å­˜å‚¨ã€‚ Cross-layer Rank Allocation\næå‡ºåŸºäºæ¢¯åº¦å†…ç§¯çš„ singular value é‡è¦æ€§è¯„ä¼°ï¼› å®ç°å…¨å±€ç§©é¢„ç®—åˆ†é…ï¼Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æœ€å°åŒ–æ¨¡å‹ç§©ã€‚ Low-precision Quantization within SVD Framework\nåœ¨ä½ç§©ç©ºé—´å¼•å…¥æ­£äº¤æ—‹è½¬çŸ©é˜µ (H_1, H_2)ï¼Œæ¶ˆé™¤ä¸­é—´è¡¨ç¤º (C_{qkv}) çš„é€šé“ outlierï¼› æå‡ºå­¦ä¹ å‹æŒ‡æ•°å‚æ•° (\\beta)ï¼Œè‡ªé€‚åº”æ§åˆ¶å¥‡å¼‚å€¼æ”¾ç¼©å¼ºåº¦ã€‚ é«˜æ•ˆä½æ¯”ç‰¹ VLM\nQSVD åœ¨ W8A8ã€W8A4ã€W4A4 ä¸‹å‡ä¿æŒæ¥è¿‘ FP16 æ€§èƒ½ï¼› æ˜¾è‘—é™ä½ KV-cacheã€æƒé‡ä¸æ¨ç† FLOPsã€‚ 4. Method 4.1 Singular-Value Decomposition over Joint QKV Weights ä¼ ç»Ÿæ³¨æ„åŠ›å±‚ä¸­ï¼š\nQuery/Key/Value æƒé‡çŸ©é˜µåˆ†åˆ«ä¸º ( W_q, W_k, W_v \\in \\mathbb{R}^{E \\times E} )ï¼› å¯¹è¾“å…¥ (X \\in \\mathbb{R}^{L \\times E})ï¼Œè®¡ç®—ï¼š ( Q = XW_q, , K = XW_k, , V = XW_v )ã€‚ QSVD å°†è¿™ä¸‰è€…æ‹¼æ¥ä¸ºç»Ÿä¸€çŸ©é˜µï¼š\n$$ W_{concat} = [W_q, W_k, W_v] \\in \\mathbb{R}^{E \\times 3E} $$\nå¯¹å…¶è¿›è¡Œä½ç§©åˆ†è§£ï¼š\n$$ W_{concat} \\approx W^d_r \\Sigma_r W^u_r $$\nå¹¶å®šä¹‰å¹‚æ¬¡åŠ æƒï¼š\n$$ W_{qkv}^d = W^d_r \\Sigma_r^\\beta, \\quad W_{qkv}^u = \\Sigma_r^{1-\\beta} W^u_r $$\næ­¤æ—¶ï¼š\n$$ [W_q, W_k, W_v] \\approx W_{qkv}^d [W_q^u, W_k^u, W_v^u] $$\nParameter / Memory / FLOPs Analysis é¡¹ç›® åŸå§‹ (FP16) ç‹¬ç«‹ SVD è”åˆ SVD (QSVD) å‚æ•°é‡ (3E^2) (6rE) (4rE) ä¸­é—´ç¼“å­˜ (2LE) (2rL) (rL) FLOPs (3LE^2) (6LrE) (4LrE) æ¡ä»¶ (r \u003c 0.75E) å³å¯ä¿è¯å‹ç¼©æ”¶ç›Šæ˜¾è‘—ã€‚ åŒæ—¶è”åˆ SVD ä»…éœ€è®¡ç®—ä¸€æ¬¡ (XW_{qkv}^d)ï¼Œå‡å°‘é‡å¤è®¡ç®—ä¸è®¿å­˜ã€‚\næ¨ç†é˜¶æ®µï¼š\nç¼“å­˜ä¸­é—´è¡¨ç¤ºï¼š $$ C_{qkv} = X W_{qkv}^d $$\né‡æ„ï¼š $$ K = C_{qkv} W_k^u, \\quad V = C_{qkv} W_v^u $$\nç”±æ­¤ï¼Œç¼“å­˜ä»…éœ€å­˜å‚¨ (C_{qkv}) è€Œé (K,V)ï¼ŒKV-cache å‡åŠä»¥ä¸Šã€‚\n4.2 Cross-layer Rank Allocation for Low-rank SVD ä½ç§©åˆ†è§£çš„å…³é”®åœ¨äºï¼šå¦‚ä½•ç¡®å®šæ¯å±‚åº”ä¿ç•™çš„ rank (r)ã€‚ QSVD æå‡ºåŸºäºæ¢¯åº¦å†…ç§¯çš„ singular value importance scoringã€‚\nåŸºæœ¬æ¨å¯¼ï¼š è®¾ $$ W = \\sum_{i=1}^{n} \\sigma_i u_i v_i^T $$\nè‹¥æˆªæ–­ç¬¬ i ä¸ªå¥‡å¼‚å€¼ï¼š $$ \\Delta W_{\\sigma_i} = \\sigma_i u_i v_i^T $$\nå¯¹è®­ç»ƒæŸå¤± (L_t(W)) åšä¸€é˜¶è¿‘ä¼¼ï¼š $$ L_t(W - \\Delta W_{\\sigma_i}) \\approx L_t(W) - \\sum_{j,k} \\Delta W_{\\sigma_i}[j,k] \\frac{\\partial L_t}{\\partial W[j,k]} $$\nå³æŸå¤±å˜åŒ–ï¼š $$ \\Delta L_{\\sigma_i} = \\langle \\Delta W_{\\sigma_i}, G_W \\rangle_F $$\nå¤šæ ·æœ¬æœŸæœ›çš„é‡è¦æ€§åˆ†æ•°ï¼š $$ \\hat I_{\\sigma_i} = \\mathbb{E}{x\\sim D}\\left[(\\Delta L{\\sigma_i})^2\\right] \\approx \\frac{1}{N}\\sum_{n=1}^{N}\\left( \\sum_{j,k} \\Delta W_{\\sigma_i}[j,k] G_W^{(n)}[j,k] \\right)^2 $$\nç›´æ¥è®¡ç®—éœ€ (O(E^3)) å†…å­˜ã€‚ è®ºæ–‡æ¨å¯¼ç­‰ä»·è¡¨è¾¾ï¼ˆAppendix A.1ï¼‰ï¼š\n$$ \\hat I_{\\sigma_i} = \\frac{1}{N} \\sum_{n=1}^N \\sigma_i^2 [U^T G_W^{(n)} V]_{(i,i)}^2 $$\næ­¤å¼ä»…éœ€ (O(E^2)) å†…å­˜ã€‚\nCross-layer Global Ranking å¯¹æ¯å±‚è®¡ç®—æ‰€æœ‰å¥‡å¼‚å€¼çš„é‡è¦æ€§åˆ†æ•°ï¼› å°†å…¨æ¨¡å‹æ‰€æœ‰å¥‡å¼‚å€¼æ’åºï¼› åœ¨æ€» rank é¢„ç®— (k) ä¸‹ä¿ç•™å‰ (k) ä¸ªï¼› å…¶ä½™å¥‡å¼‚å€¼ç½®é›¶ï¼ˆtruncationï¼‰ã€‚ è¯¥æ–¹æ³•èƒ½å®ç°å…¨æ¨¡å‹å±‚é—´ rank è‡ªé€‚åº”åˆ†é…ï¼Œç¡®ä¿ä¿ç•™å¯¹æ•´ä½“ä»»åŠ¡æœ€å…³é”®çš„æ–¹å‘ã€‚\n4.3 Post-Training Quantization for Low-rank VLMs SVD å‹ç¼©åï¼Œæ¨¡å‹å†…éƒ¨ä»å­˜åœ¨ä¸¥é‡çš„é€šé“ outlierï¼Œç‰¹åˆ«æ˜¯ä¸­é—´è¡¨ç¤ºï¼š\n$$ C_{qkv} = X W^d_{qkv} $$\nä¸ºæ­¤ï¼ŒQSVD æå‡ºæ—‹è½¬ + Î² å­¦ä¹  çš„è”åˆé‡åŒ–æ–¹æ¡ˆã€‚\n(1) Orthogonal Rotation (Hâ‚, Hâ‚‚) å¼•å…¥ä¸¤ä¸ªæ­£äº¤çŸ©é˜µ (H_1, H_2)ï¼Œä½¿å¾—ï¼š\n$$ Y = X W^d_{qkv} W^u_{qkv} = (XH_1^T)(H_1 W^d_{qkv} H_2^T)(H_2 W^u_{qkv}) $$\nè¿™æ ·åœ¨é‡åŒ–æ—¶å¯å†™ä¸ºï¼š\n$$ C_{qkv} \\approx Q(XH_1^T) Q(H_1 W^d_{qkv} H_2^T) $$\næ­£äº¤æ—‹è½¬å¯ä»¥åœ¨ä¸æ”¹å˜è¾“å‡ºçš„å‰æä¸‹å¹³æ»‘æ¿€æ´»åˆ†å¸ƒï¼Œä»è€Œå‡å°‘é‡åŒ–è¯¯å·®ã€‚\n(2) Î² å­¦ä¹ æœºåˆ¶ å› ï¼š $$ W^d_{qkv} = W^d_r \\Sigma_r^{\\beta} $$\nè‹¥å¥‡å¼‚å€¼åˆ†å¸ƒè·¨åº¦å¤§ï¼Œåˆ™ï¼š $$ C_{qkv} = XW^d_r \\Sigma_r^{\\beta} $$ ä¸­æŸäº›é€šé“ä¼šå‡ºç°æç«¯å€¼ã€‚\nä¸ºç¼“è§£æ­¤é—®é¢˜ï¼ŒQSVD é€šè¿‡åœ¨æ ¡å‡†é›†ä¸Šæœ€å°åŒ–é‡åŒ–å‰åè¾“å‡ºè¯¯å·®å­¦ä¹ æœ€ä¼˜ (\\beta)ï¼š\n$$ \\min_\\beta \\sum_{d\\in D} | Y_d - Yâ€™_d |_2^2 $$\nå…¶ä¸­ (Y_d) ä¸ºéé‡åŒ–è¾“å‡ºï¼Œ(Yâ€™_d) ä¸ºé‡åŒ–è¾“å‡ºã€‚\n(\\beta) å¯åœ¨æ¯å±‚ç‹¬ç«‹å­¦ä¹ ï¼Œé€šå¸¸å–å€¼ 0.4â€“0.8 èŒƒå›´ã€‚\n(3) Quantization Details Component Scheme Note Weight Per-channel symmetric RTN å¯å­¦ä¹  clip ratio Activation Per-token symmetric æ—‹è½¬ååˆ†å¸ƒæ›´å¹³æ»‘ Bitwidth 8/4 bits æ”¯æŒ W8A8, W8A4, W4A4 Calibration 256 ScienceQA æ ·æœ¬ ç”¨äº rank åˆ†é… \u0026 Î² å­¦ä¹  5. Evaluation å…³äº (R_1) ä¸ (R_2)\nç¬¦å· å«ä¹‰ æ•°å­¦å®šä¹‰ ç›´è§‚è§£é‡Š ( R_1 ) ç»¼åˆè®¡ç®—ä¸å‚æ•°å‹ç¼©æ¯”ç‡ ( R_1 = \\frac{\\alpha_i}{\\alpha_{fp}} = \\frac{\\gamma_i}{\\gamma_{fp}} ) è¡¨ç¤ºå½“å‰æ–¹æ¡ˆï¼ˆiï¼‰ç›¸å¯¹äºåŸå§‹ FP16 æ¨¡å‹çš„æƒé‡å‚æ•°é‡å’Œè®¡ç®— FLOPs æ¯”ä¾‹ ( R_2 ) ç¼“å­˜å‹ç¼©æ¯”ç‡ ( R_2 = \\frac{\\eta_i}{\\eta_{fp}} ) è¡¨ç¤ºå½“å‰æ–¹æ¡ˆçš„ KV ç¼“å­˜ï¼ˆæˆ–ä¸­é—´è¡¨ç¤ºï¼‰å åŸæ¨¡å‹çš„æ¯”ä¾‹ å…¶ä¸­ï¼š\n(\\alpha) è¡¨ç¤ºæ¨¡å‹å‚æ•°é‡ï¼› (\\gamma) è¡¨ç¤ºè®¡ç®— FLOPsï¼› (\\eta) è¡¨ç¤º KV cache æˆ– intermediate buffer å¤§å°ã€‚ 5.1 Experimental Setup Models: SmolVLM-2B, LLaVA-v1.5 7B / 13B, LLaVA-Next 7B / 13B Tasks: ScienceQA, VizWiz, SEED-Bench-IMG, HallusionBench Calibration Set: 256 samples from ScienceQA Hardware: NVIDIA RTX A6000 (48GB) Metrics: Accuracy / Groundedness / Hallucination Rate 5.2 SVD-only (QSVD-noQ) Results: åœ¨ LLaVA-v1.5 13B ä¸Šï¼Œå½“ rank ratio (R_1 = 46.7%, R_2 = 17.5%) æ—¶ï¼š\nScienceQA-IMG: accuracy ä¸‹é™ \u003c1%ï¼› VizWiz ä¸Šç”šè‡³ç•¥ä¼˜äº FP16ã€‚ Insights: è”åˆ SVD çš„å…±äº«ä¸‹æŠ•å½±ä½¿å‹ç¼©åæ€§èƒ½æ›´ç¨³å®šï¼› å½“ (r) è¿‡ä½ (\u003c0.5E) æ—¶ä»èƒ½ä¿æŒè¾ƒå¥½è¡¨ç°ï¼› åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸Šå‡ºç°æ­£å‘æ­£åˆ™åŒ–æ•ˆåº”ï¼ˆå‡è½» hallucinationï¼‰ã€‚ 5.3 QSVD (SVD + Quantization) (a) W8A8 Results åœ¨ç›¸åŒå‹ç¼©æ¯”ä¸‹ QSVD æ˜æ˜¾ä¼˜äº DuQuantã€Q-VLMï¼› åœ¨ LLaVA-v1.5 13B ä¸Šæ¥è¿‘ FP16 ç²¾åº¦ï¼› ä¸­é—´ç¼“å­˜ç¼©å‡è‡³ 18.75%ã€‚ (b) W8A4 Results æ¿€è¿›å‹ç¼©ä¸‹ (KV ä»… 9.38%) QSVD ä»æ¥è¿‘ FP16ï¼› å…¶ä»–æ–¹æ³•å¦‚ DuQuant ç²¾åº¦æ˜¾è‘—ä¸‹é™ã€‚ (c) W4A4 Results QASVD / DuQuant å‡ ä¹é€€åŒ–ï¼› QSVD ä»ä¿æŒå¯ç”¨ç²¾åº¦ï¼ŒéªŒè¯äº†æ—‹è½¬ + Î² å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚ 5.4 Ablation Studies (1) Rank Allocation æ–¹æ³• æè¿° ç»“æœ Uniform-rank æ¯å±‚ç›¸åŒ r æœ€å·® Fisher-based æŒ‰ Fisher ä¿¡æ¯åˆ†é… ä¸­ç­‰ QSVD-importance åŸºäºæ¢¯åº¦å†…ç§¯é‡è¦æ€§è¯„åˆ† æœ€ä½³æ€§èƒ½ =\u003e QSVD çš„é‡è¦æ€§æ‰“åˆ†èƒ½æ›´ç²¾å‡†åœ°æ•æ‰å¯¹ä»»åŠ¡å…³é”®çš„æ–¹å‘ã€‚\n(2) Î² å­¦ä¹  å›ºå®š Î² = 0.0/0.4/0.8 vs å­¦ä¹  Î²ï¼› åœ¨ W4A4 ä¸‹å­¦ä¹  Î² å¸¦æ¥ 4â€“6% ç²¾åº¦æå‡ï¼› åœ¨é«˜æ¯”ç‰¹ä¸‹ï¼ˆW8A8ï¼‰å½±å“è¾ƒå°ï¼› è¡¨æ˜ Î² åœ¨æç«¯ä½æ¯”ç‰¹å‹ç¼©ä¸­å°¤ä¸ºå…³é”®ã€‚ (3) Hallucination Reduction åœ¨ HallusionBench ä¸Šï¼š\nModel FP16 QSVD-noQ Î”Groundedness LLaVA-v1.5 13B 26.7 30.3 +3.6 è¯´æ˜ä½ç§©è¿‘ä¼¼å¸¦æ¥è½»å¾®â€œæ­£åˆ™åŒ–â€æ•ˆæœï¼Œæœ‰åŠ©å‡å°‘å¹»è§‰ç”Ÿæˆã€‚\n5.5 Latency and Throughput QSVD-noQ åœ¨ 4070 GPUï¼ˆ12GBï¼‰ä¸Šé¿å… KV-cache offloadï¼› QSVD (W8A8) åœ¨ seq=4K æ—¶æœ€é«˜è¾¾ 13.1Ã— æ¨ç†åŠ é€Ÿï¼› å­˜å‚¨ä¸è®¡ç®—åŒæ—¶ä¸‹é™ï¼Œæ¨ç†å»¶è¿Ÿæ˜¾è‘—æ”¹å–„ã€‚ 5.6 Overall Findings Setting Memory â†“ FLOPs â†“ Accuracy â†“ SVD-noQ (r/E=0.5) 65% 60% \u003c1% QSVD W8A8 80% 70% \u003c2% QSVD W4A4 90% 80% \u003c5% 6. Conclusion \u0026 Discussion Summary QSVD æ˜¯é¦–ä¸ªç»“åˆ joint SVD + importance-based rank allocation + orthogonal quantization çš„ VLM å‹ç¼©\næ¡†æ¶ã€‚ åœ¨å¤šæ¨¡å‹ä¸ŠéªŒè¯ï¼Œè¾¾åˆ°äº†ï¼š\nKV-cache æœ€é«˜ç¼©å‡ 82%ï¼› æ¨ç†é€Ÿåº¦æå‡ 13Ã—ï¼› ç²¾åº¦ä¿æŒåœ¨ FP16 Â±1% ä»¥å†…ã€‚ Limitation \u0026 Future Work ç›®å‰ä»…é’ˆå¯¹ self-attention å±‚ï¼› Future work: extend to FFN å±‚ / cross-modal adapterï¼› é«˜æ•ˆ VLM å¯èƒ½å¯¼è‡´æ»¥ç”¨ï¼ˆéšç§ã€ç›‘æ§ï¼‰ï¼Œéœ€è¿›ä¸€æ­¥ä¼¦ç†ç ”ç©¶ã€‚ ",
  "wordCount" : "948",
  "inLanguage": "zh-cn",
  "datePublished": "2025-10-30T00:00:00Z",
  "dateModified": "2025-10-30T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jjl357.github.io/blog/posts/qsvd----efficient-low-rank-approximation-for-unified-query-key-value-weight-compression-in-low-precision-vision-language-models---neurips25-spotlight/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JJ's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jjl357.github.io/blog/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
  <nav class="nav">
    <div class="logo">
      
      <a href="https://jjl357.github.io/" accesskey="h" title="ğŸ¥› â˜• ğŸµ (Alt + H)">
        <span>ğŸ¥› â˜• ğŸµ</span>
        
      </a>

      <div class="logo-switches">
        <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
          <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      </div>
    </div>
    <ul id="menu">
    </ul>
  </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models
    </h1>
    <div class="post-meta"><span title='2025-10-30 00:00:00 +0000 UTC'>October 30, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/title.png"></p>
<p><strong>Conference:</strong> NeurIPS'25 Spotlight
<strong>Github:</strong> <a href="https://github.com/SAI-Lab-NYU/QSVD">https://github.com/SAI-Lab-NYU/QSVD</a></p>
<hr>
<h2 id="1-motivation">1. Motivation<a hidden class="anchor" aria-hidden="true" href="#1-motivation">#</a></h2>
<p>Visionâ€“Language Models (VLMs) å¦‚ LLaVAã€BLIP2 ç­‰åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­” (VQA) ç­‰ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†è¿™äº›æ¨¡å‹éœ€è¦æå¤§çš„è®¡ç®—ä¸å­˜å‚¨å¼€é”€ï¼Œå°¤å…¶åœ¨æ¨ç†æ—¶ï¼š</p>
<ul>
<li><strong>KV Cache å ç”¨é«˜</strong>ï¼šæ³¨æ„åŠ›æœºåˆ¶ä¸­éœ€å­˜å‚¨ Keyã€Valueï¼Œæ¯å±‚ç¼“å­˜å¤§å°éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ã€‚</li>
<li><strong>Q/K/V æŠ•å½±é‡å¤è®¡ç®—</strong>ï¼šä¸‰ç»„æƒé‡çŸ©é˜µç‹¬ç«‹è®¡ç®—ï¼Œé€ æˆç®—åŠ›æµªè´¹ã€‚</li>
<li><strong>æ¨¡å‹é‡åŒ–å›°éš¾</strong>ï¼šæ¿€æ´»åˆ†å¸ƒå­˜åœ¨æç«¯ outliersï¼Œéš¾ä»¥ç¨³å®šè¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ã€‚</li>
</ul>
<p>QSVD çš„ç›®æ ‡æ˜¯<strong>ç»Ÿä¸€åœ°å¯¹ Q/K/V æƒé‡çŸ©é˜µè¿›è¡Œä½ç§©è¿‘ä¼¼</strong>å¹¶ç»“åˆ<strong>åè®­ç»ƒé‡åŒ– (PTQ)</strong>ï¼Œå®ç°ä»¥ä¸‹ä¸‰ç‚¹ï¼š</p>
<ol>
<li>å‡å°‘å‚æ•°é‡ã€è®¡ç®—é‡ã€ç¼“å­˜å ç”¨ï¼›</li>
<li>ä¿æŒæ¨¡å‹æ€§èƒ½ï¼›</li>
<li>æ”¯æŒä½ç²¾åº¦ç¡¬ä»¶éƒ¨ç½²ã€‚</li>
</ol>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/figure1.png"></p>
<hr>
<h2 id="2-related-work">2. Related Work<a hidden class="anchor" aria-hidden="true" href="#2-related-work">#</a></h2>
<h3 id="21-svd-in-large-models">2.1 SVD in Large Models<a hidden class="anchor" aria-hidden="true" href="#21-svd-in-large-models">#</a></h3>
<p><strong>Singular Value Decomposition (SVD)</strong> æ˜¯ç»å…¸çš„çŸ©é˜µåˆ†è§£æ–¹æ³•ã€‚
å¯¹äºçŸ©é˜µ ( W \in \mathbb{R}^{m \times n} )ï¼Œå¯åˆ†è§£ä¸ºï¼š</p>
<p>$$
W = U \Sigma V^T
$$</p>
<p>å…¶ä¸­ï¼š</p>
<ul>
<li>(U, V) ä¸ºæ­£äº¤çŸ©é˜µï¼›</li>
<li>(\Sigma) ä¸ºå¥‡å¼‚å€¼å¯¹è§’çŸ©é˜µï¼›</li>
<li>ä¿ç•™å‰ (r) ä¸ªå¥‡å¼‚å€¼å¯å¾—åˆ° rank-(r) è¿‘ä¼¼ï¼š</li>
</ul>
<p>$$
W \approx U_r \Sigma_r V_r^T
$$</p>
<p>æˆ–å†™ä½œï¼š</p>
<p>$$
W \approx AB, \quad A = U_r \Sigma_r^{1/2}, , B = \Sigma_r^{1/2} V_r^T
$$</p>
<hr>
<p>SVD åœ¨å¤§æ¨¡å‹å‹ç¼©ä¸­çš„åº”ç”¨å¹¿æ³›ï¼Œä½†é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼š</p>
<ul>
<li><strong>FWSVD</strong> åŸºäº Fisher ä¿¡æ¯ç¡®å®šé‡è¦å‚æ•°ï¼›</li>
<li><strong>ASVD</strong> è€ƒè™‘æ¿€æ´» outliersï¼›</li>
<li><strong>SVD-LLM / AdaSVD</strong> é€šè¿‡è¯¯å·®æ„ŸçŸ¥æˆªæ–­å‡å°‘æŸå¤±ï¼›</li>
<li><strong>Palu / SVD-LLM V2</strong> è¿›ä¸€æ­¥ç»“åˆ KV-cache å‹ç¼©ï¼›</li>
<li><strong>DeepSeek / MLA</strong> å¼•å…¥ latent attentionï¼Œå°† attention ä¸­çš„ Key/Value æŠ•å½±åˆ°ä½ç§©ç©ºé—´ä»¥å‡å°‘è®¡ç®—ã€‚</li>
</ul>
<p>QSVD çš„åˆ›æ–°åœ¨äºï¼š
â†’ å°† <strong>Q/K/V æ‹¼æ¥ä¸ºç»Ÿä¸€çŸ©é˜µ</strong> è¿›è¡Œ SVDï¼Œ<strong>å…±äº«ä¸‹æŠ•å½±çŸ©é˜µ</strong>ï¼Œå®ç° KV-cache çš„ç»Ÿä¸€å‹ç¼©ã€‚</p>
<hr>
<h3 id="22-quantization-for-large-models">2.2 Quantization for Large Models<a hidden class="anchor" aria-hidden="true" href="#22-quantization-for-large-models">#</a></h3>
<p><strong>Post-Training Quantization (PTQ)</strong> æ˜¯å®ç°å¤§æ¨¡å‹æ¨ç†é«˜æ•ˆåŒ–çš„å…³é”®æŠ€æœ¯ã€‚</p>
<ul>
<li><strong>AffineQuant</strong>: é€šè¿‡å¯å­¦ä¹ ä»¿å°„å˜æ¢ä¼˜åŒ–ç¼©æ”¾å› å­ï¼›</li>
<li><strong>SmoothQuant</strong>: å°†æ¿€æ´» outliers è½¬ç§»åˆ°æƒé‡ï¼›</li>
<li><strong>QuaRot / DuQuant / SpinQuant</strong>: å¼•å…¥æ­£äº¤æ—‹è½¬çŸ©é˜µ ( H )ï¼Œå¹³æ»‘é€šé“åˆ†å¸ƒã€‚</li>
</ul>
<p>æ ¸å¿ƒæ€æƒ³ï¼š
è‹¥ ( Y = XW )ï¼Œå¯å†™ä½œï¼š</p>
<p>$$
Y = (XH)(H^TW)
$$</p>
<p>å…¶ä¸­ (H) ä¸ºæ­£äº¤çŸ©é˜µï¼Œå¯ç¦»çº¿é¢„è®¡ç®—ï¼Œä»è€Œåœ¨ä¸æ”¹å˜è¾“å‡ºçš„å‰æä¸‹å¹³æ»‘æ¿€æ´»åˆ†å¸ƒã€‚</p>
<hr>
<p>é’ˆå¯¹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œè¿‘å¹´ä¹Ÿæœ‰ç‰¹åŒ–çš„é‡åŒ–ç ”ç©¶ï¼š</p>
<ul>
<li><strong>QSLAW</strong>: å¼•å…¥å¤šæ¨¡æ€ warmup + group-wise scalingï¼›</li>
<li><strong>Q-VLM</strong>: ç”¨æ¿€æ´»ç†µè¡¡é‡è·¨å±‚ä¾èµ–ï¼›</li>
<li><strong>MBQ</strong>: å¹³è¡¡è§†è§‰/æ–‡æœ¬æ¨¡æ€æ¢¯åº¦å·®å¼‚ã€‚</li>
</ul>
<p>ä½†ä»¥å¾€å·¥ä½œå‡æœªæ¢ç´¢ <strong>â€œSVD ä¸é‡åŒ–è”åˆä¼˜åŒ–â€</strong> çš„æ–¹æ¡ˆã€‚
QSVD æ˜¯é¦–ä¸ªæå‡º<strong>ä½ç§©è”åˆåˆ†è§£ + ä½æ¯”ç‰¹é‡åŒ–</strong>çš„é«˜æ•ˆ VLM æ¡†æ¶ã€‚</p>
<hr>
<h2 id="3-contributions--ä¸»è¦è´¡çŒ®">3. Contributions | ä¸»è¦è´¡çŒ®<a hidden class="anchor" aria-hidden="true" href="#3-contributions--ä¸»è¦è´¡çŒ®">#</a></h2>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/figure1.png"></p>
<ol>
<li>
<p><strong>Unified Joint SVD on Q/K/V</strong></p>
<ul>
<li>å°† Q/K/V ä¸‰ä¸ªæƒé‡æ‹¼æ¥ä¸ºä¸€ä¸ªçŸ©é˜µè¿›è¡Œ SVDï¼›</li>
<li>å¾—åˆ°å…±äº«çš„ä¸‹æŠ•å½±çŸ©é˜µ (W_{qkv}^d)ï¼Œæ˜¾è‘—å‡å°‘æƒé‡å‚æ•°ä¸ KV-cache å­˜å‚¨ã€‚</li>
</ul>
</li>
<li>
<p><strong>Cross-layer Rank Allocation</strong></p>
<ul>
<li>æå‡ºåŸºäºæ¢¯åº¦å†…ç§¯çš„ singular value é‡è¦æ€§è¯„ä¼°ï¼›</li>
<li>å®ç°å…¨å±€ç§©é¢„ç®—åˆ†é…ï¼Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æœ€å°åŒ–æ¨¡å‹ç§©ã€‚</li>
</ul>
</li>
<li>
<p><strong>Low-precision Quantization within SVD Framework</strong></p>
<ul>
<li>åœ¨ä½ç§©ç©ºé—´å¼•å…¥æ­£äº¤æ—‹è½¬çŸ©é˜µ (H_1, H_2)ï¼Œæ¶ˆé™¤ä¸­é—´è¡¨ç¤º (C_{qkv}) çš„é€šé“ outlierï¼›</li>
<li>æå‡ºå­¦ä¹ å‹æŒ‡æ•°å‚æ•° (\beta)ï¼Œè‡ªé€‚åº”æ§åˆ¶å¥‡å¼‚å€¼æ”¾ç¼©å¼ºåº¦ã€‚</li>
</ul>
</li>
<li>
<p><strong>é«˜æ•ˆä½æ¯”ç‰¹ VLM</strong></p>
<ul>
<li>QSVD åœ¨ W8A8ã€W8A4ã€W4A4 ä¸‹å‡ä¿æŒæ¥è¿‘ FP16 æ€§èƒ½ï¼›</li>
<li>æ˜¾è‘—é™ä½ KV-cacheã€æƒé‡ä¸æ¨ç† FLOPsã€‚</li>
</ul>
</li>
</ol>
<hr>
<h2 id="4-method">4. Method<a hidden class="anchor" aria-hidden="true" href="#4-method">#</a></h2>
<h3 id="41-singular-value-decomposition-over-joint-qkv-weights">4.1 Singular-Value Decomposition over Joint QKV Weights<a hidden class="anchor" aria-hidden="true" href="#41-singular-value-decomposition-over-joint-qkv-weights">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/figure2.png"></p>
<p>ä¼ ç»Ÿæ³¨æ„åŠ›å±‚ä¸­ï¼š</p>
<ul>
<li>Query/Key/Value æƒé‡çŸ©é˜µåˆ†åˆ«ä¸º ( W_q, W_k, W_v \in \mathbb{R}^{E \times E} )ï¼›</li>
<li>å¯¹è¾“å…¥ (X \in \mathbb{R}^{L \times E})ï¼Œè®¡ç®—ï¼š
( Q = XW_q, , K = XW_k, , V = XW_v )ã€‚</li>
</ul>
<hr>
<p>QSVD å°†è¿™ä¸‰è€…æ‹¼æ¥ä¸ºç»Ÿä¸€çŸ©é˜µï¼š</p>
<p>$$
W_{concat} = [W_q, W_k, W_v] \in \mathbb{R}^{E \times 3E}
$$</p>
<p>å¯¹å…¶è¿›è¡Œä½ç§©åˆ†è§£ï¼š</p>
<p>$$
W_{concat} \approx W^d_r \Sigma_r W^u_r
$$</p>
<p>å¹¶å®šä¹‰å¹‚æ¬¡åŠ æƒï¼š</p>
<p>$$
W_{qkv}^d = W^d_r \Sigma_r^\beta, \quad W_{qkv}^u = \Sigma_r^{1-\beta} W^u_r
$$</p>
<p>æ­¤æ—¶ï¼š</p>
<p>$$
[W_q, W_k, W_v] \approx W_{qkv}^d [W_q^u, W_k^u, W_v^u]
$$</p>
<hr>
<h4 id="parameter--memory--flops-analysis">Parameter / Memory / FLOPs Analysis<a hidden class="anchor" aria-hidden="true" href="#parameter--memory--flops-analysis">#</a></h4>
<table>
  <thead>
      <tr>
          <th>é¡¹ç›®</th>
          <th>åŸå§‹ (FP16)</th>
          <th>ç‹¬ç«‹ SVD</th>
          <th>è”åˆ SVD (QSVD)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>å‚æ•°é‡</td>
          <td>(3E^2)</td>
          <td>(6rE)</td>
          <td><strong>(4rE)</strong></td>
      </tr>
      <tr>
          <td>ä¸­é—´ç¼“å­˜</td>
          <td>(2LE)</td>
          <td>(2rL)</td>
          <td><strong>(rL)</strong></td>
      </tr>
      <tr>
          <td>FLOPs</td>
          <td>(3LE^2)</td>
          <td>(6LrE)</td>
          <td><strong>(4LrE)</strong></td>
      </tr>
  </tbody>
</table>
<p>æ¡ä»¶ (r &lt; 0.75E) å³å¯ä¿è¯å‹ç¼©æ”¶ç›Šæ˜¾è‘—ã€‚
åŒæ—¶è”åˆ SVD ä»…éœ€è®¡ç®—ä¸€æ¬¡ (XW_{qkv}^d)ï¼Œå‡å°‘é‡å¤è®¡ç®—ä¸è®¿å­˜ã€‚</p>
<hr>
<p>æ¨ç†é˜¶æ®µï¼š</p>
<ol>
<li>
<p>ç¼“å­˜ä¸­é—´è¡¨ç¤ºï¼š
$$
C_{qkv} = X W_{qkv}^d
$$</p>
</li>
<li>
<p>é‡æ„ï¼š
$$
K = C_{qkv} W_k^u, \quad V = C_{qkv} W_v^u
$$</p>
</li>
</ol>
<p>ç”±æ­¤ï¼Œç¼“å­˜ä»…éœ€å­˜å‚¨ (C_{qkv}) è€Œé (K,V)ï¼ŒKV-cache å‡åŠä»¥ä¸Šã€‚</p>
<hr>
<h3 id="42-cross-layer-rank-allocation-for-low-rank-svd">4.2 Cross-layer Rank Allocation for Low-rank SVD<a hidden class="anchor" aria-hidden="true" href="#42-cross-layer-rank-allocation-for-low-rank-svd">#</a></h3>
<p>ä½ç§©åˆ†è§£çš„å…³é”®åœ¨äºï¼šå¦‚ä½•ç¡®å®šæ¯å±‚åº”ä¿ç•™çš„ rank (r)ã€‚
QSVD æå‡ºåŸºäºæ¢¯åº¦å†…ç§¯çš„ <strong>singular value importance scoring</strong>ã€‚</p>
<hr>
<h4 id="åŸºæœ¬æ¨å¯¼">åŸºæœ¬æ¨å¯¼ï¼š<a hidden class="anchor" aria-hidden="true" href="#åŸºæœ¬æ¨å¯¼">#</a></h4>
<p>è®¾
$$
W = \sum_{i=1}^{n} \sigma_i u_i v_i^T
$$</p>
<p>è‹¥æˆªæ–­ç¬¬ i ä¸ªå¥‡å¼‚å€¼ï¼š
$$
\Delta W_{\sigma_i} = \sigma_i u_i v_i^T
$$</p>
<p>å¯¹è®­ç»ƒæŸå¤± (L_t(W)) åšä¸€é˜¶è¿‘ä¼¼ï¼š
$$
L_t(W - \Delta W_{\sigma_i}) \approx L_t(W) - \sum_{j,k} \Delta W_{\sigma_i}[j,k] \frac{\partial L_t}{\partial W[j,k]}
$$</p>
<p>å³æŸå¤±å˜åŒ–ï¼š
$$
\Delta L_{\sigma_i} = \langle \Delta W_{\sigma_i}, G_W \rangle_F
$$</p>
<hr>
<h4 id="å¤šæ ·æœ¬æœŸæœ›çš„é‡è¦æ€§åˆ†æ•°">å¤šæ ·æœ¬æœŸæœ›çš„é‡è¦æ€§åˆ†æ•°ï¼š<a hidden class="anchor" aria-hidden="true" href="#å¤šæ ·æœ¬æœŸæœ›çš„é‡è¦æ€§åˆ†æ•°">#</a></h4>
<p>$$
\hat I_{\sigma_i} = \mathbb{E}<em>{x\sim D}\left[(\Delta L</em>{\sigma_i})^2\right] \approx \frac{1}{N}\sum_{n=1}^{N}\left( \sum_{j,k} \Delta W_{\sigma_i}[j,k] G_W^{(n)}[j,k] \right)^2
$$</p>
<p>ç›´æ¥è®¡ç®—éœ€ (O(E^3)) å†…å­˜ã€‚
è®ºæ–‡æ¨å¯¼ç­‰ä»·è¡¨è¾¾ï¼ˆAppendix A.1ï¼‰ï¼š</p>
<p>$$
\hat I_{\sigma_i} = \frac{1}{N} \sum_{n=1}^N \sigma_i^2 [U^T G_W^{(n)} V]_{(i,i)}^2
$$</p>
<p>æ­¤å¼ä»…éœ€ (O(E^2)) å†…å­˜ã€‚</p>
<hr>
<h4 id="cross-layer-global-ranking">Cross-layer Global Ranking<a hidden class="anchor" aria-hidden="true" href="#cross-layer-global-ranking">#</a></h4>
<ol>
<li>å¯¹æ¯å±‚è®¡ç®—æ‰€æœ‰å¥‡å¼‚å€¼çš„é‡è¦æ€§åˆ†æ•°ï¼›</li>
<li>å°†å…¨æ¨¡å‹æ‰€æœ‰å¥‡å¼‚å€¼æ’åºï¼›</li>
<li>åœ¨æ€» rank é¢„ç®— (k) ä¸‹ä¿ç•™å‰ (k) ä¸ªï¼›</li>
<li>å…¶ä½™å¥‡å¼‚å€¼ç½®é›¶ï¼ˆtruncationï¼‰ã€‚</li>
</ol>
<p>è¯¥æ–¹æ³•èƒ½å®ç°å…¨æ¨¡å‹å±‚é—´ rank è‡ªé€‚åº”åˆ†é…ï¼Œç¡®ä¿ä¿ç•™å¯¹æ•´ä½“ä»»åŠ¡æœ€å…³é”®çš„æ–¹å‘ã€‚</p>
<hr>
<h3 id="43-post-training-quantization-for-low-rank-vlms">4.3 Post-Training Quantization for Low-rank VLMs<a hidden class="anchor" aria-hidden="true" href="#43-post-training-quantization-for-low-rank-vlms">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/figure3.png"></p>
<p>SVD å‹ç¼©åï¼Œæ¨¡å‹å†…éƒ¨ä»å­˜åœ¨ä¸¥é‡çš„é€šé“ outlierï¼Œç‰¹åˆ«æ˜¯ä¸­é—´è¡¨ç¤ºï¼š</p>
<p>$$
C_{qkv} = X W^d_{qkv}
$$</p>
<p>ä¸ºæ­¤ï¼ŒQSVD æå‡º<strong>æ—‹è½¬ + Î² å­¦ä¹ </strong> çš„è”åˆé‡åŒ–æ–¹æ¡ˆã€‚</p>
<hr>
<h4 id="1-orthogonal-rotation-h-h">(1) Orthogonal Rotation (Hâ‚, Hâ‚‚)<a hidden class="anchor" aria-hidden="true" href="#1-orthogonal-rotation-h-h">#</a></h4>
<p>å¼•å…¥ä¸¤ä¸ªæ­£äº¤çŸ©é˜µ (H_1, H_2)ï¼Œä½¿å¾—ï¼š</p>
<p>$$
Y = X W^d_{qkv} W^u_{qkv} = (XH_1^T)(H_1 W^d_{qkv} H_2^T)(H_2 W^u_{qkv})
$$</p>
<p>è¿™æ ·åœ¨é‡åŒ–æ—¶å¯å†™ä¸ºï¼š</p>
<p>$$
C_{qkv} \approx Q(XH_1^T) Q(H_1 W^d_{qkv} H_2^T)
$$</p>
<p>æ­£äº¤æ—‹è½¬å¯ä»¥åœ¨ä¸æ”¹å˜è¾“å‡ºçš„å‰æä¸‹å¹³æ»‘æ¿€æ´»åˆ†å¸ƒï¼Œä»è€Œå‡å°‘é‡åŒ–è¯¯å·®ã€‚</p>
<hr>
<h4 id="2-Î²-å­¦ä¹ æœºåˆ¶">(2) Î² å­¦ä¹ æœºåˆ¶<a hidden class="anchor" aria-hidden="true" href="#2-Î²-å­¦ä¹ æœºåˆ¶">#</a></h4>
<p>å› ï¼š
$$
W^d_{qkv} = W^d_r \Sigma_r^{\beta}
$$</p>
<p>è‹¥å¥‡å¼‚å€¼åˆ†å¸ƒè·¨åº¦å¤§ï¼Œåˆ™ï¼š
$$
C_{qkv} = XW^d_r \Sigma_r^{\beta}
$$
ä¸­æŸäº›é€šé“ä¼šå‡ºç°æç«¯å€¼ã€‚</p>
<p>ä¸ºç¼“è§£æ­¤é—®é¢˜ï¼ŒQSVD é€šè¿‡åœ¨æ ¡å‡†é›†ä¸Šæœ€å°åŒ–é‡åŒ–å‰åè¾“å‡ºè¯¯å·®å­¦ä¹ æœ€ä¼˜ (\beta)ï¼š</p>
<p>$$
\min_\beta \sum_{d\in D} | Y_d - Y&rsquo;_d |_2^2
$$</p>
<p>å…¶ä¸­ (Y_d) ä¸ºéé‡åŒ–è¾“å‡ºï¼Œ(Y&rsquo;_d) ä¸ºé‡åŒ–è¾“å‡ºã€‚</p>
<p>(\beta) å¯åœ¨æ¯å±‚ç‹¬ç«‹å­¦ä¹ ï¼Œé€šå¸¸å–å€¼ 0.4â€“0.8 èŒƒå›´ã€‚</p>
<hr>
<h4 id="3-quantization-details">(3) Quantization Details<a hidden class="anchor" aria-hidden="true" href="#3-quantization-details">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Scheme</th>
          <th>Note</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Weight</td>
          <td>Per-channel symmetric RTN</td>
          <td>å¯å­¦ä¹  clip ratio</td>
      </tr>
      <tr>
          <td>Activation</td>
          <td>Per-token symmetric</td>
          <td>æ—‹è½¬ååˆ†å¸ƒæ›´å¹³æ»‘</td>
      </tr>
      <tr>
          <td>Bitwidth</td>
          <td>8/4 bits</td>
          <td>æ”¯æŒ W8A8, W8A4, W4A4</td>
      </tr>
      <tr>
          <td>Calibration</td>
          <td>256 ScienceQA æ ·æœ¬</td>
          <td>ç”¨äº rank åˆ†é… &amp; Î² å­¦ä¹ </td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="5-evaluation">5. Evaluation<a hidden class="anchor" aria-hidden="true" href="#5-evaluation">#</a></h2>
<p><strong>å…³äº (R_1) ä¸ (R_2)</strong></p>
<table>
  <thead>
      <tr>
          <th>ç¬¦å·</th>
          <th>å«ä¹‰</th>
          <th>æ•°å­¦å®šä¹‰</th>
          <th>ç›´è§‚è§£é‡Š</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>( R_1 )</td>
          <td><strong>ç»¼åˆè®¡ç®—ä¸å‚æ•°å‹ç¼©æ¯”ç‡</strong></td>
          <td>( R_1 = \frac{\alpha_i}{\alpha_{fp}} = \frac{\gamma_i}{\gamma_{fp}} )</td>
          <td>è¡¨ç¤ºå½“å‰æ–¹æ¡ˆï¼ˆiï¼‰ç›¸å¯¹äºåŸå§‹ FP16 æ¨¡å‹çš„<strong>æƒé‡å‚æ•°é‡å’Œè®¡ç®— FLOPs æ¯”ä¾‹</strong></td>
      </tr>
      <tr>
          <td>( R_2 )</td>
          <td><strong>ç¼“å­˜å‹ç¼©æ¯”ç‡</strong></td>
          <td>( R_2 = \frac{\eta_i}{\eta_{fp}} )</td>
          <td>è¡¨ç¤ºå½“å‰æ–¹æ¡ˆçš„ KV ç¼“å­˜ï¼ˆæˆ–ä¸­é—´è¡¨ç¤ºï¼‰å åŸæ¨¡å‹çš„æ¯”ä¾‹</td>
      </tr>
  </tbody>
</table>
<p>å…¶ä¸­ï¼š</p>
<ul>
<li>(\alpha) è¡¨ç¤ºæ¨¡å‹å‚æ•°é‡ï¼›</li>
<li>(\gamma) è¡¨ç¤ºè®¡ç®— FLOPsï¼›</li>
<li>(\eta) è¡¨ç¤º KV cache æˆ– intermediate buffer å¤§å°ã€‚</li>
</ul>
<h3 id="51-experimental-setup">5.1 Experimental Setup<a hidden class="anchor" aria-hidden="true" href="#51-experimental-setup">#</a></h3>
<ul>
<li><strong>Models</strong>: SmolVLM-2B, LLaVA-v1.5 7B / 13B, LLaVA-Next 7B / 13B</li>
<li><strong>Tasks</strong>: ScienceQA, VizWiz, SEED-Bench-IMG, HallusionBench</li>
<li><strong>Calibration Set</strong>: 256 samples from ScienceQA</li>
<li><strong>Hardware</strong>: NVIDIA RTX A6000 (48GB)</li>
<li><strong>Metrics</strong>: Accuracy / Groundedness / Hallucination Rate</li>
</ul>
<hr>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/table1.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/table2.png"></p>
<h3 id="52-svd-only-qsvd-noq">5.2 SVD-only (QSVD-noQ)<a hidden class="anchor" aria-hidden="true" href="#52-svd-only-qsvd-noq">#</a></h3>
<h4 id="results">Results:<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h4>
<ul>
<li>
<p>åœ¨ LLaVA-v1.5 13B ä¸Šï¼Œå½“ rank ratio (R_1 = 46.7%, R_2 = 17.5%) æ—¶ï¼š</p>
<ul>
<li>ScienceQA-IMG: accuracy ä¸‹é™ &lt;1%ï¼›</li>
<li>VizWiz ä¸Šç”šè‡³ç•¥ä¼˜äº FP16ã€‚</li>
</ul>
</li>
</ul>
<h4 id="insights">Insights:<a hidden class="anchor" aria-hidden="true" href="#insights">#</a></h4>
<ul>
<li>è”åˆ SVD çš„å…±äº«ä¸‹æŠ•å½±ä½¿å‹ç¼©åæ€§èƒ½æ›´ç¨³å®šï¼›</li>
<li>å½“ (r) è¿‡ä½ (&lt;0.5E) æ—¶ä»èƒ½ä¿æŒè¾ƒå¥½è¡¨ç°ï¼›</li>
<li>åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸Šå‡ºç°æ­£å‘æ­£åˆ™åŒ–æ•ˆåº”ï¼ˆå‡è½» hallucinationï¼‰ã€‚</li>
</ul>
<hr>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/table3.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/table4.png"></p>
<h3 id="53-qsvd-svd--quantization">5.3 QSVD (SVD + Quantization)<a hidden class="anchor" aria-hidden="true" href="#53-qsvd-svd--quantization">#</a></h3>
<h4 id="a-w8a8-results">(a) W8A8 Results<a hidden class="anchor" aria-hidden="true" href="#a-w8a8-results">#</a></h4>
<ul>
<li>åœ¨ç›¸åŒå‹ç¼©æ¯”ä¸‹ QSVD æ˜æ˜¾ä¼˜äº DuQuantã€Q-VLMï¼›</li>
<li>åœ¨ LLaVA-v1.5 13B ä¸Šæ¥è¿‘ FP16 ç²¾åº¦ï¼›</li>
<li>ä¸­é—´ç¼“å­˜ç¼©å‡è‡³ 18.75%ã€‚</li>
</ul>
<h4 id="b-w8a4-results">(b) W8A4 Results<a hidden class="anchor" aria-hidden="true" href="#b-w8a4-results">#</a></h4>
<ul>
<li>æ¿€è¿›å‹ç¼©ä¸‹ (KV ä»… 9.38%) QSVD ä»æ¥è¿‘ FP16ï¼›</li>
<li>å…¶ä»–æ–¹æ³•å¦‚ DuQuant ç²¾åº¦æ˜¾è‘—ä¸‹é™ã€‚</li>
</ul>
<h4 id="c-w4a4-results">(c) W4A4 Results<a hidden class="anchor" aria-hidden="true" href="#c-w4a4-results">#</a></h4>
<ul>
<li>QASVD / DuQuant å‡ ä¹é€€åŒ–ï¼›</li>
<li>QSVD ä»ä¿æŒå¯ç”¨ç²¾åº¦ï¼ŒéªŒè¯äº†æ—‹è½¬ + Î² å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<hr>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/table5.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/table6.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/figure4.png"></p>
<h3 id="54-ablation-studies">5.4 Ablation Studies<a hidden class="anchor" aria-hidden="true" href="#54-ablation-studies">#</a></h3>
<h4 id="1-rank-allocation">(1) Rank Allocation<a hidden class="anchor" aria-hidden="true" href="#1-rank-allocation">#</a></h4>
<table>
  <thead>
      <tr>
          <th>æ–¹æ³•</th>
          <th>æè¿°</th>
          <th>ç»“æœ</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Uniform-rank</td>
          <td>æ¯å±‚ç›¸åŒ r</td>
          <td>æœ€å·®</td>
      </tr>
      <tr>
          <td>Fisher-based</td>
          <td>æŒ‰ Fisher ä¿¡æ¯åˆ†é…</td>
          <td>ä¸­ç­‰</td>
      </tr>
      <tr>
          <td><strong>QSVD-importance</strong></td>
          <td>åŸºäºæ¢¯åº¦å†…ç§¯é‡è¦æ€§è¯„åˆ†</td>
          <td><strong>æœ€ä½³æ€§èƒ½</strong></td>
      </tr>
  </tbody>
</table>
<p>=&gt; QSVD çš„é‡è¦æ€§æ‰“åˆ†èƒ½æ›´ç²¾å‡†åœ°æ•æ‰å¯¹ä»»åŠ¡å…³é”®çš„æ–¹å‘ã€‚</p>
<hr>
<h4 id="2-Î²-å­¦ä¹ ">(2) Î² å­¦ä¹ <a hidden class="anchor" aria-hidden="true" href="#2-Î²-å­¦ä¹ ">#</a></h4>
<ul>
<li>å›ºå®š Î² = 0.0/0.4/0.8 vs å­¦ä¹  Î²ï¼›</li>
<li>åœ¨ W4A4 ä¸‹å­¦ä¹  Î² å¸¦æ¥ 4â€“6% ç²¾åº¦æå‡ï¼›</li>
<li>åœ¨é«˜æ¯”ç‰¹ä¸‹ï¼ˆW8A8ï¼‰å½±å“è¾ƒå°ï¼›</li>
<li>è¡¨æ˜ Î² åœ¨æç«¯ä½æ¯”ç‰¹å‹ç¼©ä¸­å°¤ä¸ºå…³é”®ã€‚</li>
</ul>
<hr>
<h4 id="3-hallucination-reduction">(3) Hallucination Reduction<a hidden class="anchor" aria-hidden="true" href="#3-hallucination-reduction">#</a></h4>
<p>åœ¨ HallusionBench ä¸Šï¼š</p>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>FP16</th>
          <th>QSVD-noQ</th>
          <th>Î”Groundedness</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>LLaVA-v1.5 13B</td>
          <td>26.7</td>
          <td>30.3</td>
          <td><strong>+3.6</strong></td>
      </tr>
  </tbody>
</table>
<p>è¯´æ˜ä½ç§©è¿‘ä¼¼å¸¦æ¥è½»å¾®â€œæ­£åˆ™åŒ–â€æ•ˆæœï¼Œæœ‰åŠ©å‡å°‘å¹»è§‰ç”Ÿæˆã€‚</p>
<hr>
<h3 id="55-latency-and-throughput">5.5 Latency and Throughput<a hidden class="anchor" aria-hidden="true" href="#55-latency-and-throughput">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/QSVD/figure4.png"></p>
<ul>
<li>QSVD-noQ åœ¨ 4070 GPUï¼ˆ12GBï¼‰ä¸Šé¿å… KV-cache offloadï¼›</li>
<li>QSVD (W8A8) åœ¨ seq=4K æ—¶æœ€é«˜è¾¾ <strong>13.1Ã— æ¨ç†åŠ é€Ÿ</strong>ï¼›</li>
<li>å­˜å‚¨ä¸è®¡ç®—åŒæ—¶ä¸‹é™ï¼Œæ¨ç†å»¶è¿Ÿæ˜¾è‘—æ”¹å–„ã€‚</li>
</ul>
<hr>
<h3 id="56-overall-findings">5.6 Overall Findings<a hidden class="anchor" aria-hidden="true" href="#56-overall-findings">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Setting</th>
          <th>Memory â†“</th>
          <th>FLOPs â†“</th>
          <th>Accuracy â†“</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>SVD-noQ (r/E=0.5)</td>
          <td>65%</td>
          <td>60%</td>
          <td>&lt;1%</td>
      </tr>
      <tr>
          <td>QSVD W8A8</td>
          <td>80%</td>
          <td>70%</td>
          <td>&lt;2%</td>
      </tr>
      <tr>
          <td>QSVD W4A4</td>
          <td>90%</td>
          <td>80%</td>
          <td>&lt;5%</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="6-conclusion--discussion">6. Conclusion &amp; Discussion<a hidden class="anchor" aria-hidden="true" href="#6-conclusion--discussion">#</a></h2>
<h3 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h3>
<p>QSVD æ˜¯é¦–ä¸ªç»“åˆ <strong>joint SVD + importance-based rank allocation + orthogonal quantization</strong> çš„ VLM å‹ç¼©</p>
<p>æ¡†æ¶ã€‚
åœ¨å¤šæ¨¡å‹ä¸ŠéªŒè¯ï¼Œè¾¾åˆ°äº†ï¼š</p>
<ul>
<li>KV-cache æœ€é«˜ç¼©å‡ 82%ï¼›</li>
<li>æ¨ç†é€Ÿåº¦æå‡ 13Ã—ï¼›</li>
<li>ç²¾åº¦ä¿æŒåœ¨ FP16 Â±1% ä»¥å†…ã€‚</li>
</ul>
<hr>
<h3 id="limitation--future-work">Limitation &amp; Future Work<a hidden class="anchor" aria-hidden="true" href="#limitation--future-work">#</a></h3>
<ul>
<li>ç›®å‰ä»…é’ˆå¯¹ self-attention å±‚ï¼›</li>
<li>Future work: extend to FFN å±‚ / cross-modal adapterï¼›</li>
<li>é«˜æ•ˆ VLM å¯èƒ½å¯¼è‡´æ»¥ç”¨ï¼ˆéšç§ã€ç›‘æ§ï¼‰ï¼Œéœ€è¿›ä¸€æ­¥ä¼¦ç†ç ”ç©¶ã€‚</li>
</ul>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jjl357.github.io/blog/tags/kv-cache/">KV Cache</a></li>
      <li><a href="https://jjl357.github.io/blog/tags/quantization/">Quantization</a></li>
      <li><a href="https://jjl357.github.io/blog/tags/mllm/">MLLM</a></li>
      <li><a href="https://jjl357.github.io/blog/tags/paper-note/">Paper Note</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://jjl357.github.io/blog/">JJ&#39;s Blog</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
