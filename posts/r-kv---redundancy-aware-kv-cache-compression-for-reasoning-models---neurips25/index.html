<!DOCTYPE html>
<html lang="zh-cn" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>R-KV: Redundancy-aware KV Cache Compression for Reasoning Models | JJ&#39;s Blog</title>
<meta name="keywords" content="KV Cache, Paper Note">
<meta name="description" content="
Conference: NeurIPS&#39;25
Github: https://github.com/Zefan-Cai/R-KV

My Thoughts
R-KV é’ˆå¯¹previous worksåœ¨efficient reasoningä¸­ç”±äºåªä¾é attention scoresè€Œé€ æˆè¿‡å¤šåœ°ä¿ç•™äº†redundant tokens(é‡å¤è€Œä¸”å¯¹æ¨ç†æ²¡æœ‰ä¿¡æ¯å¢ç›Šçš„token),R-KVçš„é‡ç‚¹æˆ‘è§‰å¾—æ˜¯å¢åŠ äº†Redundancy Estimation via Semantic Similarityæ¥è§£å†³è¿™ä¸ªé—®é¢˜,å°±Evaluationçš„ç»“æœæ¥è¯´ï¼ŒR-KVçš„æ•ˆæœæ˜¯éå¸¸å¥½çš„ï¼Œå³æå‡äº†Throughputè¿˜maintainäº†performanceï¼Œç”šè‡³åœ¨budgetå……è¶³çš„æƒ…å†µä¸‹è¿˜å¯ä»¥åšåˆ°æç‚¹ã€‚
1. Motivation
Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning.
However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference.
For instance, a DeepSeek-R1-Distill-Llama-8B model may generate 32K tokens to solve a complex math problem, consuming 15.5GB of memory to load model weights and 4.1GB to store the KV cache.
This long CoT (chain-of-thought) generation necessitates the development of KV cache compression.">
<meta name="author" content="">
<link rel="canonical" href="https://jjl357.github.io/blog/posts/r-kv---redundancy-aware-kv-cache-compression-for-reasoning-models---neurips25/">
<link crossorigin="anonymous" href="https://jjl357.github.io/blog/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jjl357.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jjl357.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jjl357.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jjl357.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://jjl357.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh-cn" href="https://jjl357.github.io/blog/posts/r-kv---redundancy-aware-kv-cache-compression-for-reasoning-models---neurips25/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><script type="text/javascript"
        async
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<meta property="og:url" content="https://jjl357.github.io/blog/posts/r-kv---redundancy-aware-kv-cache-compression-for-reasoning-models---neurips25/">
  <meta property="og:site_name" content="JJ&#39;s Blog">
  <meta property="og:title" content="R-KV: Redundancy-aware KV Cache Compression for Reasoning Models">
  <meta property="og:description" content="
Conference: NeurIPS&#39;25
Github: https://github.com/Zefan-Cai/R-KV
My Thoughts R-KV é’ˆå¯¹previous worksåœ¨efficient reasoningä¸­ç”±äºåªä¾é attention scoresè€Œé€ æˆè¿‡å¤šåœ°ä¿ç•™äº†redundant tokens(é‡å¤è€Œä¸”å¯¹æ¨ç†æ²¡æœ‰ä¿¡æ¯å¢ç›Šçš„token),R-KVçš„é‡ç‚¹æˆ‘è§‰å¾—æ˜¯å¢åŠ äº†Redundancy Estimation via Semantic Similarityæ¥è§£å†³è¿™ä¸ªé—®é¢˜,å°±Evaluationçš„ç»“æœæ¥è¯´ï¼ŒR-KVçš„æ•ˆæœæ˜¯éå¸¸å¥½çš„ï¼Œå³æå‡äº†Throughputè¿˜maintainäº†performanceï¼Œç”šè‡³åœ¨budgetå……è¶³çš„æƒ…å†µä¸‹è¿˜å¯ä»¥åšåˆ°æç‚¹ã€‚
1. Motivation Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference.
For instance, a DeepSeek-R1-Distill-Llama-8B model may generate 32K tokens to solve a complex math problem, consuming 15.5GB of memory to load model weights and 4.1GB to store the KV cache. This long CoT (chain-of-thought) generation necessitates the development of KV cache compression.">
  <meta property="og:locale" content="zh-cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-25T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-25T00:00:00+00:00">
    <meta property="article:tag" content="KV Cache">
    <meta property="article:tag" content="Paper Note">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="R-KV: Redundancy-aware KV Cache Compression for Reasoning Models">
<meta name="twitter:description" content="
Conference: NeurIPS&#39;25
Github: https://github.com/Zefan-Cai/R-KV

My Thoughts
R-KV é’ˆå¯¹previous worksåœ¨efficient reasoningä¸­ç”±äºåªä¾é attention scoresè€Œé€ æˆè¿‡å¤šåœ°ä¿ç•™äº†redundant tokens(é‡å¤è€Œä¸”å¯¹æ¨ç†æ²¡æœ‰ä¿¡æ¯å¢ç›Šçš„token),R-KVçš„é‡ç‚¹æˆ‘è§‰å¾—æ˜¯å¢åŠ äº†Redundancy Estimation via Semantic Similarityæ¥è§£å†³è¿™ä¸ªé—®é¢˜,å°±Evaluationçš„ç»“æœæ¥è¯´ï¼ŒR-KVçš„æ•ˆæœæ˜¯éå¸¸å¥½çš„ï¼Œå³æå‡äº†Throughputè¿˜maintainäº†performanceï¼Œç”šè‡³åœ¨budgetå……è¶³çš„æƒ…å†µä¸‹è¿˜å¯ä»¥åšåˆ°æç‚¹ã€‚
1. Motivation
Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning.
However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference.
For instance, a DeepSeek-R1-Distill-Llama-8B model may generate 32K tokens to solve a complex math problem, consuming 15.5GB of memory to load model weights and 4.1GB to store the KV cache.
This long CoT (chain-of-thought) generation necessitates the development of KV cache compression.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jjl357.github.io/blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
      "item": "https://jjl357.github.io/blog/posts/r-kv---redundancy-aware-kv-cache-compression-for-reasoning-models---neurips25/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
  "name": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
  "description": "\nConference: NeurIPS'25\nGithub: https://github.com/Zefan-Cai/R-KV\nMy Thoughts R-KV é’ˆå¯¹previous worksåœ¨efficient reasoningä¸­ç”±äºåªä¾é attention scoresè€Œé€ æˆè¿‡å¤šåœ°ä¿ç•™äº†redundant tokens(é‡å¤è€Œä¸”å¯¹æ¨ç†æ²¡æœ‰ä¿¡æ¯å¢ç›Šçš„token),R-KVçš„é‡ç‚¹æˆ‘è§‰å¾—æ˜¯å¢åŠ äº†Redundancy Estimation via Semantic Similarityæ¥è§£å†³è¿™ä¸ªé—®é¢˜,å°±Evaluationçš„ç»“æœæ¥è¯´ï¼ŒR-KVçš„æ•ˆæœæ˜¯éå¸¸å¥½çš„ï¼Œå³æå‡äº†Throughputè¿˜maintainäº†performanceï¼Œç”šè‡³åœ¨budgetå……è¶³çš„æƒ…å†µä¸‹è¿˜å¯ä»¥åšåˆ°æç‚¹ã€‚\n1. Motivation Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference.\nFor instance, a DeepSeek-R1-Distill-Llama-8B model may generate 32K tokens to solve a complex math problem, consuming 15.5GB of memory to load model weights and 4.1GB to store the KV cache. This long CoT (chain-of-thought) generation necessitates the development of KV cache compression.\n",
  "keywords": [
    "KV Cache", "Paper Note"
  ],
  "articleBody": "\nConference: NeurIPS'25\nGithub: https://github.com/Zefan-Cai/R-KV\nMy Thoughts R-KV é’ˆå¯¹previous worksåœ¨efficient reasoningä¸­ç”±äºåªä¾é attention scoresè€Œé€ æˆè¿‡å¤šåœ°ä¿ç•™äº†redundant tokens(é‡å¤è€Œä¸”å¯¹æ¨ç†æ²¡æœ‰ä¿¡æ¯å¢ç›Šçš„token),R-KVçš„é‡ç‚¹æˆ‘è§‰å¾—æ˜¯å¢åŠ äº†Redundancy Estimation via Semantic Similarityæ¥è§£å†³è¿™ä¸ªé—®é¢˜,å°±Evaluationçš„ç»“æœæ¥è¯´ï¼ŒR-KVçš„æ•ˆæœæ˜¯éå¸¸å¥½çš„ï¼Œå³æå‡äº†Throughputè¿˜maintainäº†performanceï¼Œç”šè‡³åœ¨budgetå……è¶³çš„æƒ…å†µä¸‹è¿˜å¯ä»¥åšåˆ°æç‚¹ã€‚\n1. Motivation Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference.\nFor instance, a DeepSeek-R1-Distill-Llama-8B model may generate 32K tokens to solve a complex math problem, consuming 15.5GB of memory to load model weights and 4.1GB to store the KV cache. This long CoT (chain-of-thought) generation necessitates the development of KV cache compression.\nè¿™ç§æ¨¡å‹åœ¨æ¨ç†æ—¶ç»å¸¸ä¼šäº§ç”Ÿæé•¿çš„ç”Ÿæˆåºåˆ—ï¼Œå…¶ä¸­åŒ…å«å¤§é‡å†—ä½™å†…å®¹ï¼Œå¦‚ï¼š\né‡å¤çš„è‡ªæˆ‘åæ€ï¼ˆself-reflectionï¼‰ å¤šæ¬¡é‡å¤çš„æ¨ç†å°è¯• å†—é•¿çš„è‡ªå¯¹è¯å¼è¾“å‡º è¿™äº›éƒ¨åˆ†è™½ç„¶åœ¨è¯­ä¹‰ä¸Šè´¡çŒ®æœ‰é™ï¼Œå´æ˜¾è‘—å¢åŠ äº† KV ç¼“å­˜é•¿åº¦å’Œå†…å­˜è´Ÿæ‹…ã€‚\n2. Challenge While chain-of-thought inference significantly improves performance on complex reasoning tasks, existing KV cache compression methods often fail in this scenario.\nåŸå› ï¼š ä¼ ç»Ÿå‹ç¼©æ–¹æ³•å¤šåŸºäº attention é‡è¦æ€§è¿‡æ»¤ï¼ˆå¦‚æŒ‰æ³¨æ„åŠ›æƒé‡ç­›é€‰ tokenï¼‰ï¼Œä½† reasoning æ¨¡å‹çš„è¾“å‡ºå¾€å¾€åŒ…å«é«˜åº¦é‡å¤çš„æ®µè½ã€‚ ç”±äºè¿™äº›é‡å¤æ®µè½ä¹‹é—´äº’ç›¸å¼ºåŒ–æ³¨æ„åŠ›å¾—åˆ†ï¼Œç®€å•åŸºäºæ³¨æ„åŠ›çš„è£å‰ªç­–ç•¥ä¼šï¼š\né”™è¯¯åœ°ä¿ç•™å†—ä½™é‡å¤å†…å®¹ï¼› é”™è¯¯åœ°åˆ é™¤ç¦»æ•£ä½†å…³é”®çš„ reasoning ç‰‡æ®µã€‚ æœ€ç»ˆå¯¼è‡´æ¨ç†é“¾æ–­è£‚æˆ–ç­”æ¡ˆé”™è¯¯ã€‚\n3. Contribution We propose R-KV (Redundancy-aware KV Cache Compression) â€” a novel decoding-time KV compression method for reasoning LLMs.\næ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼š\nAttention-based importance scoringï¼šæ ¹æ®æ³¨æ„åŠ›åˆ†æ•°è¡¡é‡æ¯ä¸ª token çš„ä¸Šä¸‹æ–‡é‡è¦æ€§ã€‚ Dynamic redundancy scoringï¼šé€šè¿‡ Key å‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦åŠ¨æ€ä¼°è®¡ token çš„å†—ä½™åº¦ã€‚ Joint eviction mechanismï¼šç»“åˆé‡è¦æ€§ä¸å†—ä½™åº¦çš„è”åˆæ‰“åˆ†æœºåˆ¶ï¼Œä¼˜åŒ–ç¼“å­˜åˆ©ç”¨ç‡ã€‚ å®éªŒç»“æœï¼š\nåœ¨ä¿ç•™ä»… 10â€“34% çš„åŸå§‹ KV cache æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¸æœªå‹ç¼©æ¨¡å‹å‡ ä¹ä¸€è‡´ï¼› åœ¨ AIME-24 æ•°æ®é›†ä¸Šï¼Œä»…ç”¨ 16% KV cache å³å®ç° 105% of FullKV accuracyï¼› æ–¹æ³•è®­ç»ƒæ— å…³ã€æ¨¡å‹æ— å…³ï¼Œå¯ç›´æ¥éƒ¨ç½²äº RL rollout æˆ–æ¨ç†æœåŠ¡ã€‚ 4. Observation 4.1 Redundancy in Reasoning Models è§‚å¯Ÿï¼š\nå¤šç§ reasoning æ¨¡å‹ï¼ˆDeepSeek-R1-Distill-Llama-8Bã€DeepSeek-R1-Distill-Qwen-7Bã€Qwen-14Bï¼‰ç”Ÿæˆçš„è¾“å‡ºé•¿åº¦æ™®éæ¯” ground truth é•¿ 8 å€ä»¥ä¸Šï¼› è¾“å‡ºä¸­ 1-gram ä¸ 2-gram çš„å¹³å‡é‡å¤é¢‘ç‡æ˜¾è‘—é«˜äºå‚è€ƒç­”æ¡ˆï¼Œè¡¨æ˜ç”Ÿæˆæ–‡æœ¬å­˜åœ¨æ˜¾è‘—çš„å†—ä½™ç»“æ„ã€‚ 4.2 Failure of Existing KV Compression Methods ç°æœ‰å¦‚ SnapKV ç­‰ attention-based æ–¹æ³•å€¾å‘äºä¿ç•™é‡å¤çš„ reasoning ç‰‡æ®µï¼Œä¾‹å¦‚é‡å¤çš„ â€œreflectionâ€ æˆ– â€œfinal answerâ€ æ®µè½ï¼Œé€ æˆæœ‰æ•ˆä¸Šä¸‹æ–‡è¢«å†—ä½™ä¿¡æ¯æŒ¤å ã€‚\nR-KV çš„ç›®æ ‡æ­£æ˜¯é’ˆå¯¹è¿™ç§å†—ä½™å¤±æ•ˆæ¨¡å¼è¿›è¡Œä¿®æ­£ã€‚\n5. Method R-KV çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨ decoding é˜¶æ®µï¼ˆå³ç”Ÿæˆæ—¶ï¼‰å®æ—¶å‹ç¼© KV cacheï¼Œ åŠ¨æ€é€‰æ‹©æœ€æœ‰ç”¨çš„å†å² tokenï¼ŒåŒæ—¶è¿‡æ»¤æ‰å†—ä½™å†…å®¹ã€‚\næ•´ä¸ªæ–¹æ³•åŒ…æ‹¬ä»¥ä¸‹æ¨¡å—ï¼š\n5.1 Decoding-time Compression ä¸å¤§å¤šæ•°ä»…ä½œç”¨äº prefilling é˜¶æ®µ çš„æ–¹æ³•ä¸åŒï¼ŒR-KV é’ˆå¯¹ decoding é˜¶æ®µè®¾è®¡ã€‚\nè®¾å®šä¸¤ä¸ªç¼“å­˜åŒºï¼š\nCache åŒºï¼šå­˜æ”¾ä¿ç•™çš„ KV å¯¹ï¼Œå®¹é‡ä¸º ( B_{\\text{budget}} ) Buffer åŒºï¼šå­˜æ”¾æœ€æ–°ç”Ÿæˆçš„ tokenï¼Œå®¹é‡ä¸º ( B_{\\text{buffer}} ) æ€»å†…å­˜éœ€æ±‚ï¼š $$ B_{\\text{total}} = B_{\\text{budget}} + B_{\\text{buffer}} $$\nå‹ç¼©æ­¥éª¤ï¼ˆå¾ªç¯æ‰§è¡Œï¼‰ï¼š\næ¨¡å‹ç”Ÿæˆä¸€æ®µé•¿åº¦ä¸º ( B_{\\text{buffer}} ) çš„æ–‡æœ¬ï¼› Buffer çš„æœ€å (\\alpha) ä¸ª tokenï¼ˆç§°ä½œ observation tokensï¼‰å§‹ç»ˆä¿ç•™ï¼› æ‹¼æ¥ï¼š $$ n = B_{\\text{budget}} + B_{\\text{buffer}} - \\alpha $$ å½¢æˆå€™é€‰é›†åˆï¼› å¯¹æ‰€æœ‰å€™é€‰ token è®¡ç®—ç»¼åˆå¾—åˆ†ï¼› é€‰å‡ºå‰ (k = B_{\\text{budget}} - \\alpha) ä¸ª token + (\\alpha) ä¸ª observation token ç»„æˆæ–°çš„ cacheã€‚ 5.2 Importance Scoring via Attention Weights å¯¹æ¯ä¸ª attention head (h)ï¼š\n$$ A^{(h)} = \\operatorname{softmax}!\\left(\\frac{Q^{(h)} (K^{(h)})^\\top}{\\sqrt{d}}\\right) $$\nå…¶ä¸­ï¼š\n( Q^{(h)} \\in \\mathbb{R}^{\\alpha \\times d} )ï¼šæœ€è¿‘ (\\alpha) ä¸ª queryï¼› ( K^{(h)} \\in \\mathbb{R}^{n \\times d} )ï¼šå€™é€‰ keyã€‚ ä¸ºäº†é˜²æ­¢æç«¯æ³¨æ„åŠ›å€¼å¯¼è‡´ä¸ç¨³å®šï¼Œ åœ¨ token ç»´åº¦ä¸Šè¿›è¡Œæ»‘çª—æœ€å¤§æ± åŒ–ï¼ˆwindow size = (2W)ï¼‰ï¼š\n$$ \\tilde A^{(h)}{j,i} = \\max(A^{(h)}{j,i-W},\\ldots,A^{(h)}_{j,i+W-1}) $$\næœ€åå¯¹ query å¹³å‡åŒ–å¾—åˆ° importance scoreï¼š\n$$ I^{(h)}i = \\frac{1}{\\alpha}\\sum{j=0}^{\\alpha-1}\\tilde A^{(h)}_{j,i} $$\n5.3 Redundancy Estimation via Semantic Similarity ç›®æ ‡ï¼šæ‰¾å‡ºè¯­ä¹‰é‡å¤çš„ tokenã€‚\nå½’ä¸€åŒ–æ¯ä¸ª key å‘é‡ï¼š $$ \\hat K^{(h)}_i = \\frac{K^{(h)}_i}{|K^{(h)}_i|_2 + \\varepsilon} $$\nè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µï¼š $$ S^{(h)} = \\hat K^{(h)} (\\hat K^{(h)})^\\top,\\quad S^{(h)}_{i,i}=0 $$\nä¸ºé¿å…è¯¯åˆ æœ€è¿‘ç›¸å…³çš„é‡å¤ tokenï¼Œå¯¹æ¯ä¸ª token (i)ï¼š\nå–ä¸å…¶ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼ (T) çš„é›†åˆ (\\mathcal{I}^{(h)}i = {j | S^{(h)}{j,i} \u003e T}) ä»…ä¿ç•™æœ€è¿‘ (\\beta) ä¸ªï¼ˆå³ç”Ÿæˆæ—¶é—´ä¸Šé è¿‘çš„ï¼‰ï¼Œå…¶ä»–ç›¸ä¼¼ token è®¾ä¸º 0ï¼š $$ S^{(h)}{j,i} = 0,\\quad \\forall j \\in \\mathcal{I}^{(h)}{i,\\beta} $$ è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦å¹¶å½’ä¸€åŒ–ï¼š $$ \\bar S^{(h)}i = \\frac{1}{n}\\sum{j=0}^{n-1} S^{(h)}_{j,i}, \\quad R^{(h)}_i = \\operatorname{softmax}(\\bar S^{(h)}) $$\né«˜ç›¸ä¼¼åº¦æ„å‘³ç€å†—ä½™åº¦é«˜ï¼Œåº”è¢«æ·˜æ±°ã€‚\n5.4 Joint Selection Strategy æœ€ç»ˆç»¼åˆå¾—åˆ†ä¸ºï¼š\n$$ Z^{(h)}_i = \\lambda I^{(h)}_i - (1 - \\lambda) R^{(h)}_i $$\n(\\lambda)ï¼šæ§åˆ¶é‡è¦æ€§ä¸å†—ä½™çš„å¹³è¡¡\n(\\lambda \\to 1)ï¼šæ¥è¿‘ attention-only (\\lambda \\to 0)ï¼šæ¥è¿‘ redundancy-only è®ºæ–‡å®éªŒè¡¨æ˜ï¼š æœ€ä½³èŒƒå›´ ( 0.01 \\le \\lambda \\le 0.1 )ï¼Œé»˜è®¤å– ( \\lambda=0.1 )ã€‚\n5.5 Aggregation and Selection å¯¹æ‰€æœ‰ head çš„ç»“æœå–å¹³å‡ï¼š $$ Z_i = \\frac{1}{H}\\sum_h Z^{(h)}_i $$\nç„¶åé€‰å‡º top-(k = B_{\\text{budget}} - \\alpha) ä¸ª tokenï¼š $$ \\text{Idx}_{\\text{sel}} = \\operatorname{TopK}(Z, k) $$\næœ€ç»ˆ cacheï¼š $$ K_{\\text{new}} = [K_{\\text{cand}}[\\text{Idx}{\\text{sel}}]; K{\\text{obs}}] $$\n6. Evaluation 6.1 Setup Models:\nDeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Qwen-14B Datasets:\nMATH-500 AIME-24 / AIME-25 Hyperparams:\n( B_{\\text{buffer}} = 128 ), ( \\alpha = 8 ), ( \\lambda = 0.1 ) Sampling: temperature = 0.6, top-p = 0.95\nMetric: pass@1\n6.2 Baselines FullKV â€” æ— å‹ç¼©åŸºçº¿ï¼› SnapKV â€” attention-only å‹ç¼©æ–¹æ³•ï¼› R-KV â€” æœ¬æ–‡æ–¹æ³•ã€‚ å‹ç¼©å‘¨æœŸï¼šæ¯ç”Ÿæˆ 128 token åæ‰§è¡Œä¸€æ¬¡ã€‚\n6.3 Main Results (1) DeepSeek-R1-Distill-Llama-8B åœ¨ MATH-500 ä¸Šï¼š\n34% KV cache å³å¯ä¿æŒä¸ FullKV å‡ ä¹ä¸€è‡´æ€§èƒ½ã€‚ åœ¨ AIME-24 ä¸Šï¼š\n10% KV cache å³èƒ½æ— æŸå‹ç¼©ï¼› 16% KV cache æ—¶æ€§èƒ½ç”šè‡³ è¶…è¶Š FullKV (105%)ã€‚ (2) DeepSeek-R1-Distill-Qwen-14B åœ¨ MATH-500 ä¸Šï¼šlossless ratio ä¸º 54%ï¼› åœ¨ AIME-24 ä¸Šï¼šlossless ratio ä¸º 25%ï¼› åœ¨ 33% cache æƒ…å†µä¸‹è¾¾åˆ° 105% accuracyã€‚ (3) å¯¹æ¯” SnapKV SnapKV å¾€å¾€é”™è¯¯ä¿ç•™å¤§é‡é‡å¤åæ€æ®µè½ï¼› R-KV é€šè¿‡å†—ä½™å¾—åˆ†æ£€æµ‹ï¼Œèƒ½æœ‰æ•ˆè¿‡æ»¤æ­¤ç±»å†…å®¹ï¼Œåœ¨æ‰€æœ‰é¢„ç®—æ¡ä»¶ä¸‹å‡ä¼˜äº SnapKVã€‚\n6.4 Efficiency \u0026 Throughput åœ¨ Llama3-8B ä¸Šï¼š\nå‹ç¼©åˆ° 10% KV cache æ—¶ï¼Œå†…å­˜å‡å°‘çº¦ 90%ï¼› ååæå‡å¯è¾¾ 4.5â€“9Ã—ï¼› æœ€å¤§ batch size æå‡ 7â€“13Ã—ã€‚ è®¡ç®—å¼€é”€ï¼š è™½ç„¶ redundancy ä¼°è®¡éœ€è¦é¢å¤–è®¡ç®—ï¼Œä½†ç”±äº attention è®¡ç®—è§„æ¨¡é™ä½ï¼Œæ•´ä½“æ¨ç†é€Ÿåº¦ä»æå‡ã€‚\n6.5 Quantitative Summary Model Dataset KV Retained Accuracy (vs Full) Speedup Memory â†“ R1-Llama-8B MATH-500 34% â‰ˆ100% 4.3Ã— 72% R1-Llama-8B AIME-24 16% 105% 6.5Ã— 84% R1-Qwen-14B AIME-24 33% 105% 7.2Ã— 80% 7. Discussion 7.1 How to Choose Î» Î» æ§åˆ¶é‡è¦æ€§ä¸å†—ä½™çš„æƒè¡¡ï¼š\nå½“ Î» â†’ 0ï¼Œä»…ä¾èµ–å†—ä½™ï¼Œå¯èƒ½è¯¯åˆ å…³é”®ä¿¡æ¯ï¼›\nå½“ Î» â†’ 1ï¼Œä»…ä¾èµ–æ³¨æ„åŠ›ï¼Œå†—ä½™å‹ç¼©å¤±æ•ˆï¼›\næœ€ä½³åŒºé—´ï¼š $$ 0.01 \\le Î» \\le 0.1 $$\nç»éªŒè®¾ç½®ï¼š\nreasoning å†—ä½™è¾ƒé‡æ—¶å–å° Î»ï¼› context ç¨³å®šæ€§è¦æ±‚é«˜æ—¶å–ç¨å¤§ Î»ã€‚ 7.2 Failure of Attention-Based Methods SnapKV ç­‰æ–¹æ³•ä»…ä¾æ® attentionï¼Œå¯¼è‡´é‡å¤å¥å­è¢«åå¤ä¿ç•™ã€‚ R-KV é€šè¿‡å†—ä½™çŸ©é˜µæœ‰æ•ˆè¯†åˆ«è¿™äº›é‡å¤é¡¹å¹¶åˆ é™¤ã€‚\nå…³é”®åŒºåˆ«ï¼š Attention â‰  Information Novelty æ³¨æ„åŠ›è¡¨ç¤ºâ€œè¢«å…³æ³¨â€ï¼Œä¸ä»£è¡¨â€œä¿¡æ¯å¢é‡â€ã€‚\n7.3 Efficiency Analysis R-KV å‹ç¼©å attention çŸ©é˜µè§„æ¨¡ä¸‹é™ï¼Œè®¡ç®—é‡ä» (O(n^2 d)) é™åˆ° (O(k n d))ã€‚\nå†—ä½™è®¡ç®—ä¸º (O(n^2 d))ï¼Œä½† n è¿œå°äºå®Œæ•´åºåˆ—é•¿åº¦ï¼Œæ•´ä½“æ”¶ç›Šæ˜¾è‘—ã€‚\nå®æµ‹è¡¨æ˜ï¼š\nbatch size â†‘ 13.4Ã— throughput â†‘ 9.19Ã— memory â†“ 90% 8. Conclusion We introduced R-KV, a decoding-time KV cache compression method for reasoning LLMs.\né€šè¿‡è”åˆå»ºæ¨¡ importance ä¸ redundancyï¼Œ R-KV åœ¨ä¿ç•™ä»… 10â€“34% çš„ KV cache ä¸‹ï¼Œæ€§èƒ½ä¸ FullKV æŒå¹³æˆ–è¶…è¶Šã€‚\nå…¶è®­ç»ƒæ— å…³ã€æ¨¡å‹æ— å…³ã€å®ç°ç®€å•ï¼Œå¯ç›´æ¥é›†æˆè‡³ RL rollout æˆ–æ¨ç†æœåŠ¡ï¼Œ åœ¨é•¿åºåˆ—ç”Ÿæˆä¸­å¸¦æ¥é«˜è¾¾ 9Ã— ååæå‡ã€13Ã— batch æ‰©å±•èƒ½åŠ›ã€‚\nR-KV = ä¿æ€§èƒ½ + å‡å†…å­˜ + æåå æ˜¯ reasoning å‹ LLM æ¨ç†ä¼˜åŒ–çš„é‡è¦ä¸€æ­¥ã€‚\n",
  "wordCount" : "807",
  "inLanguage": "zh-cn",
  "datePublished": "2025-10-25T00:00:00Z",
  "dateModified": "2025-10-25T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jjl357.github.io/blog/posts/r-kv---redundancy-aware-kv-cache-compression-for-reasoning-models---neurips25/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JJ's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jjl357.github.io/blog/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
  <nav class="nav">
    <div class="logo">
      
      <a href="https://jjl357.github.io/" accesskey="h" title="ğŸ¥› â˜• ğŸµ (Alt + H)">
        <span>ğŸ¥› â˜• ğŸµ</span>
        
      </a>

      <div class="logo-switches">
        <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
          <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      </div>
    </div>
    <ul id="menu">
    </ul>
  </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      R-KV: Redundancy-aware KV Cache Compression for Reasoning Models
    </h1>
    <div class="post-meta"><span title='2025-10-25 00:00:00 +0000 UTC'>October 25, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="https://jjl357.github.io/blog/image/RKV/title.png"></p>
<p><strong>Conference:</strong> NeurIPS'25</p>
<p><strong>Github:</strong> <a href="https://github.com/Zefan-Cai/R-KV">https://github.com/Zefan-Cai/R-KV</a></p>
<hr>
<h2 id="my-thoughts">My Thoughts<a hidden class="anchor" aria-hidden="true" href="#my-thoughts">#</a></h2>
<p>R-KV é’ˆå¯¹previous worksåœ¨efficient reasoningä¸­ç”±äºåªä¾é attention scoresè€Œé€ æˆè¿‡å¤šåœ°ä¿ç•™äº†redundant tokens(é‡å¤è€Œä¸”å¯¹æ¨ç†æ²¡æœ‰ä¿¡æ¯å¢ç›Šçš„token),R-KVçš„é‡ç‚¹æˆ‘è§‰å¾—æ˜¯å¢åŠ äº†Redundancy Estimation via Semantic Similarityæ¥è§£å†³è¿™ä¸ªé—®é¢˜,å°±Evaluationçš„ç»“æœæ¥è¯´ï¼ŒR-KVçš„æ•ˆæœæ˜¯éå¸¸å¥½çš„ï¼Œå³æå‡äº†Throughputè¿˜maintainäº†performanceï¼Œç”šè‡³åœ¨budgetå……è¶³çš„æƒ…å†µä¸‹è¿˜å¯ä»¥åšåˆ°æç‚¹ã€‚</p>
<h2 id="1-motivation">1. Motivation<a hidden class="anchor" aria-hidden="true" href="#1-motivation">#</a></h2>
<p>Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning.
However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference.</p>
<p>For instance, a <strong>DeepSeek-R1-Distill-Llama-8B</strong> model may generate <strong>32K tokens</strong> to solve a complex math problem, consuming <strong>15.5GB</strong> of memory to load model weights and <strong>4.1GB</strong> to store the KV cache.
This long CoT (chain-of-thought) generation necessitates the development of <strong>KV cache compression</strong>.</p>
<p>è¿™ç§æ¨¡å‹åœ¨æ¨ç†æ—¶ç»å¸¸ä¼šäº§ç”Ÿ<strong>æé•¿çš„ç”Ÿæˆåºåˆ—</strong>ï¼Œå…¶ä¸­åŒ…å«å¤§é‡<strong>å†—ä½™å†…å®¹</strong>ï¼Œå¦‚ï¼š</p>
<ul>
<li>é‡å¤çš„è‡ªæˆ‘åæ€ï¼ˆself-reflectionï¼‰</li>
<li>å¤šæ¬¡é‡å¤çš„æ¨ç†å°è¯•</li>
<li>å†—é•¿çš„è‡ªå¯¹è¯å¼è¾“å‡º</li>
</ul>
<p>è¿™äº›éƒ¨åˆ†è™½ç„¶åœ¨è¯­ä¹‰ä¸Šè´¡çŒ®æœ‰é™ï¼Œå´æ˜¾è‘—å¢åŠ äº† KV ç¼“å­˜é•¿åº¦å’Œå†…å­˜è´Ÿæ‹…ã€‚</p>
<hr>
<h2 id="2-challenge">2. Challenge<a hidden class="anchor" aria-hidden="true" href="#2-challenge">#</a></h2>
<p>While chain-of-thought inference significantly improves performance on complex reasoning tasks,
existing KV cache compression methods often fail in this scenario.</p>
<p><strong>åŸå› ï¼š</strong>
ä¼ ç»Ÿå‹ç¼©æ–¹æ³•å¤šåŸºäº attention é‡è¦æ€§è¿‡æ»¤ï¼ˆå¦‚æŒ‰æ³¨æ„åŠ›æƒé‡ç­›é€‰ tokenï¼‰ï¼Œä½† reasoning æ¨¡å‹çš„è¾“å‡ºå¾€å¾€åŒ…å«é«˜åº¦é‡å¤çš„æ®µè½ã€‚
ç”±äºè¿™äº›é‡å¤æ®µè½ä¹‹é—´äº’ç›¸å¼ºåŒ–æ³¨æ„åŠ›å¾—åˆ†ï¼Œç®€å•åŸºäºæ³¨æ„åŠ›çš„è£å‰ªç­–ç•¥ä¼šï¼š</p>
<ul>
<li>é”™è¯¯åœ°ä¿ç•™å†—ä½™é‡å¤å†…å®¹ï¼›</li>
<li>é”™è¯¯åœ°åˆ é™¤ç¦»æ•£ä½†å…³é”®çš„ reasoning ç‰‡æ®µã€‚</li>
</ul>
<p>æœ€ç»ˆå¯¼è‡´æ¨ç†é“¾æ–­è£‚æˆ–ç­”æ¡ˆé”™è¯¯ã€‚</p>
<hr>
<h2 id="3-contribution">3. Contribution<a hidden class="anchor" aria-hidden="true" href="#3-contribution">#</a></h2>
<p>We propose <strong>R-KV (Redundancy-aware KV Cache Compression)</strong> â€” a novel <strong>decoding-time</strong> KV compression method for reasoning LLMs.</p>
<p><strong>æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼š</strong></p>
<ol>
<li><strong>Attention-based importance scoring</strong>ï¼šæ ¹æ®æ³¨æ„åŠ›åˆ†æ•°è¡¡é‡æ¯ä¸ª token çš„ä¸Šä¸‹æ–‡é‡è¦æ€§ã€‚</li>
<li><strong>Dynamic redundancy scoring</strong>ï¼šé€šè¿‡ Key å‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦åŠ¨æ€ä¼°è®¡ token çš„å†—ä½™åº¦ã€‚</li>
<li><strong>Joint eviction mechanism</strong>ï¼šç»“åˆé‡è¦æ€§ä¸å†—ä½™åº¦çš„è”åˆæ‰“åˆ†æœºåˆ¶ï¼Œä¼˜åŒ–ç¼“å­˜åˆ©ç”¨ç‡ã€‚</li>
</ol>
<p><strong>å®éªŒç»“æœï¼š</strong></p>
<ul>
<li>åœ¨ä¿ç•™ä»… <strong>10â€“34%</strong> çš„åŸå§‹ KV cache æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¸æœªå‹ç¼©æ¨¡å‹å‡ ä¹ä¸€è‡´ï¼›</li>
<li>åœ¨ AIME-24 æ•°æ®é›†ä¸Šï¼Œä»…ç”¨ <strong>16% KV cache</strong> å³å®ç° <strong>105%</strong> of FullKV accuracyï¼›</li>
<li>æ–¹æ³•<strong>è®­ç»ƒæ— å…³</strong>ã€<strong>æ¨¡å‹æ— å…³</strong>ï¼Œå¯ç›´æ¥éƒ¨ç½²äº RL rollout æˆ–æ¨ç†æœåŠ¡ã€‚</li>
</ul>
<hr>
<h2 id="4-observation">4. Observation<a hidden class="anchor" aria-hidden="true" href="#4-observation">#</a></h2>
<h3 id="41-redundancy-in-reasoning-models">4.1 Redundancy in Reasoning Models<a hidden class="anchor" aria-hidden="true" href="#41-redundancy-in-reasoning-models">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/RKV/figure2.png"></p>
<p><strong>è§‚å¯Ÿï¼š</strong></p>
<ul>
<li>å¤šç§ reasoning æ¨¡å‹ï¼ˆDeepSeek-R1-Distill-Llama-8Bã€DeepSeek-R1-Distill-Qwen-7Bã€Qwen-14Bï¼‰ç”Ÿæˆçš„è¾“å‡ºé•¿åº¦æ™®éæ¯” ground truth <strong>é•¿ 8 å€ä»¥ä¸Š</strong>ï¼›</li>
<li>è¾“å‡ºä¸­ 1-gram ä¸ 2-gram çš„å¹³å‡é‡å¤é¢‘ç‡æ˜¾è‘—é«˜äºå‚è€ƒç­”æ¡ˆï¼Œè¡¨æ˜ç”Ÿæˆæ–‡æœ¬å­˜åœ¨æ˜¾è‘—çš„å†—ä½™ç»“æ„ã€‚</li>
</ul>
<hr>
<h3 id="42-failure-of-existing-kv-compression-methods">4.2 Failure of Existing KV Compression Methods<a hidden class="anchor" aria-hidden="true" href="#42-failure-of-existing-kv-compression-methods">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/RKV/figure3.png"></p>
<p>ç°æœ‰å¦‚ <strong>SnapKV</strong> ç­‰ attention-based æ–¹æ³•å€¾å‘äºä¿ç•™é‡å¤çš„ reasoning ç‰‡æ®µï¼Œä¾‹å¦‚é‡å¤çš„ â€œreflectionâ€ æˆ– â€œfinal answerâ€ æ®µè½ï¼Œé€ æˆæœ‰æ•ˆä¸Šä¸‹æ–‡è¢«å†—ä½™ä¿¡æ¯æŒ¤å ã€‚</p>
<p>R-KV çš„ç›®æ ‡æ­£æ˜¯é’ˆå¯¹è¿™ç§å†—ä½™å¤±æ•ˆæ¨¡å¼è¿›è¡Œä¿®æ­£ã€‚</p>
<hr>
<h2 id="5-method">5. Method<a hidden class="anchor" aria-hidden="true" href="#5-method">#</a></h2>
<p>R-KV çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨ <strong>decoding é˜¶æ®µ</strong>ï¼ˆå³ç”Ÿæˆæ—¶ï¼‰å®æ—¶å‹ç¼© KV cacheï¼Œ
åŠ¨æ€é€‰æ‹©æœ€æœ‰ç”¨çš„å†å² tokenï¼ŒåŒæ—¶è¿‡æ»¤æ‰å†—ä½™å†…å®¹ã€‚</p>
<p>æ•´ä¸ªæ–¹æ³•åŒ…æ‹¬ä»¥ä¸‹æ¨¡å—ï¼š</p>
<h3 id="51-decoding-time-compression">5.1 Decoding-time Compression<a hidden class="anchor" aria-hidden="true" href="#51-decoding-time-compression">#</a></h3>
<p>ä¸å¤§å¤šæ•°ä»…ä½œç”¨äº <strong>prefilling é˜¶æ®µ</strong> çš„æ–¹æ³•ä¸åŒï¼ŒR-KV é’ˆå¯¹ <strong>decoding é˜¶æ®µ</strong>è®¾è®¡ã€‚</p>
<ul>
<li>
<p>è®¾å®šä¸¤ä¸ªç¼“å­˜åŒºï¼š</p>
<ul>
<li><strong>Cache åŒº</strong>ï¼šå­˜æ”¾ä¿ç•™çš„ KV å¯¹ï¼Œå®¹é‡ä¸º ( B_{\text{budget}} )</li>
<li><strong>Buffer åŒº</strong>ï¼šå­˜æ”¾æœ€æ–°ç”Ÿæˆçš„ tokenï¼Œå®¹é‡ä¸º ( B_{\text{buffer}} )</li>
</ul>
</li>
<li>
<p>æ€»å†…å­˜éœ€æ±‚ï¼š
$$
B_{\text{total}} = B_{\text{budget}} + B_{\text{buffer}}
$$</p>
</li>
</ul>
<p><strong>å‹ç¼©æ­¥éª¤ï¼ˆå¾ªç¯æ‰§è¡Œï¼‰</strong>ï¼š</p>
<ol>
<li>æ¨¡å‹ç”Ÿæˆä¸€æ®µé•¿åº¦ä¸º ( B_{\text{buffer}} ) çš„æ–‡æœ¬ï¼›</li>
<li>Buffer çš„æœ€å (\alpha) ä¸ª tokenï¼ˆç§°ä½œ <em>observation tokens</em>ï¼‰å§‹ç»ˆä¿ç•™ï¼›</li>
<li>æ‹¼æ¥ï¼š
$$
n = B_{\text{budget}} + B_{\text{buffer}} - \alpha
$$
å½¢æˆå€™é€‰é›†åˆï¼›</li>
<li>å¯¹æ‰€æœ‰å€™é€‰ token è®¡ç®—ç»¼åˆå¾—åˆ†ï¼›</li>
<li>é€‰å‡ºå‰ (k = B_{\text{budget}} - \alpha) ä¸ª token + (\alpha) ä¸ª observation token ç»„æˆæ–°çš„ cacheã€‚</li>
</ol>
<hr>
<h3 id="52-importance-scoring-via-attention-weights">5.2 Importance Scoring via Attention Weights<a hidden class="anchor" aria-hidden="true" href="#52-importance-scoring-via-attention-weights">#</a></h3>
<p>å¯¹æ¯ä¸ª attention head (h)ï¼š</p>
<p>$$
A^{(h)} = \operatorname{softmax}!\left(\frac{Q^{(h)} (K^{(h)})^\top}{\sqrt{d}}\right)
$$</p>
<p>å…¶ä¸­ï¼š</p>
<ul>
<li>( Q^{(h)} \in \mathbb{R}^{\alpha \times d} )ï¼šæœ€è¿‘ (\alpha) ä¸ª queryï¼›</li>
<li>( K^{(h)} \in \mathbb{R}^{n \times d} )ï¼šå€™é€‰ keyã€‚</li>
</ul>
<p>ä¸ºäº†é˜²æ­¢æç«¯æ³¨æ„åŠ›å€¼å¯¼è‡´ä¸ç¨³å®šï¼Œ
åœ¨ token ç»´åº¦ä¸Šè¿›è¡Œæ»‘çª—æœ€å¤§æ± åŒ–ï¼ˆwindow size = (2W)ï¼‰ï¼š</p>
<p>$$
\tilde A^{(h)}<em>{j,i} = \max(A^{(h)}</em>{j,i-W},\ldots,A^{(h)}_{j,i+W-1})
$$</p>
<p>æœ€åå¯¹ query å¹³å‡åŒ–å¾—åˆ° importance scoreï¼š</p>
<p>$$
I^{(h)}<em>i = \frac{1}{\alpha}\sum</em>{j=0}^{\alpha-1}\tilde A^{(h)}_{j,i}
$$</p>
<hr>
<h3 id="53-redundancy-estimation-via-semantic-similarity">5.3 Redundancy Estimation via Semantic Similarity<a hidden class="anchor" aria-hidden="true" href="#53-redundancy-estimation-via-semantic-similarity">#</a></h3>
<p>ç›®æ ‡ï¼šæ‰¾å‡ºè¯­ä¹‰é‡å¤çš„ tokenã€‚</p>
<ol>
<li>
<p>å½’ä¸€åŒ–æ¯ä¸ª key å‘é‡ï¼š
$$
\hat K^{(h)}_i = \frac{K^{(h)}_i}{|K^{(h)}_i|_2 + \varepsilon}
$$</p>
</li>
<li>
<p>è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µï¼š
$$
S^{(h)} = \hat K^{(h)} (\hat K^{(h)})^\top,\quad S^{(h)}_{i,i}=0
$$</p>
</li>
<li>
<p>ä¸ºé¿å…è¯¯åˆ æœ€è¿‘ç›¸å…³çš„é‡å¤ tokenï¼Œå¯¹æ¯ä¸ª token (i)ï¼š</p>
<ul>
<li>å–ä¸å…¶ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼ (T) çš„é›†åˆ
(\mathcal{I}^{(h)}<em>i = {j | S^{(h)}</em>{j,i} &gt; T})</li>
<li>ä»…ä¿ç•™æœ€è¿‘ (\beta) ä¸ªï¼ˆå³ç”Ÿæˆæ—¶é—´ä¸Šé è¿‘çš„ï¼‰ï¼Œå…¶ä»–ç›¸ä¼¼ token è®¾ä¸º 0ï¼š
$$
S^{(h)}<em>{j,i} = 0,\quad \forall j \in \mathcal{I}^{(h)}</em>{i,\beta}
$$</li>
</ul>
</li>
<li>
<p>è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦å¹¶å½’ä¸€åŒ–ï¼š
$$
\bar S^{(h)}<em>i = \frac{1}{n}\sum</em>{j=0}^{n-1} S^{(h)}_{j,i}, \quad
R^{(h)}_i = \operatorname{softmax}(\bar S^{(h)})
$$</p>
</li>
</ol>
<p>é«˜ç›¸ä¼¼åº¦æ„å‘³ç€å†—ä½™åº¦é«˜ï¼Œåº”è¢«æ·˜æ±°ã€‚</p>
<hr>
<h3 id="54-joint-selection-strategy">5.4 Joint Selection Strategy<a hidden class="anchor" aria-hidden="true" href="#54-joint-selection-strategy">#</a></h3>
<p>æœ€ç»ˆç»¼åˆå¾—åˆ†ä¸ºï¼š</p>
<p>$$
Z^{(h)}_i = \lambda I^{(h)}_i - (1 - \lambda) R^{(h)}_i
$$</p>
<ul>
<li>
<p>(\lambda)ï¼šæ§åˆ¶é‡è¦æ€§ä¸å†—ä½™çš„å¹³è¡¡</p>
<ul>
<li>(\lambda \to 1)ï¼šæ¥è¿‘ attention-only</li>
<li>(\lambda \to 0)ï¼šæ¥è¿‘ redundancy-only</li>
</ul>
</li>
</ul>
<p>è®ºæ–‡å®éªŒè¡¨æ˜ï¼š
æœ€ä½³èŒƒå›´ ( 0.01 \le \lambda \le 0.1 )ï¼Œé»˜è®¤å– ( \lambda=0.1 )ã€‚</p>
<hr>
<h3 id="55-aggregation-and-selection">5.5 Aggregation and Selection<a hidden class="anchor" aria-hidden="true" href="#55-aggregation-and-selection">#</a></h3>
<p>å¯¹æ‰€æœ‰ head çš„ç»“æœå–å¹³å‡ï¼š
$$
Z_i = \frac{1}{H}\sum_h Z^{(h)}_i
$$</p>
<p>ç„¶åé€‰å‡º top-(k = B_{\text{budget}} - \alpha) ä¸ª tokenï¼š
$$
\text{Idx}_{\text{sel}} = \operatorname{TopK}(Z, k)
$$</p>
<p>æœ€ç»ˆ cacheï¼š
$$
K_{\text{new}} = [K_{\text{cand}}[\text{Idx}<em>{\text{sel}}]; K</em>{\text{obs}}]
$$</p>
<hr>
<h2 id="6-evaluation">6. Evaluation<a hidden class="anchor" aria-hidden="true" href="#6-evaluation">#</a></h2>
<h3 id="61-setup">6.1 Setup<a hidden class="anchor" aria-hidden="true" href="#61-setup">#</a></h3>
<ul>
<li>
<p><strong>Models:</strong></p>
<ul>
<li>DeepSeek-R1-Distill-Llama-8B</li>
<li>DeepSeek-R1-Distill-Qwen-14B</li>
</ul>
</li>
<li>
<p><strong>Datasets:</strong></p>
<ul>
<li>MATH-500</li>
<li>AIME-24 / AIME-25</li>
</ul>
</li>
<li>
<p><strong>Hyperparams:</strong></p>
<ul>
<li>( B_{\text{buffer}} = 128 ), ( \alpha = 8 ), ( \lambda = 0.1 )</li>
</ul>
</li>
<li>
<p><strong>Sampling:</strong> temperature = 0.6, top-p = 0.95</p>
</li>
<li>
<p><strong>Metric:</strong> pass@1</p>
</li>
</ul>
<hr>
<h3 id="62-baselines">6.2 Baselines<a hidden class="anchor" aria-hidden="true" href="#62-baselines">#</a></h3>
<ul>
<li><strong>FullKV</strong> â€” æ— å‹ç¼©åŸºçº¿ï¼›</li>
<li><strong>SnapKV</strong> â€” attention-only å‹ç¼©æ–¹æ³•ï¼›</li>
<li><strong>R-KV</strong> â€” æœ¬æ–‡æ–¹æ³•ã€‚</li>
</ul>
<p>å‹ç¼©å‘¨æœŸï¼šæ¯ç”Ÿæˆ 128 token åæ‰§è¡Œä¸€æ¬¡ã€‚</p>
<hr>
<h3 id="63-main-results">6.3 Main Results<a hidden class="anchor" aria-hidden="true" href="#63-main-results">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/RKV/figure4.png"></p>
<h4 id="1-deepseek-r1-distill-llama-8b">(1) DeepSeek-R1-Distill-Llama-8B<a hidden class="anchor" aria-hidden="true" href="#1-deepseek-r1-distill-llama-8b">#</a></h4>
<ul>
<li>
<p>åœ¨ MATH-500 ä¸Šï¼š</p>
<ul>
<li><strong>34% KV cache</strong> å³å¯ä¿æŒä¸ FullKV å‡ ä¹ä¸€è‡´æ€§èƒ½ã€‚</li>
</ul>
</li>
<li>
<p>åœ¨ AIME-24 ä¸Šï¼š</p>
<ul>
<li><strong>10% KV cache</strong> å³èƒ½æ— æŸå‹ç¼©ï¼›</li>
<li><strong>16% KV cache</strong> æ—¶æ€§èƒ½ç”šè‡³ <strong>è¶…è¶Š FullKV (105%)</strong>ã€‚</li>
</ul>
</li>
</ul>
<h4 id="2-deepseek-r1-distill-qwen-14b">(2) DeepSeek-R1-Distill-Qwen-14B<a hidden class="anchor" aria-hidden="true" href="#2-deepseek-r1-distill-qwen-14b">#</a></h4>
<ul>
<li>åœ¨ MATH-500 ä¸Šï¼šlossless ratio ä¸º <strong>54%</strong>ï¼›</li>
<li>åœ¨ AIME-24 ä¸Šï¼šlossless ratio ä¸º <strong>25%</strong>ï¼›</li>
<li>åœ¨ <strong>33% cache</strong> æƒ…å†µä¸‹è¾¾åˆ° <strong>105% accuracy</strong>ã€‚</li>
</ul>
<h4 id="3-å¯¹æ¯”-snapkv">(3) å¯¹æ¯” SnapKV<a hidden class="anchor" aria-hidden="true" href="#3-å¯¹æ¯”-snapkv">#</a></h4>
<p>SnapKV å¾€å¾€é”™è¯¯ä¿ç•™å¤§é‡é‡å¤åæ€æ®µè½ï¼›
R-KV é€šè¿‡å†—ä½™å¾—åˆ†æ£€æµ‹ï¼Œèƒ½æœ‰æ•ˆè¿‡æ»¤æ­¤ç±»å†…å®¹ï¼Œåœ¨æ‰€æœ‰é¢„ç®—æ¡ä»¶ä¸‹å‡ä¼˜äº SnapKVã€‚</p>
<hr>
<h3 id="64-efficiency--throughput">6.4 Efficiency &amp; Throughput<a hidden class="anchor" aria-hidden="true" href="#64-efficiency--throughput">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/RKV/table1.png"></p>
<ul>
<li>
<p>åœ¨ Llama3-8B ä¸Šï¼š</p>
<ul>
<li>å‹ç¼©åˆ° 10% KV cache æ—¶ï¼Œå†…å­˜å‡å°‘çº¦ <strong>90%</strong>ï¼›</li>
<li>ååæå‡å¯è¾¾ <strong>4.5â€“9Ã—</strong>ï¼›</li>
<li>æœ€å¤§ batch size æå‡ <strong>7â€“13Ã—</strong>ã€‚</li>
</ul>
</li>
<li>
<p>è®¡ç®—å¼€é”€ï¼š
è™½ç„¶ redundancy ä¼°è®¡éœ€è¦é¢å¤–è®¡ç®—ï¼Œä½†ç”±äº attention è®¡ç®—è§„æ¨¡é™ä½ï¼Œæ•´ä½“æ¨ç†é€Ÿåº¦ä»æå‡ã€‚</p>
</li>
</ul>
<hr>
<h3 id="65-quantitative-summary">6.5 Quantitative Summary<a hidden class="anchor" aria-hidden="true" href="#65-quantitative-summary">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>Dataset</th>
          <th>KV Retained</th>
          <th>Accuracy (vs Full)</th>
          <th>Speedup</th>
          <th>Memory â†“</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>R1-Llama-8B</td>
          <td>MATH-500</td>
          <td>34%</td>
          <td>â‰ˆ100%</td>
          <td>4.3Ã—</td>
          <td>72%</td>
      </tr>
      <tr>
          <td>R1-Llama-8B</td>
          <td>AIME-24</td>
          <td>16%</td>
          <td><strong>105%</strong></td>
          <td>6.5Ã—</td>
          <td>84%</td>
      </tr>
      <tr>
          <td>R1-Qwen-14B</td>
          <td>AIME-24</td>
          <td>33%</td>
          <td>105%</td>
          <td>7.2Ã—</td>
          <td>80%</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="7-discussion">7. Discussion<a hidden class="anchor" aria-hidden="true" href="#7-discussion">#</a></h2>
<h3 id="71-how-to-choose-Î»">7.1 How to Choose Î»<a hidden class="anchor" aria-hidden="true" href="#71-how-to-choose-Î»">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/RKV/figure56.png"></p>
<p><strong>Î» æ§åˆ¶é‡è¦æ€§ä¸å†—ä½™çš„æƒè¡¡ï¼š</strong></p>
<ul>
<li>
<p>å½“ Î» â†’ 0ï¼Œä»…ä¾èµ–å†—ä½™ï¼Œå¯èƒ½è¯¯åˆ å…³é”®ä¿¡æ¯ï¼›</p>
</li>
<li>
<p>å½“ Î» â†’ 1ï¼Œä»…ä¾èµ–æ³¨æ„åŠ›ï¼Œå†—ä½™å‹ç¼©å¤±æ•ˆï¼›</p>
</li>
<li>
<p>æœ€ä½³åŒºé—´ï¼š
$$
0.01 \le Î» \le 0.1
$$</p>
</li>
<li>
<p>ç»éªŒè®¾ç½®ï¼š</p>
<ul>
<li>reasoning å†—ä½™è¾ƒé‡æ—¶å–å° Î»ï¼›</li>
<li>context ç¨³å®šæ€§è¦æ±‚é«˜æ—¶å–ç¨å¤§ Î»ã€‚</li>
</ul>
</li>
</ul>
<hr>
<h3 id="72-failure-of-attention-based-methods">7.2 Failure of Attention-Based Methods<a hidden class="anchor" aria-hidden="true" href="#72-failure-of-attention-based-methods">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/RKV/figure7.png"></p>
<p>SnapKV ç­‰æ–¹æ³•ä»…ä¾æ® attentionï¼Œå¯¼è‡´é‡å¤å¥å­è¢«åå¤ä¿ç•™ã€‚
R-KV é€šè¿‡å†—ä½™çŸ©é˜µæœ‰æ•ˆè¯†åˆ«è¿™äº›é‡å¤é¡¹å¹¶åˆ é™¤ã€‚</p>
<p>å…³é”®åŒºåˆ«ï¼š
<strong>Attention â‰  Information Novelty</strong>
æ³¨æ„åŠ›è¡¨ç¤ºâ€œè¢«å…³æ³¨â€ï¼Œä¸ä»£è¡¨â€œä¿¡æ¯å¢é‡â€ã€‚</p>
<hr>
<h3 id="73-efficiency-analysis">7.3 Efficiency Analysis<a hidden class="anchor" aria-hidden="true" href="#73-efficiency-analysis">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/RKV/table1.png"></p>
<ul>
<li>
<p>R-KV å‹ç¼©å attention çŸ©é˜µè§„æ¨¡ä¸‹é™ï¼Œè®¡ç®—é‡ä» (O(n^2 d)) é™åˆ° (O(k n d))ã€‚</p>
</li>
<li>
<p>å†—ä½™è®¡ç®—ä¸º (O(n^2 d))ï¼Œä½† n è¿œå°äºå®Œæ•´åºåˆ—é•¿åº¦ï¼Œæ•´ä½“æ”¶ç›Šæ˜¾è‘—ã€‚</p>
</li>
<li>
<p>å®æµ‹è¡¨æ˜ï¼š</p>
<ul>
<li>batch size â†‘ 13.4Ã—</li>
<li>throughput â†‘ 9.19Ã—</li>
<li>memory â†“ 90%</li>
</ul>
</li>
</ul>
<hr>
<h2 id="8-conclusion">8. Conclusion<a hidden class="anchor" aria-hidden="true" href="#8-conclusion">#</a></h2>
<p>We introduced <strong>R-KV</strong>, a <strong>decoding-time KV cache compression method</strong> for reasoning LLMs.</p>
<p>é€šè¿‡è”åˆå»ºæ¨¡ <strong>importance</strong> ä¸ <strong>redundancy</strong>ï¼Œ
R-KV åœ¨ä¿ç•™ä»… <strong>10â€“34%</strong> çš„ KV cache ä¸‹ï¼Œæ€§èƒ½ä¸ FullKV æŒå¹³æˆ–è¶…è¶Šã€‚</p>
<p>å…¶è®­ç»ƒæ— å…³ã€æ¨¡å‹æ— å…³ã€å®ç°ç®€å•ï¼Œå¯ç›´æ¥é›†æˆè‡³ RL rollout æˆ–æ¨ç†æœåŠ¡ï¼Œ
åœ¨é•¿åºåˆ—ç”Ÿæˆä¸­å¸¦æ¥é«˜è¾¾ <strong>9Ã— ååæå‡ã€13Ã— batch æ‰©å±•èƒ½åŠ›</strong>ã€‚</p>
<p><strong>R-KV = ä¿æ€§èƒ½ + å‡å†…å­˜ + æåå</strong>
æ˜¯ reasoning å‹ LLM æ¨ç†ä¼˜åŒ–çš„é‡è¦ä¸€æ­¥ã€‚</p>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jjl357.github.io/blog/tags/kv-cache/">KV Cache</a></li>
      <li><a href="https://jjl357.github.io/blog/tags/paper-note/">Paper Note</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://jjl357.github.io/blog/">JJ&#39;s Blog</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
