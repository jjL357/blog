<!DOCTYPE html>
<html lang="zh-cn" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs | JJ&#39;s Blog</title>
<meta name="keywords" content="KV Cache, MLLM, Paper Note">
<meta name="description" content="
Conference: NeurIPS&#39;25
github: https://github.com/Theia-4869/CDPruner

My Thoughts
相较于 Attention-based methods 中的 attention shift 问题和 Similarity-based methods 中忽略了 query 的问题，这篇论文从 增加 visual tokens 全局多样性（同时保持对 query 的关注） 的角度出发，提出了 CDPruner，通过将 token 多样性建模为 DPP（Determinantal Point Process）求解问题，实现了 SOTA 的 pruning 效果。

Motivations
在多模态大语言模型（MLLMs）中，视觉 token 的输入长度往往远大于文本 token，从而带来高昂的推理开销。例如：

LLaVA-1.5 将一张 336×336 图像转换为 576 tokens；
LLaVA-NeXT 的高分辨率版本在输入加倍的情况下生成 2,880 tokens；
LongVA 处理 2,000 帧视频时生成超过 200K visual tokens；
LongVILA 能处理 6,000 帧并产生 超过 1M visual tokens，导致巨大的计算成本。


Challenges

现有的视觉 token 剪枝方法主要分为两类：">
<meta name="author" content="">
<link rel="canonical" href="https://jjl357.github.io/blog/posts/cdpruner---beyond-attention-or-similarity---maximizing-conditional-diversity-for-token-pruning-in-mllms-neuirips25/">
<link crossorigin="anonymous" href="https://jjl357.github.io/blog/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jjl357.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jjl357.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jjl357.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jjl357.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://jjl357.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh-cn" href="https://jjl357.github.io/blog/posts/cdpruner---beyond-attention-or-similarity---maximizing-conditional-diversity-for-token-pruning-in-mllms-neuirips25/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://jjl357.github.io/blog/posts/cdpruner---beyond-attention-or-similarity---maximizing-conditional-diversity-for-token-pruning-in-mllms-neuirips25/">
  <meta property="og:site_name" content="JJ&#39;s Blog">
  <meta property="og:title" content="Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs">
  <meta property="og:description" content="
Conference: NeurIPS&#39;25 github: https://github.com/Theia-4869/CDPruner
My Thoughts 相较于 Attention-based methods 中的 attention shift 问题和 Similarity-based methods 中忽略了 query 的问题，这篇论文从 增加 visual tokens 全局多样性（同时保持对 query 的关注） 的角度出发，提出了 CDPruner，通过将 token 多样性建模为 DPP（Determinantal Point Process）求解问题，实现了 SOTA 的 pruning 效果。
Motivations 在多模态大语言模型（MLLMs）中，视觉 token 的输入长度往往远大于文本 token，从而带来高昂的推理开销。例如：
LLaVA-1.5 将一张 336×336 图像转换为 576 tokens； LLaVA-NeXT 的高分辨率版本在输入加倍的情况下生成 2,880 tokens； LongVA 处理 2,000 帧视频时生成超过 200K visual tokens； LongVILA 能处理 6,000 帧并产生 超过 1M visual tokens，导致巨大的计算成本。 Challenges 现有的视觉 token 剪枝方法主要分为两类：">
  <meta property="og:locale" content="zh-cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-24T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-24T00:00:00+00:00">
    <meta property="article:tag" content="KV Cache">
    <meta property="article:tag" content="MLLM">
    <meta property="article:tag" content="Paper Note">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs">
<meta name="twitter:description" content="
Conference: NeurIPS&#39;25
github: https://github.com/Theia-4869/CDPruner

My Thoughts
相较于 Attention-based methods 中的 attention shift 问题和 Similarity-based methods 中忽略了 query 的问题，这篇论文从 增加 visual tokens 全局多样性（同时保持对 query 的关注） 的角度出发，提出了 CDPruner，通过将 token 多样性建模为 DPP（Determinantal Point Process）求解问题，实现了 SOTA 的 pruning 效果。

Motivations
在多模态大语言模型（MLLMs）中，视觉 token 的输入长度往往远大于文本 token，从而带来高昂的推理开销。例如：

LLaVA-1.5 将一张 336×336 图像转换为 576 tokens；
LLaVA-NeXT 的高分辨率版本在输入加倍的情况下生成 2,880 tokens；
LongVA 处理 2,000 帧视频时生成超过 200K visual tokens；
LongVILA 能处理 6,000 帧并产生 超过 1M visual tokens，导致巨大的计算成本。


Challenges

现有的视觉 token 剪枝方法主要分为两类：">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jjl357.github.io/blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs",
      "item": "https://jjl357.github.io/blog/posts/cdpruner---beyond-attention-or-similarity---maximizing-conditional-diversity-for-token-pruning-in-mllms-neuirips25/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs",
  "name": "Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs",
  "description": "\nConference: NeurIPS'25 github: https://github.com/Theia-4869/CDPruner\nMy Thoughts 相较于 Attention-based methods 中的 attention shift 问题和 Similarity-based methods 中忽略了 query 的问题，这篇论文从 增加 visual tokens 全局多样性（同时保持对 query 的关注） 的角度出发，提出了 CDPruner，通过将 token 多样性建模为 DPP（Determinantal Point Process）求解问题，实现了 SOTA 的 pruning 效果。\nMotivations 在多模态大语言模型（MLLMs）中，视觉 token 的输入长度往往远大于文本 token，从而带来高昂的推理开销。例如：\nLLaVA-1.5 将一张 336×336 图像转换为 576 tokens； LLaVA-NeXT 的高分辨率版本在输入加倍的情况下生成 2,880 tokens； LongVA 处理 2,000 帧视频时生成超过 200K visual tokens； LongVILA 能处理 6,000 帧并产生 超过 1M visual tokens，导致巨大的计算成本。 Challenges 现有的视觉 token 剪枝方法主要分为两类：\n",
  "keywords": [
    "KV Cache", "MLLM", "Paper Note"
  ],
  "articleBody": "\nConference: NeurIPS'25 github: https://github.com/Theia-4869/CDPruner\nMy Thoughts 相较于 Attention-based methods 中的 attention shift 问题和 Similarity-based methods 中忽略了 query 的问题，这篇论文从 增加 visual tokens 全局多样性（同时保持对 query 的关注） 的角度出发，提出了 CDPruner，通过将 token 多样性建模为 DPP（Determinantal Point Process）求解问题，实现了 SOTA 的 pruning 效果。\nMotivations 在多模态大语言模型（MLLMs）中，视觉 token 的输入长度往往远大于文本 token，从而带来高昂的推理开销。例如：\nLLaVA-1.5 将一张 336×336 图像转换为 576 tokens； LLaVA-NeXT 的高分辨率版本在输入加倍的情况下生成 2,880 tokens； LongVA 处理 2,000 帧视频时生成超过 200K visual tokens； LongVILA 能处理 6,000 帧并产生 超过 1M visual tokens，导致巨大的计算成本。 Challenges 现有的视觉 token 剪枝方法主要分为两类：\nAttention-based pruning：利用 text-visual attention 分数衡量视觉 token 的重要性。 Similarity-based pruning：依据视觉 token 间的相似性移除冗余部分。 然而，这两种方法均存在固有缺陷。Attention-based 方法只考虑重要性，容易保留大量重复 token；Similarity-based 方法忽略了指令（instruction）的关联性，导致无法针对问题进行动态剪枝，从而性能次优。\n“However, as pointed out by Zhang et al. [2024b] and Wen et al. [2025a], such methods suffer from attention shift, which compromises pruning accuracy.”\n⚠ 什么是 attention shift？ (gpt回答 不一定正确)\nAttention shift 指在 token 剪枝或输入变化后，模型的注意力分布发生偏移，导致原本重要的 token 被低估或信息丢失，从而降低剪枝准确性。由于 Transformer 的 self-attention 是全局依赖的，删除部分 token 会改变 query-key 分布，使剩余 token 的注意力重新分布，如果不考虑这种 shift，基于注意力的剪枝方法容易出现决策失真，保留重复 token 或遗漏关键 token，从而影响推理性能。(Zhang et al., 2024b, “[CLS] Attention Is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster”; Wen et al., 2025a, “Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?”)\n⚠ attention-based 方法不兼容高效实现如 FlashAttention 此外，attention-based 方法还依赖显式 attention 权重，不兼容高效实现如 FlashAttention。\n因为attention-based 方法依赖显式的 attention 权重矩阵（attention map） 来评估每个视觉 token 的重要性，需要在推理过程中访问完整的 Softmax(QKᵀ) 结果或其中的行向量。但高效注意力实现（如 FlashAttention）的核心思想正是避免显式构建与存储整个注意力矩阵。\nFlashAttention 将注意力计算分块（block-wise）执行，通过在 GPU 的高速寄存器和片上 SRAM 中即时计算 Softmax(QKᵀ)V，并在每个块结束后立刻丢弃中间的 QKᵀ 结果与注意力权重，仅保留最终输出。这种方法极大降低了显存读写和带宽占用，使得注意力计算的复杂度从内存瓶颈（memory-bound）变为计算受限（compute-bound），从而实现高效推理。\n然而，这种实现方式带来三个关键后果，使 attention-based pruning 与 FlashAttention 不兼容：\n不可访问性：FlashAttention 不显式存储或返回完整的 attention map，因此无法直接提取每个 token 的注意力权重；而 attention-based 方法恰恰需要这些分数来判断保留与删除。 存储与性能冲突：若强行修改 FlashAttention 以输出 attention map，就必须重新显式计算并缓存 QKᵀ 和 Softmax 结果，这会破坏其内存复用机制，重新引入大规模内存访问与显存占用，性能急剧下降。 多头与层次不稳定性：不同层、不同头的注意力权重分布差异显著，且 FlashAttention 内部按块累积 Softmax，会进一步导致 attention 值在不同分块间不可直接比较，增加了基于 attention 值进行统一排序和剪枝的难度。 因此，在采用 FlashAttention 或其他高效注意力优化（如 xFormers、PagedAttention）的现代 MLLM 中，attention-based pruning 方法无法直接使用或会破坏推理加速效果。\n因此，作者提出从 “beyond attention or similarity” 的角度重新思考 token pruning。\nContributions 提出 CDPruner：一种 plug-and-play、model-agnostic 的视觉 token 剪枝方案，通过最大化条件多样性（conditional diversity）实现高效动态剪枝； 将 token pruning 问题重构为 DPP（Determinantal Point Process），联合考虑 feature similarity 与 instruction relevance； 在多种视觉语言基准上实验验证，CDPruner 在不同压缩率下均取得 SOTA。 Methods Determinantal Point Process (DPP) DPP 最初用于刻画费米子系统的“排斥效应”，在机器学习中被广泛用于建模集合选择的全局多样性。\n与 Max-Min Diversity Problem (MMDP) 仅关注极端样本不同，DPP 强调全局平衡和代表性。传统 DPP 仅考虑样本间相似度，而本论文在此基础上引入 instruction relevance，使剪枝过程同时考虑“token 相关性”与“多样性”。\nDPP with Token Similarity 核心思想： 将视觉 token 的 pairwise similarity 建模为一个核矩阵 ( L )，并通过最大化其行列式（determinant）来选择最具代表性的子集。\n定义每个视觉 token 的特征向量为 ( $H^v_i \\in \\mathbb{R}^d$ )，则相似核矩阵为：\n$$ L_{ij} = \\frac{H^v_i \\cdot H^v_j}{|H^v_i| , |H^v_j|} $$\n目标是选择一个包含 ( m ) 个 token 的子集 ( $S \\subset Z$ )，使得：\n$$ S^* = \\arg\\max_{S \\subset Z, |S|=m} \\det(L_S) $$\n这里 ( $L_S$ ) 是对应子集的子矩阵。行列式越大，代表该子集在特征空间中“覆盖的方向”越多，信息冗余越低。\n直观解释： 如果两个 token 的特征非常相似（线性相关），行列式会减小。因此 DPP 天然倾向保留“互补”信息而非重复 token，从而实现高效且全局均衡的剪枝。\nInstruction Relevance 传统 DPP 仅基于视觉特征构建核矩阵，无法体现视觉 token 与文本指令的相关性。CDPruner 通过以下步骤引入 条件相关性（conditional relevance）：\n获取视觉 token 表示 ( $H_v \\in \\mathbb{R}^{n \\times d}$ )；\n获取文本嵌入（instruction embedding） ( $\\bar{H}_q \\in \\mathbb{R}^d $)，其来源可以是：\nCLIP-like text encoder（若模型具备双编码结构）； 或通过 multimodal projector 与 LLM 的指令 token 平均表示。 计算每个视觉 token 与 instruction 的余弦相似度：\n$$ r_i = \\frac{H^v_i \\cdot \\bar{H}_q}{|H^v_i| , |\\bar{H}_q|} $$\n对相关性进行 min–max 归一化：\n$$ \\tilde{r}_i = \\frac{r_i - \\min(r)}{\\max(r) - \\min(r)} $$\n得到的向量 ( $\\tilde{r} \\in [0,1]^n$ ) 反映了各视觉 token 对当前指令的重要程度。\n直观理解：\n这使得剪枝可以根据用户问题动态调整保留区域。例如，同一张图片在不同问题下，关注区域（即高 ( $\\tilde{r}_i$ )）完全不同。论文在 Figure 3 中展示了这种可视化结果。\nCDPruner 核心机制：条件 DPP（Conditional DPP）\n为了联合考虑视觉特征多样性与指令相关性，CDPruner 构建了条件核矩阵：\n$$ \\tilde{L} = \\operatorname{diag}(\\tilde{r}) , L , \\operatorname{diag}(\\tilde{r}) $$\nDPP 的目标变为最大化该核矩阵的行列式：\n$$ S^* = \\arg\\max_{S \\subset Z, |S| = m} \\det(\\tilde{L}_S) $$\n通过对数形式可以分解为：\n$$ \\log\\det(\\tilde{L}S) = \\sum{i \\in S} \\log(\\tilde{r}_i^2) + \\log\\det(L_S) $$\n这清楚地表明，CDPruner 同时优化两项：\nrelevance term（指令相关性） diversity term（全局多样性） MAP 推断与高效近似\nDPP 的 MAP inference 是 NP-hard 的，因此论文采用了 贪心近似（Fast Greedy MAP） 算法：\n初始化空集合 ( S = \\emptyset )； 每次迭代选取能最大化当前增益（即行列式增加量）的 token； 使用 Cholesky 分解快速计算增益，从而实现高效近似。 该算法的时间复杂度为：\n$$ O(nm^2) $$\n在实践中，当保留 token 数 ( m \\ll n ) 时，额外延迟仅约 \u003c10ms/sample，可直接应用于 MLLM 推理流程中。\n可调权重（平衡项）\n论文还提出可选平衡因子 ( \\theta )，用于控制相关性与多样性之间的权重：\n$$ \\log\\det(\\tilde{L}S) = \\theta \\sum{i \\in S} \\tilde{r}_i + (1 - \\theta) \\log\\det(L_S) $$\n实验显示在不同任务上最优的 ( $\\theta$ ) 不同，为模型提供了更好的灵活性。\nImplementation \u0026 Model-agnostic 特性\n对不同架构均可用（LLaVA、Qwen2.5-VL 等）； 若模型具备 CLIP-like 双编码结构，直接使用 text encoder 的输出； 否则通过 multimodal projector + LLM 的 instruction token 平均得到文本表示； 完全 training-free，可直接嵌入推理流程作为模块。 Evaluations Main Results 在多种视觉语言基准上，CDPruner 在不同 token 削减率下均超过已有方法。\nCDPruner for High-resolution Inputs 对高分辨率输入（如 LLaVA-NeXT-7B, 2880→320 tokens），CDPruner 显著减少 FLOPs 与延迟，同时保持性能。\nCDPruner for Video Understanding \u0026 Advanced Architectures 在视频任务与先进架构（如 Qwen2.5-VL）上，CDPruner 仍能在不同压缩率下取得一致提升，验证了方法的通用性与可迁移性。\nEfficiency Analysis \u0026 Ablation Study 论文的效率实验（以 LLaVA-NeXT-7B 为例）显示：\nFLOPs 减少约 10×； Prefill latency、Decode latency 显著下降； GPU memory 与 KV cache size 大幅降低； 仅在极高剪枝率下性能略有下降。 Ablation 研究表明：\n同时考虑 conditional relevance 与 DPP 多样性带来最佳性能； 仅使用 attention 或 similarity 的变体均低于 CDPruner。 Conclusions 本文提出了一个 训练无关、模型无关 的视觉 token 剪枝方法 CDPruner，通过定义基于指令的条件相似性，并以 DPP 形式最大化选中 token 的条件多样性，实现了推理加速与性能保持的统一。\nCDPruner 在多种 MLLM 架构（如 LLaVA 系列与 Qwen2.5-VL）上实现 SOTA 性能，同时显著降低延迟与显存使用，展现出在真实应用中部署多模态大模型的潜力。\n",
  "wordCount" : "650",
  "inLanguage": "zh-cn",
  "datePublished": "2025-10-24T00:00:00Z",
  "dateModified": "2025-10-24T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jjl357.github.io/blog/posts/cdpruner---beyond-attention-or-similarity---maximizing-conditional-diversity-for-token-pruning-in-mllms-neuirips25/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JJ's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jjl357.github.io/blog/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jjl357.github.io/blog/" accesskey="h" title="JJ&#39;s Blog (Alt + H)">JJ&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs
    </h1>
    <div class="post-meta"><span title='2025-10-24 00:00:00 +0000 UTC'>October 24, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/title.png"></p>
<p><strong>Conference</strong>: <strong>NeurIPS'25</strong>
<strong>github</strong>: <a href="https://github.com/Theia-4869/CDPruner">https://github.com/Theia-4869/CDPruner</a></p>
<hr>
<h2 id="my-thoughts">My Thoughts<a hidden class="anchor" aria-hidden="true" href="#my-thoughts">#</a></h2>
<p>相较于 Attention-based methods 中的 <strong>attention shift</strong> 问题和 Similarity-based methods 中忽略了 query 的问题，这篇论文从 <strong>增加 visual tokens 全局多样性（同时保持对 query 的关注）</strong> 的角度出发，提出了 <strong>CDPruner</strong>，通过将 token 多样性建模为 DPP（Determinantal Point Process）求解问题，实现了 SOTA 的 pruning 效果。</p>
<hr>
<h2 id="motivations">Motivations<a hidden class="anchor" aria-hidden="true" href="#motivations">#</a></h2>
<p>在多模态大语言模型（MLLMs）中，视觉 token 的输入长度往往远大于文本 token，从而带来高昂的推理开销。例如：</p>
<ul>
<li>LLaVA-1.5 将一张 336×336 图像转换为 <strong>576 tokens</strong>；</li>
<li>LLaVA-NeXT 的高分辨率版本在输入加倍的情况下生成 <strong>2,880 tokens</strong>；</li>
<li>LongVA 处理 2,000 帧视频时生成超过 <strong>200K visual tokens</strong>；</li>
<li>LongVILA 能处理 <strong>6,000 帧</strong>并产生 <strong>超过 1M visual tokens</strong>，导致巨大的计算成本。</li>
</ul>
<hr>
<h2 id="challenges">Challenges<a hidden class="anchor" aria-hidden="true" href="#challenges">#</a></h2>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/figure1.png"></p>
<p>现有的视觉 token 剪枝方法主要分为两类：</p>
<ol>
<li><strong>Attention-based pruning</strong>：利用 text-visual attention 分数衡量视觉 token 的重要性。</li>
<li><strong>Similarity-based pruning</strong>：依据视觉 token 间的相似性移除冗余部分。</li>
</ol>
<p>然而，这两种方法均存在固有缺陷。Attention-based 方法只考虑重要性，容易保留大量重复 token；Similarity-based 方法忽略了指令（instruction）的关联性，导致无法针对问题进行动态剪枝，从而性能次优。</p>
<blockquote>
<p>“However, as pointed out by Zhang et al. [2024b] and Wen et al. [2025a], such methods suffer from <strong>attention shift</strong>, which compromises pruning accuracy.”</p>
</blockquote>
<h3 id="-什么是-attention-shift">⚠ 什么是 attention shift？<a hidden class="anchor" aria-hidden="true" href="#-什么是-attention-shift">#</a></h3>
<p>(gpt回答 不一定正确)</p>
<p>Attention shift 指在 token 剪枝或输入变化后，模型的注意力分布发生偏移，导致原本重要的 token 被低估或信息丢失，从而降低剪枝准确性。由于 Transformer 的 self-attention 是全局依赖的，删除部分 token 会改变 query-key 分布，使剩余 token 的注意力重新分布，如果不考虑这种 shift，基于注意力的剪枝方法容易出现决策失真，保留重复 token 或遗漏关键 token，从而影响推理性能。(<strong>Zhang et al., 2024b, “[CLS] Attention Is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster”; Wen et al., 2025a, “Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?”</strong>)</p>
<h3 id="-attention-based-方法不兼容高效实现如-flashattention">⚠ attention-based 方法不兼容高效实现如 FlashAttention<a hidden class="anchor" aria-hidden="true" href="#-attention-based-方法不兼容高效实现如-flashattention">#</a></h3>
<p>此外，attention-based 方法还依赖显式 attention 权重，不兼容高效实现如 FlashAttention。</p>
<blockquote>
<p>因为attention-based 方法依赖显式的 <strong>attention 权重矩阵（attention map）</strong> 来评估每个视觉 token 的重要性，需要在推理过程中访问完整的 Softmax(QKᵀ) 结果或其中的行向量。但高效注意力实现（如 <strong>FlashAttention</strong>）的核心思想正是<strong>避免显式构建与存储整个注意力矩阵</strong>。</p>
<p>FlashAttention 将注意力计算分块（block-wise）执行，通过在 GPU 的高速寄存器和片上 SRAM 中即时计算 <code>Softmax(QKᵀ)V</code>，并在每个块结束后立刻丢弃中间的 QKᵀ 结果与注意力权重，仅保留最终输出。这种方法极大降低了显存读写和带宽占用，使得注意力计算的复杂度从内存瓶颈（memory-bound）变为计算受限（compute-bound），从而实现高效推理。</p>
<p>然而，这种实现方式带来三个关键后果，使 <strong>attention-based pruning 与 FlashAttention 不兼容</strong>：</p>
<ol>
<li><strong>不可访问性</strong>：FlashAttention 不显式存储或返回完整的 attention map，因此无法直接提取每个 token 的注意力权重；而 attention-based 方法恰恰需要这些分数来判断保留与删除。</li>
<li><strong>存储与性能冲突</strong>：若强行修改 FlashAttention 以输出 attention map，就必须重新显式计算并缓存 QKᵀ 和 Softmax 结果，这会破坏其内存复用机制，重新引入大规模内存访问与显存占用，性能急剧下降。</li>
<li><strong>多头与层次不稳定性</strong>：不同层、不同头的注意力权重分布差异显著，且 FlashAttention 内部按块累积 Softmax，会进一步导致 attention 值在不同分块间不可直接比较，增加了基于 attention 值进行统一排序和剪枝的难度。</li>
</ol>
<p>因此，在采用 FlashAttention 或其他高效注意力优化（如 xFormers、PagedAttention）的现代 MLLM 中，<strong>attention-based pruning 方法无法直接使用或会破坏推理加速效果</strong>。</p>
</blockquote>
<p>因此，作者提出从 “beyond attention or similarity” 的角度重新思考 token pruning。</p>
<hr>
<h2 id="contributions">Contributions<a hidden class="anchor" aria-hidden="true" href="#contributions">#</a></h2>
<ol>
<li>提出 <strong>CDPruner</strong>：一种 plug-and-play、model-agnostic 的视觉 token 剪枝方案，通过最大化条件多样性（conditional diversity）实现高效动态剪枝；</li>
<li>将 token pruning 问题重构为 <strong>DPP（Determinantal Point Process）</strong>，联合考虑 feature similarity 与 instruction relevance；</li>
<li>在多种视觉语言基准上实验验证，CDPruner 在不同压缩率下均取得 SOTA。</li>
</ol>
<hr>
<h2 id="methods">Methods<a hidden class="anchor" aria-hidden="true" href="#methods">#</a></h2>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/figure2.png"></p>
<h3 id="determinantal-point-process-dpp">Determinantal Point Process (DPP)<a hidden class="anchor" aria-hidden="true" href="#determinantal-point-process-dpp">#</a></h3>
<p>DPP 最初用于刻画费米子系统的“排斥效应”，在机器学习中被广泛用于建模集合选择的<strong>全局多样性</strong>。</p>
<p>与 Max-Min Diversity Problem (MMDP) 仅关注极端样本不同，DPP 强调全局平衡和代表性。传统 DPP 仅考虑样本间相似度，而本论文在此基础上引入 instruction relevance，使剪枝过程同时考虑“token 相关性”与“多样性”。</p>
<hr>
<h3 id="dpp-with-token-similarity">DPP with Token Similarity<a hidden class="anchor" aria-hidden="true" href="#dpp-with-token-similarity">#</a></h3>
<p><strong>核心思想：</strong>
将视觉 token 的 pairwise similarity 建模为一个核矩阵 ( L )，并通过最大化其行列式（determinant）来选择最具代表性的子集。</p>
<p>定义每个视觉 token 的特征向量为 ( $H^v_i \in \mathbb{R}^d$ )，则相似核矩阵为：</p>
<p>$$
L_{ij} = \frac{H^v_i \cdot H^v_j}{|H^v_i| , |H^v_j|}
$$</p>
<p>目标是选择一个包含 ( m ) 个 token 的子集 ( $S \subset Z$ )，使得：</p>
<p>$$
S^* = \arg\max_{S \subset Z, |S|=m} \det(L_S)
$$</p>
<p>这里 ( $L_S$ ) 是对应子集的子矩阵。行列式越大，代表该子集在特征空间中“覆盖的方向”越多，信息冗余越低。</p>
<p><strong>直观解释：</strong>
如果两个 token 的特征非常相似（线性相关），行列式会减小。因此 DPP 天然倾向保留“互补”信息而非重复 token，从而实现高效且全局均衡的剪枝。</p>
<hr>
<h3 id="instruction-relevance">Instruction Relevance<a hidden class="anchor" aria-hidden="true" href="#instruction-relevance">#</a></h3>
<p>传统 DPP 仅基于视觉特征构建核矩阵，无法体现视觉 token 与文本指令的相关性。CDPruner 通过以下步骤引入 <strong>条件相关性（conditional relevance）</strong>：</p>
<ol>
<li>
<p>获取视觉 token 表示 ( $H_v \in \mathbb{R}^{n \times d}$ )；</p>
</li>
<li>
<p>获取文本嵌入（instruction embedding） ( $\bar{H}_q \in \mathbb{R}^d $)，其来源可以是：</p>
<ul>
<li>CLIP-like text encoder（若模型具备双编码结构）；</li>
<li>或通过 multimodal projector 与 LLM 的指令 token 平均表示。</li>
</ul>
</li>
<li>
<p>计算每个视觉 token 与 instruction 的余弦相似度：</p>
<p>$$
r_i = \frac{H^v_i \cdot \bar{H}_q}{|H^v_i| , |\bar{H}_q|}
$$</p>
</li>
<li>
<p>对相关性进行 min–max 归一化：</p>
<p>$$
\tilde{r}_i = \frac{r_i - \min(r)}{\max(r) - \min(r)}
$$</p>
</li>
</ol>
<p>得到的向量 ( $\tilde{r} \in [0,1]^n$ ) 反映了各视觉 token 对当前指令的重要程度。</p>
<p><strong>直观理解：</strong></p>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/figure3.png"></p>
<p>这使得剪枝可以根据用户问题动态调整保留区域。例如，同一张图片在不同问题下，关注区域（即高 ( $\tilde{r}_i$ )）完全不同。论文在 Figure 3 中展示了这种可视化结果。</p>
<hr>
<h3 id="cdpruner">CDPruner<a hidden class="anchor" aria-hidden="true" href="#cdpruner">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/figure2.png"></p>
<p><strong>核心机制：条件 DPP（Conditional DPP）</strong></p>
<p>为了联合考虑视觉特征多样性与指令相关性，CDPruner 构建了条件核矩阵：</p>
<p>$$
\tilde{L} = \operatorname{diag}(\tilde{r}) , L , \operatorname{diag}(\tilde{r})
$$</p>
<p>DPP 的目标变为最大化该核矩阵的行列式：</p>
<p>$$
S^* = \arg\max_{S \subset Z, |S| = m} \det(\tilde{L}_S)
$$</p>
<p>通过对数形式可以分解为：</p>
<p>$$
\log\det(\tilde{L}<em>S) = \sum</em>{i \in S} \log(\tilde{r}_i^2) + \log\det(L_S)
$$</p>
<p>这清楚地表明，CDPruner 同时优化两项：</p>
<ul>
<li>relevance term（指令相关性）</li>
<li>diversity term（全局多样性）</li>
</ul>
<hr>
<p><strong>MAP 推断与高效近似</strong></p>
<p>DPP 的 MAP inference 是 NP-hard 的，因此论文采用了 <strong>贪心近似（Fast Greedy MAP）</strong> 算法：</p>
<ol>
<li>初始化空集合 ( S = \emptyset )；</li>
<li>每次迭代选取能最大化当前增益（即行列式增加量）的 token；</li>
<li>使用 Cholesky 分解快速计算增益，从而实现高效近似。</li>
</ol>
<p>该算法的时间复杂度为：</p>
<p>$$
O(nm^2)
$$</p>
<p>在实践中，当保留 token 数 ( m \ll n ) 时，额外延迟仅约 <strong>&lt;10ms/sample</strong>，可直接应用于 MLLM 推理流程中。</p>
<hr>
<p><strong>可调权重（平衡项）</strong></p>
<p>论文还提出可选平衡因子 ( \theta )，用于控制相关性与多样性之间的权重：</p>
<p>$$
\log\det(\tilde{L}<em>S) = \theta \sum</em>{i \in S} \tilde{r}_i + (1 - \theta) \log\det(L_S)
$$</p>
<p>实验显示在不同任务上最优的 ( $\theta$ ) 不同，为模型提供了更好的灵活性。</p>
<hr>
<p><strong>Implementation &amp; Model-agnostic 特性</strong></p>
<ul>
<li>对不同架构均可用（LLaVA、Qwen2.5-VL 等）；</li>
<li>若模型具备 CLIP-like 双编码结构，直接使用 text encoder 的输出；</li>
<li>否则通过 multimodal projector + LLM 的 instruction token 平均得到文本表示；</li>
<li>完全 <strong>training-free</strong>，可直接嵌入推理流程作为模块。</li>
</ul>
<hr>
<h2 id="evaluations">Evaluations<a hidden class="anchor" aria-hidden="true" href="#evaluations">#</a></h2>
<h3 id="main-results">Main Results<a hidden class="anchor" aria-hidden="true" href="#main-results">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/table1.png"></p>
<p>在多种视觉语言基准上，CDPruner 在不同 token 削减率下均超过已有方法。</p>
<hr>
<h3 id="cdpruner-for-high-resolution-inputs">CDPruner for High-resolution Inputs<a hidden class="anchor" aria-hidden="true" href="#cdpruner-for-high-resolution-inputs">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/table2.png"></p>
<p>对高分辨率输入（如 LLaVA-NeXT-7B, 2880→320 tokens），CDPruner 显著减少 FLOPs 与延迟，同时保持性能。</p>
<hr>
<h3 id="cdpruner-for-video-understanding--advanced-architectures">CDPruner for Video Understanding &amp; Advanced Architectures<a hidden class="anchor" aria-hidden="true" href="#cdpruner-for-video-understanding--advanced-architectures">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/table34.png"></p>
<p>在视频任务与先进架构（如 Qwen2.5-VL）上，CDPruner 仍能在不同压缩率下取得一致提升，验证了方法的通用性与可迁移性。</p>
<hr>
<h3 id="efficiency-analysis--ablation-study">Efficiency Analysis &amp; Ablation Study<a hidden class="anchor" aria-hidden="true" href="#efficiency-analysis--ablation-study">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/t5f4.png"></p>
<p>论文的效率实验（以 LLaVA-NeXT-7B 为例）显示：</p>
<ul>
<li><strong>FLOPs</strong> 减少约 10×；</li>
<li><strong>Prefill latency</strong>、<strong>Decode latency</strong> 显著下降；</li>
<li><strong>GPU memory</strong> 与 <strong>KV cache size</strong> 大幅降低；</li>
<li>仅在极高剪枝率下性能略有下降。</li>
</ul>
<p>Ablation 研究表明：</p>
<ul>
<li>同时考虑 conditional relevance 与 DPP 多样性带来最佳性能；</li>
<li>仅使用 attention 或 similarity 的变体均低于 CDPruner。</li>
</ul>
<hr>
<h2 id="conclusions">Conclusions<a hidden class="anchor" aria-hidden="true" href="#conclusions">#</a></h2>
<p>本文提出了一个 <strong>训练无关、模型无关</strong> 的视觉 token 剪枝方法 <strong>CDPruner</strong>，通过定义基于指令的条件相似性，并以 DPP 形式最大化选中 token 的条件多样性，实现了推理加速与性能保持的统一。</p>
<p>CDPruner 在多种 MLLM 架构（如 LLaVA 系列与 Qwen2.5-VL）上实现 SOTA 性能，同时显著降低延迟与显存使用，展现出在真实应用中部署多模态大模型的潜力。</p>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jjl357.github.io/blog/tags/kv-cache/">KV Cache</a></li>
      <li><a href="https://jjl357.github.io/blog/tags/mllm/">MLLM</a></li>
      <li><a href="https://jjl357.github.io/blog/tags/paper-note/">Paper Note</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://jjl357.github.io/blog/">JJ&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
