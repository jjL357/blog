<!DOCTYPE html>
<html lang="zh-cn" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference  | JJ&#39;s Blog</title>
<meta name="keywords" content="KV Cache, Paper Note">
<meta name="description" content="
Conference: NeurIPS&#39;25
Github: https://github.com/FFY0/AdaKV
My Thoughts
这篇工作的idea在各个head之间adaptive 分配 budget, 直觉上就能知道这种设计是有效的，实验也验证了这一点。
比较惊喜的是作者实现了With efficient CUDA kernel implementations来解决variable-sized cache elements across attention heads，从而来真正实现了计算加速，以及其他KV Cache工作在也实现了对AdaKV集成，再次证明了其作为通用增强模块的价值。
1. Motivation
大型语言模型（LLM）在各个领域表现出色，但由于长序列推理所需的不断增长的键值（KV）cache，面临着效率挑战。LLM 的广泛应用推动了其处理扩展序列能力的发展。例如，GPT 支持长达 128K 的序列，Claude3 支持 200K，Gemini-Pro-1.5 甚至支持高达 2M 个 token。然而，这种 token 长度的增长带来了显著的挑战，尤其是在推理过程中cache大小的急剧膨胀。对于一个 8B 的 LLM，处理一个 2M token 的序列可能需要高达 256GB 的cache，这严重影响了 GPU 内存效率和计算运行时效率。
现有的 KV cache驱逐方法通常在所有注意力head上均匀分配压缩预算，忽略了每个head独特的注意力模式。如 Figure 1a 所示，不同head的注意力集中度（concentration）差异巨大：一些head（sparse heads）的注意力高度集中在少数几个 token 上，而另一些head（dispersed heads）的注意力则分布得更广。这种均匀分配导致了效率低下——要么在稀疏集中的head上浪费cache预算，要么在分散分布的head上造成显著的驱逐损失。

2. Relative Work
2.1 Cache Eviction Methods
cache驱逐方法主要分为两类：滑动窗口驱逐和 Top-k 驱逐。

滑动窗口方法（如 StreamingLLM）简单地保留初始cache元素和滑动窗口内的元素，但这种无差别的驱逐会显著降低生成质量。
Top-k 驱逐方法（如 H2O, SnapKV, Pyramid）基于注意力权重识别并保留 k 个关键cache元素。然而，现有 Top-k 方法通常在不同head上均匀分配总预算。Ada-KV 通过自适应预算分配来增强这些方法。

2.2 Sparse Attention Methods
稀疏注意力方法与 KV cache驱逐在根本上不同：前者保留所有cache，但在计算时只选择性地使用关键子集，因此不减少内存占用。而 KV cache驱逐直接移除非关键条目，从而减小内存占用。这两种技术是正orthogonal的，未来可以结合使用。">
<meta name="author" content="">
<link rel="canonical" href="https://jjl357.github.io/blog/posts/ada-kv---optimizing-kv-cache-eviction-by-adaptive-budget-allocation-for-efficient-llm-inference---neurips25/">
<link crossorigin="anonymous" href="https://jjl357.github.io/blog/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jjl357.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jjl357.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jjl357.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jjl357.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://jjl357.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh-cn" href="https://jjl357.github.io/blog/posts/ada-kv---optimizing-kv-cache-eviction-by-adaptive-budget-allocation-for-efficient-llm-inference---neurips25/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><script type="text/javascript"
        async
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<meta property="og:url" content="https://jjl357.github.io/blog/posts/ada-kv---optimizing-kv-cache-eviction-by-adaptive-budget-allocation-for-efficient-llm-inference---neurips25/">
  <meta property="og:site_name" content="JJ&#39;s Blog">
  <meta property="og:title" content="Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference ">
  <meta property="og:description" content="
Conference: NeurIPS&#39;25
Github: https://github.com/FFY0/AdaKV
My Thoughts 这篇工作的idea在各个head之间adaptive 分配 budget, 直觉上就能知道这种设计是有效的，实验也验证了这一点。
比较惊喜的是作者实现了With efficient CUDA kernel implementations来解决variable-sized cache elements across attention heads，从而来真正实现了计算加速，以及其他KV Cache工作在也实现了对AdaKV集成，再次证明了其作为通用增强模块的价值。
1. Motivation 大型语言模型（LLM）在各个领域表现出色，但由于长序列推理所需的不断增长的键值（KV）cache，面临着效率挑战。LLM 的广泛应用推动了其处理扩展序列能力的发展。例如，GPT 支持长达 128K 的序列，Claude3 支持 200K，Gemini-Pro-1.5 甚至支持高达 2M 个 token。然而，这种 token 长度的增长带来了显著的挑战，尤其是在推理过程中cache大小的急剧膨胀。对于一个 8B 的 LLM，处理一个 2M token 的序列可能需要高达 256GB 的cache，这严重影响了 GPU 内存效率和计算运行时效率。
现有的 KV cache驱逐方法通常在所有注意力head上均匀分配压缩预算，忽略了每个head独特的注意力模式。如 Figure 1a 所示，不同head的注意力集中度（concentration）差异巨大：一些head（sparse heads）的注意力高度集中在少数几个 token 上，而另一些head（dispersed heads）的注意力则分布得更广。这种均匀分配导致了效率低下——要么在稀疏集中的head上浪费cache预算，要么在分散分布的head上造成显著的驱逐损失。
2. Relative Work 2.1 Cache Eviction Methods cache驱逐方法主要分为两类：滑动窗口驱逐和 Top-k 驱逐。
滑动窗口方法（如 StreamingLLM）简单地保留初始cache元素和滑动窗口内的元素，但这种无差别的驱逐会显著降低生成质量。 Top-k 驱逐方法（如 H2O, SnapKV, Pyramid）基于注意力权重识别并保留 k 个关键cache元素。然而，现有 Top-k 方法通常在不同head上均匀分配总预算。Ada-KV 通过自适应预算分配来增强这些方法。 2.2 Sparse Attention Methods 稀疏注意力方法与 KV cache驱逐在根本上不同：前者保留所有cache，但在计算时只选择性地使用关键子集，因此不减少内存占用。而 KV cache驱逐直接移除非关键条目，从而减小内存占用。这两种技术是正orthogonal的，未来可以结合使用。">
  <meta property="og:locale" content="zh-cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-01T00:00:00+00:00">
    <meta property="article:tag" content="KV Cache">
    <meta property="article:tag" content="Paper Note">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference ">
<meta name="twitter:description" content="
Conference: NeurIPS&#39;25
Github: https://github.com/FFY0/AdaKV
My Thoughts
这篇工作的idea在各个head之间adaptive 分配 budget, 直觉上就能知道这种设计是有效的，实验也验证了这一点。
比较惊喜的是作者实现了With efficient CUDA kernel implementations来解决variable-sized cache elements across attention heads，从而来真正实现了计算加速，以及其他KV Cache工作在也实现了对AdaKV集成，再次证明了其作为通用增强模块的价值。
1. Motivation
大型语言模型（LLM）在各个领域表现出色，但由于长序列推理所需的不断增长的键值（KV）cache，面临着效率挑战。LLM 的广泛应用推动了其处理扩展序列能力的发展。例如，GPT 支持长达 128K 的序列，Claude3 支持 200K，Gemini-Pro-1.5 甚至支持高达 2M 个 token。然而，这种 token 长度的增长带来了显著的挑战，尤其是在推理过程中cache大小的急剧膨胀。对于一个 8B 的 LLM，处理一个 2M token 的序列可能需要高达 256GB 的cache，这严重影响了 GPU 内存效率和计算运行时效率。
现有的 KV cache驱逐方法通常在所有注意力head上均匀分配压缩预算，忽略了每个head独特的注意力模式。如 Figure 1a 所示，不同head的注意力集中度（concentration）差异巨大：一些head（sparse heads）的注意力高度集中在少数几个 token 上，而另一些head（dispersed heads）的注意力则分布得更广。这种均匀分配导致了效率低下——要么在稀疏集中的head上浪费cache预算，要么在分散分布的head上造成显著的驱逐损失。

2. Relative Work
2.1 Cache Eviction Methods
cache驱逐方法主要分为两类：滑动窗口驱逐和 Top-k 驱逐。

滑动窗口方法（如 StreamingLLM）简单地保留初始cache元素和滑动窗口内的元素，但这种无差别的驱逐会显著降低生成质量。
Top-k 驱逐方法（如 H2O, SnapKV, Pyramid）基于注意力权重识别并保留 k 个关键cache元素。然而，现有 Top-k 方法通常在不同head上均匀分配总预算。Ada-KV 通过自适应预算分配来增强这些方法。

2.2 Sparse Attention Methods
稀疏注意力方法与 KV cache驱逐在根本上不同：前者保留所有cache，但在计算时只选择性地使用关键子集，因此不减少内存占用。而 KV cache驱逐直接移除非关键条目，从而减小内存占用。这两种技术是正orthogonal的，未来可以结合使用。">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jjl357.github.io/blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference ",
      "item": "https://jjl357.github.io/blog/posts/ada-kv---optimizing-kv-cache-eviction-by-adaptive-budget-allocation-for-efficient-llm-inference---neurips25/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference ",
  "name": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference ",
  "description": "\nConference: NeurIPS'25\nGithub: https://github.com/FFY0/AdaKV\nMy Thoughts 这篇工作的idea在各个head之间adaptive 分配 budget, 直觉上就能知道这种设计是有效的，实验也验证了这一点。\n比较惊喜的是作者实现了With efficient CUDA kernel implementations来解决variable-sized cache elements across attention heads，从而来真正实现了计算加速，以及其他KV Cache工作在也实现了对AdaKV集成，再次证明了其作为通用增强模块的价值。\n1. Motivation 大型语言模型（LLM）在各个领域表现出色，但由于长序列推理所需的不断增长的键值（KV）cache，面临着效率挑战。LLM 的广泛应用推动了其处理扩展序列能力的发展。例如，GPT 支持长达 128K 的序列，Claude3 支持 200K，Gemini-Pro-1.5 甚至支持高达 2M 个 token。然而，这种 token 长度的增长带来了显著的挑战，尤其是在推理过程中cache大小的急剧膨胀。对于一个 8B 的 LLM，处理一个 2M token 的序列可能需要高达 256GB 的cache，这严重影响了 GPU 内存效率和计算运行时效率。\n现有的 KV cache驱逐方法通常在所有注意力head上均匀分配压缩预算，忽略了每个head独特的注意力模式。如 Figure 1a 所示，不同head的注意力集中度（concentration）差异巨大：一些head（sparse heads）的注意力高度集中在少数几个 token 上，而另一些head（dispersed heads）的注意力则分布得更广。这种均匀分配导致了效率低下——要么在稀疏集中的head上浪费cache预算，要么在分散分布的head上造成显著的驱逐损失。\n2. Relative Work 2.1 Cache Eviction Methods cache驱逐方法主要分为两类：滑动窗口驱逐和 Top-k 驱逐。\n滑动窗口方法（如 StreamingLLM）简单地保留初始cache元素和滑动窗口内的元素，但这种无差别的驱逐会显著降低生成质量。 Top-k 驱逐方法（如 H2O, SnapKV, Pyramid）基于注意力权重识别并保留 k 个关键cache元素。然而，现有 Top-k 方法通常在不同head上均匀分配总预算。Ada-KV 通过自适应预算分配来增强这些方法。 2.2 Sparse Attention Methods 稀疏注意力方法与 KV cache驱逐在根本上不同：前者保留所有cache，但在计算时只选择性地使用关键子集，因此不减少内存占用。而 KV cache驱逐直接移除非关键条目，从而减小内存占用。这两种技术是正orthogonal的，未来可以结合使用。\n",
  "keywords": [
    "KV Cache", "Paper Note"
  ],
  "articleBody": "\nConference: NeurIPS'25\nGithub: https://github.com/FFY0/AdaKV\nMy Thoughts 这篇工作的idea在各个head之间adaptive 分配 budget, 直觉上就能知道这种设计是有效的，实验也验证了这一点。\n比较惊喜的是作者实现了With efficient CUDA kernel implementations来解决variable-sized cache elements across attention heads，从而来真正实现了计算加速，以及其他KV Cache工作在也实现了对AdaKV集成，再次证明了其作为通用增强模块的价值。\n1. Motivation 大型语言模型（LLM）在各个领域表现出色，但由于长序列推理所需的不断增长的键值（KV）cache，面临着效率挑战。LLM 的广泛应用推动了其处理扩展序列能力的发展。例如，GPT 支持长达 128K 的序列，Claude3 支持 200K，Gemini-Pro-1.5 甚至支持高达 2M 个 token。然而，这种 token 长度的增长带来了显著的挑战，尤其是在推理过程中cache大小的急剧膨胀。对于一个 8B 的 LLM，处理一个 2M token 的序列可能需要高达 256GB 的cache，这严重影响了 GPU 内存效率和计算运行时效率。\n现有的 KV cache驱逐方法通常在所有注意力head上均匀分配压缩预算，忽略了每个head独特的注意力模式。如 Figure 1a 所示，不同head的注意力集中度（concentration）差异巨大：一些head（sparse heads）的注意力高度集中在少数几个 token 上，而另一些head（dispersed heads）的注意力则分布得更广。这种均匀分配导致了效率低下——要么在稀疏集中的head上浪费cache预算，要么在分散分布的head上造成显著的驱逐损失。\n2. Relative Work 2.1 Cache Eviction Methods cache驱逐方法主要分为两类：滑动窗口驱逐和 Top-k 驱逐。\n滑动窗口方法（如 StreamingLLM）简单地保留初始cache元素和滑动窗口内的元素，但这种无差别的驱逐会显著降低生成质量。 Top-k 驱逐方法（如 H2O, SnapKV, Pyramid）基于注意力权重识别并保留 k 个关键cache元素。然而，现有 Top-k 方法通常在不同head上均匀分配总预算。Ada-KV 通过自适应预算分配来增强这些方法。 2.2 Sparse Attention Methods 稀疏注意力方法与 KV cache驱逐在根本上不同：前者保留所有cache，但在计算时只选择性地使用关键子集，因此不减少内存占用。而 KV cache驱逐直接移除非关键条目，从而减小内存占用。这两种技术是正orthogonal的，未来可以结合使用。\n3. Contribution 自适应预算分配：这篇工作指出了当前 KV cache驱逐方法的一个关键限制：均匀预算分配忽略了各个head的独特注意力模式。为此，这篇工作提出了 Ada-KV，这是首个head级别的自适应预算分配策略，通过提高各个head的预算利用率，带来更高效的cache驱逐。 理论分析：这篇工作为cache驱逐建立了一个理论框架，定义了驱逐损失并推导了其上界。该框架解释了先前方法的优化目标，指导了 Ada-KV 的设计，实现了有原则的自适应预算分配。 具体实现：高效的 CUDA 内核实现，Ada-KV 具有即插即用的兼容性，可以集成到现有方法中。这篇工作在 Ruler 和 LongBench 的 29 个数据集上，在问答感知（question-aware）和问答不可知（question-agnostic）两种场景下都证明了其显著的性能提升。 4. Method Ada-KV 的核心思想是：将所有head的注意力权重“拉平”成一个大的列表，然后在这个全局列表中选出权重最大的 B 个元素。每个head i 分配到的预算 $B_i^*$ 就等于其权重在全局 Top-B 中出现的次数。这确保了总预算被分配给了全局最重要的cache元素，从而最小化了理论上的驱逐损失上界。\n4.1 理论基础：驱逐损失的上界 论文首先为 KV Cache 驱逐建立了一个理论框架。\n符号定义:\n考虑一个包含 h 个注意力head的多head自注意力层。 对于第 i 个head，其注意力权重向量为 $A_i \\in \\mathbb{R}^{1 \\times n}$，其中 n 是当前上下文长度。 驱逐决策由一个指示变量 $I_i \\in \\mathbb{R}^{1 \\times n}$ 表示，其中 $I_{ij} = 1$ 表示保留第 j 个 KV cache元素，否则为 0。 第 i 个head的预算为 $B_i$，满足 $\\sum_{j=1}^{n} I_{ij} = B_i$。整个层的总预算为 $B = \\sum_{i=1}^{h} B_i$。 驱逐前后的自注意力输出分别为 y 和 $\\hat{y}$。 驱逐损失 (Eviction Loss): 定义为驱逐前后输出的 L1 距离： $$ L1 \\text{ Eviction Loss} = |y - \\hat{y}|_1 $$\n上界推导: 论文证明了该损失存在一个上界 $\\epsilon$： $$ L1 \\text{ Eviction Loss} \\leq \\epsilon = 2hC - 2C \\sum_{i=1}^{h} \\sum_{j=1}^{n} I_{ij} A_{ji} $$ 其中 $C = \\max{|V_i W_i^O|_{\\infty}}$ 是一个常数。\n对现有方法的解释: 这个上界揭示了现有 Top-k 驱逐方法的优化目标。为了最小化 $\\epsilon$，需要最大化 $\\sum_{i} \\sum_{j} I_{ij} A_{ji}$，即在每个head i 内部，选择注意力权重 $A_{ji}$ 最大的 Bi 个元素保留。这正是 Top-k 驱逐策略。\n4.2 自适应预算分配策略 (Ada-KV) Algorithm 1 描述了 Ada-KV 的核心分配逻辑。 Algorithm 1: Ada-KV: 自适应预算分配\n输入: 总预算 B; 每个head i 的注意力权重 {A_i}\r输出: 分配的预算 {B_i^*}\r1: 将所有head的注意力权重拼接成一个大向量 A = Cat({A_i})\r2: 从 A 中选出最大的 B 个权重: Top-k(A, k = B)\r3: 统计每个head i 在 Top-B 中被选中的次数: {f_i}\r4: 设置分配的预算为 {B_i^* = f_i}\r返回 分配的预算 {B_i^*} 理论优势: Theorem 3.3 证明了这种分配策略能够获得所有可能分配方案中最小的上界 $\\epsilon^{**}$。 4.3 与现有方法的集成 (Ada-SnapKV / Ada-Pyramid) Ada-KV 被设计为“即插即用”，可以无缝集成到现有的 Top-k 驱逐方法中。Algorithm 2 展示了如何将其集成到 SnapKV/Pyramid 中。 Algorithm 2: Ada-SnapKV/Ada-Pyramid 在单层中的流程\n输入: 总预算 B, 观察窗口内的 token X_win ∈ R^{win*d}, 观察窗口内的cache {K_win, V_win}, 观察窗口外的cache {K_i, V_i}\r输出: 保留的cache {K̂_i, V̂_i}\r1: for i ← 1 to h do\r2: Q_win_i = X_win W_i^Q\r3: Ā_i = softmax(Q_win_i K_i^T)\r4: Ā_i = Ā_i.maxpooling(dim=1).mean(dim=0) // 聚合观察窗口内的注意力\r5: end for\r6: B = B - winsize × h // 减去观察窗口内强制保留的cache开销\r7: 使用 Algorithm 1(B, {Ā_i}) 推导预算分配 {B_i^*}\r8: 安全防护: {B_i^*} = α × {B_i^*} + (1 − α) × (B/h) // α 默认为 0.2\r9: 根据 {B_i^*} 确定 Top-k 驱逐决策 {I_i^*}\r10: 根据 {I_i^*} 从 {K_i, V_i} 中选择 {K̂_i, V̂_i}\r11: {K̂_i, V̂_i} = Cat({K̂_i, V̂_i}, {K_win, V_win}) // 与观察窗口cache拼接\r返回 保留的cache {K̂_i, V̂_i} 详细解释：\n输入\nB：总预算（总的 KV cache 可保留的容量）。 X_win ∈ R^{win × d}：观察窗口内的 token embedding，窗口大小为 win，embedding 维度为 d。 K_win, V_win：观察窗口内的 key/value cache。 K_i, V_i：观察窗口外的 key/value cache（通常是历史 token 对应的 cache）。 输出\nK̂_i, V̂_i：最终保留的 cache。 步骤解析\n1–5 行：计算观察窗口内的注意力聚合 1: for i ← 1 to h do 2: Q_win_i = X_win W_i^Q 3: Ā_i = softmax(Q_win_i K_i^T) 4: Ā_i = Ā_i.maxpooling(dim=1).mean(dim=0) 5: end for 目的：评估每个头 i 对观察窗口外的 KV cache 的重要性。\n解释：\nQ_win_i = X_win W_i^Q 计算第 i 个头的查询矩阵。\nĀ_i = softmax(Q_win_i K_i^T) 计算观察窗口内 token 对每个历史 KV 的注意力分数。\nĀ_i = Ā_i.maxpooling(dim=1).mean(dim=0) 聚合注意力信息：\nmaxpooling(dim=1)：取每个观察窗口 token 对历史 KV 中的最大注意力。 mean(dim=0)：对观察窗口 token 求平均，得到每个历史 KV 的重要性分数。 结果：每个 head 得到一个向量 Ā_i，表示该 head 对各个历史 KV 的注意力重要性。\nĀ_i = Ā_i.maxpooling(dim=1).mean(dim=0) Ā_i 是第 i 个注意力头对 历史 KV cache 的注意力分数矩阵。\n形状大概是 (win, num_cache)：\nwin：观察窗口内的 token 数量 num_cache：历史 KV cache 的数量 也就是说，每一行表示一个观察窗口 token 对所有历史 KV 的注意力分布。\n1️⃣ maxpooling(dim=1) 对 每一行（每个观察窗口 token）进行 最大池化：\n取这个 token 对历史 KV 的 最大注意力值。 结果：\n原本 (win, num_cache) → (win,) 向量 这个向量表示 每个 token 最关注的历史 KV 的重要性。 直观理解：我们只关心每个 token 在历史 KV 中的 “最重要的记忆”，而不是平均分散的注意力。\n2️⃣ mean(dim=0) 对 所有观察窗口 token 求平均：\n把每个 token 的最大注意力值加起来再除以 token 数量。 结果：\n一个标量或向量（视具体实现），表示 该注意力头整体对某些历史 KV 的重要性评分。 作用：\n聚合观察窗口内所有 token 的信息，得到一个 全局的历史 KV 重要性向量。 6 行：扣除观察窗口 cache 的开销 6: B = B - winsize × h 解释：观察窗口内的 cache K_win, V_win 是 必须保留的，所以总预算要减去它们占用的空间。 7–8 行：预算分配 7: 使用 Algorithm 1(B, {Ā_i}) 推导预算分配 {B_i^*} 8: {B_i^*} = α × {B_i^*} + (1 − α) × (B/h) 步骤 7：\n调用另一个算法（Algorithm 1）根据各头的重要性向量 Ā_i 来分配剩余预算 B。 得到每个头分配到的预算 B_i^*（可保留的 KV 数量）。 步骤 8：安全防护：\n用一个平滑系数 α 做线性插值，避免某些头的预算过低。 公式： $$ B_i^* = \\alpha \\cdot B_i^* + (1 - \\alpha) \\cdot \\frac{B}{h} $$\n默认 α=0.2，即 20% 依赖算法分配，80% 保证每个头有基础份额。 9–10 行：选择保留的 KV 9: 根据 {B_i^*} 确定 Top-k 驱逐决策 {I_i^*} 10: 根据 {I_i^*} 从 {K_i, V_i} 中选择 {K̂_i, V̂_i} 步骤 9：\n对每个头，选择 Top-k KV，其中 k = B_i^*。 I_i^* 是被保留的 KV 的索引。 步骤 10：\n根据索引 I_i^* 从历史 cache 中提取对应的 K̂_i, V̂_i。 11 行：与观察窗口 cache 拼接 11: {K̂_i, V̂_i} = Cat({K̂_i, V̂_i}, {K_win, V_win}) 4.4 高效实现 自适应预算分配会导致不同head的cache长度可变，给高效计算带来挑战。Ada-KV 通过以下方式解决：\n扁平化cache布局 (Flattened Cache Storage Layout): 将一个层内所有head的 KV cache拼接成一个大的张量。 可变长度 FlashAttention: 利用 flash_attn_varlen_func 来高效处理这种可变长度的注意力计算。 自定义 CUDA 内核: 实现了高效的cache更新操作，以支持在扁平化布局下动态插入新的 KV 对。如 Figure 2 所示，更新过程分为 malloc、copy old value 和 insert new value 三个阶段。 GQA 兼容性: 针对现代 LLM（如 Llama, Mistral）广泛使用的 Grouped Query Attention (GQA)，Ada-KV 通过在每个 GQA 组内使用平均注意力权重作为选择标准，避免了冗余计算。 5. Evaluation 5.1 实验设置 基座模型: Llama-3.1-8B-Instruct 和 Mistral-7B-Instruct-v0.2（均使用 GQA）。 数据集: Ruler (13个任务) 和 LongBench (16个数据集)。 评估场景: Question-aware: 压缩时已知问题（标准场景）。 Question-agnostic: 更具挑战性的场景，压缩时不知道问题，更能反映真实应用（如提示cache）。 基线方法: SOTA 的 SnapKV 和 Pyramid，以及滑动窗口方法 StreamingLLM。 参数: 观察窗口大小为32，安全系数 $\\alpha=0.2$。 5.2 Ruler 基准测试结果 总体表现: 如 Figure 3 所示，Ada-SnapKV 和 Ada-Pyramid 始终优于 原始方法，尤其在 question-agnostic 场景下优势更显著。例如，在 Llama-3.1-8B 上，20% cache预算时，Ada-SnapKV 将 SnapKV 的得分从 44.02 大幅提升至 53.29。 子任务分析: Figure 4 展示了 question-agnostic 场景下的详细结果。Ada-KV 在几乎所有任务上都带来了显著提升。特别在困难的 NIAH 任务（如 S-NIAH-3, MK-NIAH-2）上，Ada-SnapKV 在 80% 预算下几乎实现了无损性能（得分 97.6 和 99.6），而原始 SnapKV 则有明显下降。 5.3 LongBench 基准测试结果 固定预算评估: Figure 5 显示，在 question-aware 场景下，Ada-变体持续优于基线。在 question-agnostic 场景下，所有方法性能都有所下降，但 Ada-变体依然保持领先。 按比例预算评估: Table 1 和 Table 2（Llama-70B）展示了按任务领域的详细结果。Ada-SnapKV 在绝大多数领域（18个领域中的15个）都有效缓解了压缩带来的性能损失，证明了其通用性和鲁棒性。 5.4 计算效率分析 Ada-KV 的核心目标是在不牺牲效率的前提下提升质量。Figure 6 的结果表明，得益于高效的 CUDA 实现和可变长度 FlashAttention，Ada-SnapKV 在峰值内存占用和解码延迟方面与原始的 SnapKV 基本持平，并且都远优于使用完整cache的情况。\n5.5 广泛适用性 由于其即插即用的特性，Ada-KV 已被后续多项工作（如 CriticalKV, DefensiveKV）采用。Table 3 显示，将 Ada-KV 应用于这些新方法后，性能得到了进一步提升，再次证明了其作为通用增强模块的价值。\n6. Conclusion 本研究重新审视了用于高效 LLM 推理的cache驱逐策略，并发现了一个被忽视的关键因素：跨注意力head的自适应预算分配。在驱逐损失上界的理论分析指导下，这篇工作提出了 Ada-KV，这是首个用于优化 KV cache驱逐方法的自适应预算分配策略。这篇工作通过将其集成到两个现有的 SOTA 方法中，引入了 Ada-SnapKV 和 Ada-Pyramid。除了常见的问答感知压缩场景，这篇工作还在更具挑战性且较少被探索的问答不可知压缩场景中评估了这些方法。使用 Ruler 和 LongBench 基准，这篇工作证明了自适应预算分配在这两种设置下的有效性。这篇工作的结果不仅暴露了当前cache驱逐策略的局限性，也突显了自适应分配在增强cache驱逐方面的潜力。\n",
  "wordCount" : "898",
  "inLanguage": "zh-cn",
  "datePublished": "2025-11-01T00:00:00Z",
  "dateModified": "2025-11-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jjl357.github.io/blog/posts/ada-kv---optimizing-kv-cache-eviction-by-adaptive-budget-allocation-for-efficient-llm-inference---neurips25/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JJ's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jjl357.github.io/blog/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
  <nav class="nav">
    <div class="logo">
      
      <a href="https://jjl357.github.io/" accesskey="h" title="🥛 ☕ 🍵 (Alt + H)">
        <span>🥛 ☕ 🍵</span>
        
      </a>

      <div class="logo-switches">
        <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
          <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      </div>
    </div>
    <ul id="menu">
    </ul>
  </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference 
    </h1>
    <div class="post-meta"><span title='2025-11-01 00:00:00 +0000 UTC'>November 1, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="https://jjl357.github.io/blog/image/AdaKV/title.png"></p>
<p><strong>Conference</strong>: <strong>NeurIPS'25</strong></p>
<p><strong>Github</strong>: <a href="https://github.com/FFY0/AdaKV">https://github.com/FFY0/AdaKV</a></p>
<h2 id="my-thoughts">My Thoughts<a hidden class="anchor" aria-hidden="true" href="#my-thoughts">#</a></h2>
<p>这篇工作的idea在各个head之间adaptive 分配 budget, 直觉上就能知道这种设计是有效的，实验也验证了这一点。</p>
<p>比较惊喜的是作者实现了With efficient CUDA kernel implementations来解决variable-sized cache elements across attention heads，从而来真正实现了计算加速，以及其他KV Cache工作在也实现了对AdaKV集成，再次证明了其作为通用增强模块的价值。</p>
<h2 id="1-motivation">1. Motivation<a hidden class="anchor" aria-hidden="true" href="#1-motivation">#</a></h2>
<p>大型语言模型（LLM）在各个领域表现出色，但由于长序列推理所需的不断增长的键值（KV）cache，面临着效率挑战。LLM 的广泛应用推动了其处理扩展序列能力的发展。例如，GPT 支持长达 128K 的序列，Claude3 支持 200K，Gemini-Pro-1.5 甚至支持高达 2M 个 token。然而，这种 token 长度的增长带来了显著的挑战，尤其是在推理过程中cache大小的急剧膨胀。对于一个 8B 的 LLM，处理一个 2M token 的序列可能需要高达 256GB 的cache，这严重影响了 GPU 内存效率和计算运行时效率。</p>
<p>现有的 KV cache驱逐方法通常在所有注意力head上<strong>均匀分配</strong>压缩预算，忽略了每个head独特的注意力模式。如 <strong>Figure 1a</strong> 所示，不同head的注意力集中度（concentration）差异巨大：一些head（sparse heads）的注意力高度集中在少数几个 token 上，而另一些head（dispersed heads）的注意力则分布得更广。这种均匀分配导致了效率低下——要么在稀疏集中的head上浪费cache预算，要么在分散分布的head上造成显著的驱逐损失。</p>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/AdaKV/figure1.png"></p>
<h2 id="2-relative-work">2. Relative Work<a hidden class="anchor" aria-hidden="true" href="#2-relative-work">#</a></h2>
<h3 id="21-cache-eviction-methods">2.1 Cache Eviction Methods<a hidden class="anchor" aria-hidden="true" href="#21-cache-eviction-methods">#</a></h3>
<p>cache驱逐方法主要分为两类：滑动窗口驱逐和 Top-k 驱逐。</p>
<ul>
<li><strong>滑动窗口方法</strong>（如 StreamingLLM）简单地保留初始cache元素和滑动窗口内的元素，但这种无差别的驱逐会显著降低生成质量。</li>
<li><strong>Top-k 驱逐方法</strong>（如 H2O, SnapKV, Pyramid）基于注意力权重识别并保留 <code>k</code> 个关键cache元素。然而，现有 Top-k 方法通常在不同head上<strong>均匀分配</strong>总预算。Ada-KV 通过<strong>自适应预算分配</strong>来增强这些方法。</li>
</ul>
<h3 id="22-sparse-attention-methods">2.2 Sparse Attention Methods<a hidden class="anchor" aria-hidden="true" href="#22-sparse-attention-methods">#</a></h3>
<p>稀疏注意力方法与 KV cache驱逐在根本上不同：前者保留所有cache，但在计算时只选择性地使用关键子集，因此<strong>不减少内存占用</strong>。而 KV cache驱逐直接移除非关键条目，从而减小内存占用。这两种技术是正orthogonal的，未来可以结合使用。</p>
<h2 id="3-contribution">3. Contribution<a hidden class="anchor" aria-hidden="true" href="#3-contribution">#</a></h2>
<ul>
<li><strong>自适应预算分配</strong>：这篇工作指出了当前 KV cache驱逐方法的一个关键限制：均匀预算分配忽略了各个head的独特注意力模式。为此，这篇工作提出了 Ada-KV，这是<strong>首个</strong>head级别的自适应预算分配策略，通过提高各个head的预算利用率，带来更高效的cache驱逐。</li>
<li><strong>理论分析</strong>：这篇工作为cache驱逐建立了一个理论框架，定义了驱逐损失并推导了其上界。该框架解释了先前方法的优化目标，指导了 Ada-KV 的设计，实现了有原则的自适应预算分配。</li>
<li><strong>具体实现</strong>：高效的 CUDA 内核实现，Ada-KV 具有即插即用的兼容性，可以集成到现有方法中。这篇工作在 Ruler 和 LongBench 的 29 个数据集上，在问答感知（question-aware）和问答不可知（question-agnostic）两种场景下都证明了其显著的性能提升。</li>
</ul>
<h2 id="4-method">4. Method<a hidden class="anchor" aria-hidden="true" href="#4-method">#</a></h2>
<p>Ada-KV 的核心思想是：<strong>将所有head的注意力权重“拉平”成一个大的列表，然后在这个全局列表中选出权重最大的 <code>B</code> 个元素。每个head <code>i</code> 分配到的预算 $B_i^*$ 就等于其权重在全局 Top-B 中出现的次数</strong>。这确保了总预算被分配给了全局最重要的cache元素，从而最小化了理论上的驱逐损失上界。</p>
<h3 id="41-理论基础驱逐损失的上界">4.1 理论基础：驱逐损失的上界<a hidden class="anchor" aria-hidden="true" href="#41-理论基础驱逐损失的上界">#</a></h3>
<p>论文首先为 KV Cache 驱逐建立了一个理论框架。</p>
<ul>
<li>
<p><strong>符号定义</strong>:</p>
<ul>
<li>考虑一个包含 <code>h</code> 个注意力head的多head自注意力层。</li>
<li>对于第 <code>i</code> 个head，其注意力权重向量为 $A_i \in \mathbb{R}^{1 \times n}$，其中 <code>n</code> 是当前上下文长度。</li>
<li>驱逐决策由一个指示变量 $I_i \in \mathbb{R}^{1 \times n}$ 表示，其中 $I_{ij} = 1$ 表示保留第 <code>j</code> 个 KV cache元素，否则为 0。</li>
<li>第 <code>i</code> 个head的预算为 $B_i$，满足 $\sum_{j=1}^{n} I_{ij} = B_i$。整个层的总预算为 $B = \sum_{i=1}^{h} B_i$。</li>
<li>驱逐前后的自注意力输出分别为 <code>y</code> 和 $\hat{y}$。</li>
</ul>
</li>
<li>
<p><strong>驱逐损失 (Eviction Loss)</strong>: 定义为驱逐前后输出的 L1 距离：
$$
L1 \text{ Eviction Loss} = |y - \hat{y}|_1
$$</p>
</li>
<li>
<p><strong>上界推导</strong>: 论文证明了该损失存在一个上界 $\epsilon$：
$$
L1 \text{ Eviction Loss} \leq \epsilon = 2hC - 2C \sum_{i=1}^{h} \sum_{j=1}^{n} I_{ij} A_{ji}
$$
其中 $C = \max{|V_i W_i^O|_{\infty}}$ 是一个常数。</p>
</li>
<li>
<p><strong>对现有方法的解释</strong>: 这个上界揭示了现有 Top-k 驱逐方法的优化目标。为了最小化 $\epsilon$，需要最大化 $\sum_{i} \sum_{j} I_{ij} A_{ji}$，即在每个head <code>i</code> 内部，选择注意力权重 $A_{ji}$ 最大的 <code>Bi</code> 个元素保留。这正是 Top-k 驱逐策略。</p>
</li>
</ul>
<h3 id="42-自适应预算分配策略-ada-kv">4.2 自适应预算分配策略 (Ada-KV)<a hidden class="anchor" aria-hidden="true" href="#42-自适应预算分配策略-ada-kv">#</a></h3>
<p><strong>Algorithm 1</strong> 描述了 Ada-KV 的核心分配逻辑。
<img loading="lazy" src="https://jjl357.github.io/blog/image/AdaKV/algorithm1.png"></p>
<p><strong>Algorithm 1: Ada-KV: 自适应预算分配</strong></p>
<pre tabindex="0"><code>输入: 总预算 B; 每个head i 的注意力权重 {A_i}
输出: 分配的预算 {B_i^*}

1: 将所有head的注意力权重拼接成一个大向量 A = Cat({A_i})
2: 从 A 中选出最大的 B 个权重: Top-k(A, k = B)
3: 统计每个head i 在 Top-B 中被选中的次数: {f_i}
4: 设置分配的预算为 {B_i^* = f_i}
返回 分配的预算 {B_i^*}
</code></pre><ul>
<li><strong>理论优势</strong>: <strong>Theorem 3.3</strong> 证明了这种分配策略能够获得所有可能分配方案中<strong>最小的上界</strong> $\epsilon^{**}$。</li>
</ul>
<h3 id="43-与现有方法的集成-ada-snapkv--ada-pyramid">4.3 与现有方法的集成 (Ada-SnapKV / Ada-Pyramid)<a hidden class="anchor" aria-hidden="true" href="#43-与现有方法的集成-ada-snapkv--ada-pyramid">#</a></h3>
<p>Ada-KV 被设计为“即插即用”，可以无缝集成到现有的 Top-k 驱逐方法中。<strong>Algorithm 2</strong> 展示了如何将其集成到 SnapKV/Pyramid 中。
<img loading="lazy" src="https://jjl357.github.io/blog/image/AdaKV/algorithm2.png"></p>
<p><strong>Algorithm 2: Ada-SnapKV/Ada-Pyramid 在单层中的流程</strong></p>
<pre tabindex="0"><code>输入: 总预算 B, 观察窗口内的 token X_win ∈ R^{win*d}, 
      观察窗口内的cache {K_win, V_win}, 观察窗口外的cache {K_i, V_i}
输出: 保留的cache {K̂_i, V̂_i}

1: for i ← 1 to h do
2:   Q_win_i = X_win W_i^Q
3:   Ā_i = softmax(Q_win_i K_i^T)
4:   Ā_i = Ā_i.maxpooling(dim=1).mean(dim=0)  // 聚合观察窗口内的注意力
5: end for
6: B = B - winsize × h  // 减去观察窗口内强制保留的cache开销
7: 使用 Algorithm 1(B, {Ā_i}) 推导预算分配 {B_i^*}
8: 安全防护: {B_i^*} = α × {B_i^*} + (1 − α) × (B/h) // α 默认为 0.2
9: 根据 {B_i^*} 确定 Top-k 驱逐决策 {I_i^*}
10: 根据 {I_i^*} 从 {K_i, V_i} 中选择 {K̂_i, V̂_i}
11: {K̂_i, V̂_i} = Cat({K̂_i, V̂_i}, {K_win, V_win}) // 与观察窗口cache拼接
返回 保留的cache {K̂_i, V̂_i}
</code></pre><p><strong>详细解释：</strong></p>
<ul>
<li>
<p><strong>输入</strong></p>
<ul>
<li><code>B</code>：总预算（总的 KV cache 可保留的容量）。</li>
<li><code>X_win ∈ R^{win × d}</code>：观察窗口内的 token embedding，窗口大小为 <code>win</code>，embedding 维度为 <code>d</code>。</li>
<li><code>K_win, V_win</code>：观察窗口内的 key/value cache。</li>
<li><code>K_i, V_i</code>：观察窗口外的 key/value cache（通常是历史 token 对应的 cache）。</li>
</ul>
</li>
<li>
<p><strong>输出</strong></p>
<ul>
<li><code>K̂_i, V̂_i</code>：最终保留的 cache。</li>
</ul>
</li>
</ul>
<hr>
<p><strong>步骤解析</strong></p>
<h4 id="15-行计算观察窗口内的注意力聚合"><strong>1–5 行：计算观察窗口内的注意力聚合</strong><a hidden class="anchor" aria-hidden="true" href="#15-行计算观察窗口内的注意力聚合">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>1: for i ← 1 to h do
</span></span><span style="display:flex;"><span>2:   Q_win_i = X_win W_i^Q
</span></span><span style="display:flex;"><span>3:   Ā_i = softmax(Q_win_i K_i^T)
</span></span><span style="display:flex;"><span>4:   Ā_i = Ā_i.maxpooling(dim=1).mean(dim=0)
</span></span><span style="display:flex;"><span>5: end for
</span></span></code></pre></div><ul>
<li>
<p><strong>目的</strong>：评估每个头 <code>i</code> 对观察窗口外的 KV cache 的重要性。</p>
</li>
<li>
<p><strong>解释</strong>：</p>
<ol>
<li>
<p><code>Q_win_i = X_win W_i^Q</code>
计算第 <code>i</code> 个头的查询矩阵。</p>
</li>
<li>
<p><code>Ā_i = softmax(Q_win_i K_i^T)</code>
计算观察窗口内 token 对每个历史 KV 的注意力分数。</p>
</li>
<li>
<p><code>Ā_i = Ā_i.maxpooling(dim=1).mean(dim=0)</code>
聚合注意力信息：</p>
<ul>
<li><code>maxpooling(dim=1)</code>：取每个观察窗口 token 对历史 KV 中的最大注意力。</li>
<li><code>mean(dim=0)</code>：对观察窗口 token 求平均，得到每个历史 KV 的重要性分数。</li>
</ul>
</li>
</ol>
</li>
<li>
<p><strong>结果</strong>：每个 head 得到一个向量 <code>Ā_i</code>，表示该 head 对各个历史 KV 的注意力重要性。</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Ā_i = Ā_i.maxpooling(dim=1).mean(dim=0)
</span></span></code></pre></div><ul>
<li>
<p><code>Ā_i</code> 是第 <code>i</code> 个注意力头对 <strong>历史 KV cache</strong> 的注意力分数矩阵。</p>
</li>
<li>
<p>形状大概是 <code>(win, num_cache)</code>：</p>
<ul>
<li><code>win</code>：观察窗口内的 token 数量</li>
<li><code>num_cache</code>：历史 KV cache 的数量</li>
</ul>
</li>
</ul>
<p>也就是说，每一行表示一个观察窗口 token 对所有历史 KV 的注意力分布。</p>
<hr>
<h3 id="1-maxpoolingdim1"><strong>1️⃣ maxpooling(dim=1)</strong><a hidden class="anchor" aria-hidden="true" href="#1-maxpoolingdim1">#</a></h3>
<ul>
<li>
<p>对 <strong>每一行</strong>（每个观察窗口 token）进行 <strong>最大池化</strong>：</p>
<ul>
<li>取这个 token 对历史 KV 的 <strong>最大注意力值</strong>。</li>
</ul>
</li>
<li>
<p>结果：</p>
<ul>
<li>原本 <code>(win, num_cache)</code> → <code>(win,)</code> 向量</li>
<li>这个向量表示 <strong>每个 token 最关注的历史 KV 的重要性</strong>。</li>
</ul>
</li>
</ul>
<blockquote>
<p>直观理解：我们只关心每个 token 在历史 KV 中的 “最重要的记忆”，而不是平均分散的注意力。</p>
</blockquote>
<hr>
<h3 id="2-meandim0"><strong>2️⃣ mean(dim=0)</strong><a hidden class="anchor" aria-hidden="true" href="#2-meandim0">#</a></h3>
<ul>
<li>
<p>对 <strong>所有观察窗口 token</strong> 求平均：</p>
<ul>
<li>把每个 token 的最大注意力值加起来再除以 token 数量。</li>
</ul>
</li>
<li>
<p>结果：</p>
<ul>
<li>一个标量或向量（视具体实现），表示 <strong>该注意力头整体对某些历史 KV 的重要性评分</strong>。</li>
</ul>
</li>
<li>
<p>作用：</p>
<ul>
<li>聚合观察窗口内所有 token 的信息，得到一个 <strong>全局的历史 KV 重要性向量</strong>。</li>
</ul>
</li>
</ul>
<hr>
<h4 id="6-行扣除观察窗口-cache-的开销"><strong>6 行：扣除观察窗口 cache 的开销</strong><a hidden class="anchor" aria-hidden="true" href="#6-行扣除观察窗口-cache-的开销">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>6: B = B - winsize × h
</span></span></code></pre></div><ul>
<li><strong>解释</strong>：观察窗口内的 cache <code>K_win, V_win</code> 是 <strong>必须保留的</strong>，所以总预算要减去它们占用的空间。</li>
</ul>
<hr>
<h4 id="78-行预算分配"><strong>7–8 行：预算分配</strong><a hidden class="anchor" aria-hidden="true" href="#78-行预算分配">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>7: 使用 Algorithm 1(B, {Ā_i}) 推导预算分配 {B_i^*}
</span></span><span style="display:flex;"><span>8: {B_i^*} = α × {B_i^*} + (1 − α) × (B/h)
</span></span></code></pre></div><ul>
<li>
<p><strong>步骤 7</strong>：</p>
<ul>
<li>调用另一个算法（Algorithm 1）根据各头的重要性向量 <code>Ā_i</code> 来分配剩余预算 <code>B</code>。</li>
<li>得到每个头分配到的预算 <code>B_i^*</code>（可保留的 KV 数量）。</li>
</ul>
</li>
<li>
<p><strong>步骤 8：安全防护</strong>：</p>
<ul>
<li>用一个平滑系数 <code>α</code> 做线性插值，避免某些头的预算过低。</li>
<li>公式：</li>
</ul>
<p>$$
B_i^* = \alpha \cdot B_i^* + (1 - \alpha) \cdot \frac{B}{h}
$$</p>
<ul>
<li>默认 <code>α=0.2</code>，即 20% 依赖算法分配，80% 保证每个头有基础份额。</li>
</ul>
</li>
</ul>
<hr>
<h4 id="910-行选择保留的-kv"><strong>9–10 行：选择保留的 KV</strong><a hidden class="anchor" aria-hidden="true" href="#910-行选择保留的-kv">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>9: 根据 {B_i^*} 确定 Top-k 驱逐决策 {I_i^*}
</span></span><span style="display:flex;"><span>10: 根据 {I_i^*} 从 {K_i, V_i} 中选择 {K̂_i, V̂_i}
</span></span></code></pre></div><ul>
<li>
<p><strong>步骤 9</strong>：</p>
<ul>
<li>对每个头，选择 <strong>Top-k KV</strong>，其中 <code>k = B_i^*</code>。</li>
<li><code>I_i^*</code> 是被保留的 KV 的索引。</li>
</ul>
</li>
<li>
<p><strong>步骤 10</strong>：</p>
<ul>
<li>根据索引 <code>I_i^*</code> 从历史 cache 中提取对应的 <code>K̂_i, V̂_i</code>。</li>
</ul>
</li>
</ul>
<hr>
<h4 id="11-行与观察窗口-cache-拼接"><strong>11 行：与观察窗口 cache 拼接</strong><a hidden class="anchor" aria-hidden="true" href="#11-行与观察窗口-cache-拼接">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>11: {K̂_i, V̂_i} = Cat({K̂_i, V̂_i}, {K_win, V_win})
</span></span></code></pre></div><h3 id="44-高效实现">4.4 高效实现<a hidden class="anchor" aria-hidden="true" href="#44-高效实现">#</a></h3>
<p>自适应预算分配会导致不同head的cache长度可变，给高效计算带来挑战。Ada-KV 通过以下方式解决：</p>
<ul>
<li><strong>扁平化cache布局 (Flattened Cache Storage Layout)</strong>: 将一个层内所有head的 KV cache拼接成一个大的张量。</li>
<li><strong>可变长度 FlashAttention</strong>: 利用 <code>flash_attn_varlen_func</code> 来高效处理这种可变长度的注意力计算。</li>
<li><strong>自定义 CUDA 内核</strong>: 实现了高效的cache更新操作，以支持在扁平化布局下动态插入新的 KV 对。如 <strong>Figure 2</strong> 所示，更新过程分为 malloc、copy old value 和 insert new value 三个阶段。</li>
<li><strong>GQA 兼容性</strong>: 针对现代 LLM（如 Llama, Mistral）广泛使用的 Grouped Query Attention (GQA)，Ada-KV 通过在每个 GQA 组内使用平均注意力权重作为选择标准，避免了冗余计算。
<img loading="lazy" src="https://jjl357.github.io/blog/image/AdaKV/figure2.png"></li>
</ul>
<h2 id="5-evaluation">5. Evaluation<a hidden class="anchor" aria-hidden="true" href="#5-evaluation">#</a></h2>
<h3 id="51-实验设置">5.1 实验设置<a hidden class="anchor" aria-hidden="true" href="#51-实验设置">#</a></h3>
<ul>
<li><strong>基座模型</strong>: Llama-3.1-8B-Instruct 和 Mistral-7B-Instruct-v0.2（均使用 GQA）。</li>
<li><strong>数据集</strong>: <strong>Ruler</strong> (13个任务) 和 <strong>LongBench</strong> (16个数据集)。</li>
<li><strong>评估场景</strong>:
<ul>
<li><strong>Question-aware</strong>: 压缩时已知问题（标准场景）。</li>
<li><strong>Question-agnostic</strong>: <strong>更具挑战性的场景</strong>，压缩时不知道问题，更能反映真实应用（如提示cache）。</li>
</ul>
</li>
<li><strong>基线方法</strong>: SOTA 的 SnapKV 和 Pyramid，以及滑动窗口方法 StreamingLLM。</li>
<li><strong>参数</strong>: 观察窗口大小为32，安全系数 $\alpha=0.2$。</li>
</ul>
<h3 id="52-ruler-基准测试结果">5.2 Ruler 基准测试结果<a hidden class="anchor" aria-hidden="true" href="#52-ruler-基准测试结果">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/AdaKV/figure3.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/AdaKV/figure4.png"></p>
<ul>
<li><strong>总体表现</strong>: 如 <strong>Figure 3</strong> 所示，Ada-SnapKV 和 Ada-Pyramid <strong>始终优于</strong> 原始方法，尤其在 <strong>question-agnostic</strong> 场景下优势更显著。例如，在 Llama-3.1-8B 上，20% cache预算时，Ada-SnapKV 将 SnapKV 的得分从 44.02 大幅提升至 53.29。</li>
<li><strong>子任务分析</strong>: <strong>Figure 4</strong> 展示了 question-agnostic 场景下的详细结果。Ada-KV 在几乎所有任务上都带来了显著提升。特别在困难的 NIAH 任务（如 S-NIAH-3, MK-NIAH-2）上，Ada-SnapKV 在 80% 预算下几乎实现了无损性能（得分 97.6 和 99.6），而原始 SnapKV 则有明显下降。</li>
</ul>
<h3 id="53-longbench-基准测试结果">5.3 LongBench 基准测试结果<a hidden class="anchor" aria-hidden="true" href="#53-longbench-基准测试结果">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/AdaKV/table1.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/AdaKV/figure5.png">
<img loading="lazy" src="https://jjl357.github.io/blog/image/AdaKV/table2.png"></p>
<ul>
<li><strong>固定预算评估</strong>: <strong>Figure 5</strong> 显示，在 question-aware 场景下，Ada-变体持续优于基线。在 <strong>question-agnostic</strong> 场景下，所有方法性能都有所下降，但 Ada-变体依然保持领先。</li>
<li><strong>按比例预算评估</strong>: <strong>Table 1</strong> 和 <strong>Table 2</strong>（Llama-70B）展示了按任务领域的详细结果。Ada-SnapKV 在绝大多数领域（18个领域中的15个）都有效缓解了压缩带来的性能损失，证明了其通用性和鲁棒性。</li>
</ul>
<h3 id="54-计算效率分析">5.4 计算效率分析<a hidden class="anchor" aria-hidden="true" href="#54-计算效率分析">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/AdaKV/figure6.png">
Ada-KV 的核心目标是在不牺牲效率的前提下提升质量。<strong>Figure 6</strong> 的结果表明，得益于高效的 CUDA 实现和可变长度 FlashAttention，Ada-SnapKV 在<strong>峰值内存占用</strong>和<strong>解码延迟</strong>方面与原始的 SnapKV <strong>基本持平</strong>，并且都远优于使用完整cache的情况。</p>
<h3 id="55-广泛适用性">5.5 广泛适用性<a hidden class="anchor" aria-hidden="true" href="#55-广泛适用性">#</a></h3>
<p><img loading="lazy" src="https://jjl357.github.io/blog/image/AdaKV/table3.png">
由于其即插即用的特性，Ada-KV 已被后续多项工作（如 CriticalKV, DefensiveKV）采用。<strong>Table 3</strong> 显示，将 Ada-KV 应用于这些新方法后，性能得到了进一步提升，再次证明了其作为通用增强模块的价值。</p>
<h2 id="6-conclusion">6. Conclusion<a hidden class="anchor" aria-hidden="true" href="#6-conclusion">#</a></h2>
<p>本研究重新审视了用于高效 LLM 推理的cache驱逐策略，并发现了一个被忽视的关键因素：<strong>跨注意力head的自适应预算分配</strong>。在驱逐损失上界的理论分析指导下，这篇工作提出了 Ada-KV，这是首个用于优化 KV cache驱逐方法的自适应预算分配策略。这篇工作通过将其集成到两个现有的 SOTA 方法中，引入了 Ada-SnapKV 和 Ada-Pyramid。除了常见的问答感知压缩场景，这篇工作还在更具挑战性且较少被探索的问答不可知压缩场景中评估了这些方法。使用 Ruler 和 LongBench 基准，这篇工作证明了自适应预算分配在这两种设置下的有效性。这篇工作的结果不仅暴露了当前cache驱逐策略的局限性，也突显了自适应分配在增强cache驱逐方面的潜力。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jjl357.github.io/blog/tags/kv-cache/">KV Cache</a></li>
      <li><a href="https://jjl357.github.io/blog/tags/paper-note/">Paper Note</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://jjl357.github.io/blog/">JJ&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
