<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Paper Note on JJ&#39;s Blog</title>
    <link>https://jjl357.github.io/blog/tags/paper-note/</link>
    <description>Recent content in Paper Note on JJ&#39;s Blog</description>
    <generator>Hugo -- 0.152.2</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 01 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://jjl357.github.io/blog/tags/paper-note/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference </title>
      <link>https://jjl357.github.io/blog/posts/ada-kv---optimizing-kv-cache-eviction-by-adaptive-budget-allocation-for-efficient-llm-inference---neurips25/</link>
      <pubDate>Sat, 01 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/ada-kv---optimizing-kv-cache-eviction-by-adaptive-budget-allocation-for-efficient-llm-inference---neurips25/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/AdaKV/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference&lt;/strong&gt;: &lt;strong&gt;NeurIPS&#39;25&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Github&lt;/strong&gt;: &lt;a href=&#34;https://github.com/FFY0/AdaKV&#34;&gt;https://github.com/FFY0/AdaKV&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;my-thoughts&#34;&gt;My Thoughts&lt;/h2&gt;
&lt;p&gt;这篇工作的idea在各个head之间adaptive 分配 budget, 直觉上就能知道这种设计是有效的，实验也验证了这一点。&lt;/p&gt;
&lt;p&gt;比较惊喜的是作者实现了With efficient CUDA kernel implementations来解决variable-sized cache elements across attention heads，从而来真正实现了计算加速，以及其他KV Cache工作在也实现了对AdaKV集成，再次证明了其作为通用增强模块的价值。&lt;/p&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;p&gt;大型语言模型（LLM）在各个领域表现出色，但由于长序列推理所需的不断增长的键值（KV）cache，面临着效率挑战。LLM 的广泛应用推动了其处理扩展序列能力的发展。例如，GPT 支持长达 128K 的序列，Claude3 支持 200K，Gemini-Pro-1.5 甚至支持高达 2M 个 token。然而，这种 token 长度的增长带来了显著的挑战，尤其是在推理过程中cache大小的急剧膨胀。对于一个 8B 的 LLM，处理一个 2M token 的序列可能需要高达 256GB 的cache，这严重影响了 GPU 内存效率和计算运行时效率。&lt;/p&gt;
&lt;p&gt;现有的 KV cache驱逐方法通常在所有注意力head上&lt;strong&gt;均匀分配&lt;/strong&gt;压缩预算，忽略了每个head独特的注意力模式。如 &lt;strong&gt;Figure 1a&lt;/strong&gt; 所示，不同head的注意力集中度（concentration）差异巨大：一些head（sparse heads）的注意力高度集中在少数几个 token 上，而另一些head（dispersed heads）的注意力则分布得更广。这种均匀分配导致了效率低下——要么在稀疏集中的head上浪费cache预算，要么在分散分布的head上造成显著的驱逐损失。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/AdaKV/figure1.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-relative-work&#34;&gt;2. Relative Work&lt;/h2&gt;
&lt;h3 id=&#34;21-cache-eviction-methods&#34;&gt;2.1 Cache Eviction Methods&lt;/h3&gt;
&lt;p&gt;cache驱逐方法主要分为两类：滑动窗口驱逐和 Top-k 驱逐。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;滑动窗口方法&lt;/strong&gt;（如 StreamingLLM）简单地保留初始cache元素和滑动窗口内的元素，但这种无差别的驱逐会显著降低生成质量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Top-k 驱逐方法&lt;/strong&gt;（如 H2O, SnapKV, Pyramid）基于注意力权重识别并保留 &lt;code&gt;k&lt;/code&gt; 个关键cache元素。然而，现有 Top-k 方法通常在不同head上&lt;strong&gt;均匀分配&lt;/strong&gt;总预算。Ada-KV 通过&lt;strong&gt;自适应预算分配&lt;/strong&gt;来增强这些方法。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;22-sparse-attention-methods&#34;&gt;2.2 Sparse Attention Methods&lt;/h3&gt;
&lt;p&gt;稀疏注意力方法与 KV cache驱逐在根本上不同：前者保留所有cache，但在计算时只选择性地使用关键子集，因此&lt;strong&gt;不减少内存占用&lt;/strong&gt;。而 KV cache驱逐直接移除非关键条目，从而减小内存占用。这两种技术是正orthogonal的，未来可以结合使用。&lt;/p&gt;</description>
    </item>
    <item>
      <title>NOT ALL HEADS MATTER: A HEAD-LEVEL KV CACHE COMPRESSION METHOD WITH INTEGRATED RETRIEVAL AND REASONING</title>
      <link>https://jjl357.github.io/blog/posts/not-all-heads-matter---a-head-level-kv-cache-compression-method-with-integrated-retrieval-and-reasoning---iclr25/</link>
      <pubDate>Fri, 31 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/not-all-heads-matter---a-head-level-kv-cache-compression-method-with-integrated-retrieval-and-reasoning---iclr25/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/HeadKV/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; ICLR&#39;25&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/FYYFU/HeadKV&#34;&gt;https://github.com/FYYFU/HeadKV&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;my-thoughts&#34;&gt;My Thoughts&lt;/h2&gt;
&lt;p&gt;这篇工作和 DuoAttention 的关注点类似，都是关注不同 attention head 对模型不同能力的贡献不同，这篇工作更关注 attention head 对模型 &lt;strong&gt;Retrieval&lt;/strong&gt; 与 &lt;strong&gt;Reasoning&lt;/strong&gt; 的 importance score, 来实现 KV Cache 的 nonuniform budget allocation 。&lt;/p&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;p&gt;现代 LLM 越来越支持极长上下文（例如 GPT-4、Llama-3、Qwen-2、Claude 等），但随着输入长度增长，Transformer 的 self-attention 导致 KV cache（attention 的 key/value 状态）占用内存线性增长，成为推理阶段的主要瓶颈。已有工作通过 token eviction / 层级缓存压缩来缓解，但&lt;strong&gt;几乎没有研究在“头（head）级别”上对 KV cache 大小进行差异化分配&lt;/strong&gt;。作者观察到 attention heads 在功能上高度异质（如 retrieval heads、reasoning heads 等），因此提出基于头重要性的 head-level KV cache 压缩方法（HeadKV），并在此基础上提出结合检索与推理能力评估的 HeadKV-R2。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-relative-work&#34;&gt;2. Relative Work&lt;/h2&gt;
&lt;h3 id=&#34;21-attention-heads&#34;&gt;2.1 Attention heads&lt;/h3&gt;
&lt;p&gt;回顾了对多头注意力中 head 功能的研究（Voita et al., Olsson et al., Wu et al., Zheng et al. 等），并指出不同 head 在词法、结构、复制（induction）、检索等方面扮演不同角色。这些观察为按 head 分配 KV cache 提供理论基础。&lt;/p&gt;</description>
    </item>
    <item>
      <title>QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models</title>
      <link>https://jjl357.github.io/blog/posts/qsvd----efficient-low-rank-approximation-for-unified-query-key-value-weight-compression-in-low-precision-vision-language-models---neurips25-spotlight/</link>
      <pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/qsvd----efficient-low-rank-approximation-for-unified-query-key-value-weight-compression-in-low-precision-vision-language-models---neurips25-spotlight/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/QSVD/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; NeurIPS&#39;25 Spotlight&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/SAI-Lab-NYU/QSVD&#34;&gt;https://github.com/SAI-Lab-NYU/QSVD&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;p&gt;Vision–Language Models (VLMs) 如 LLaVA、BLIP2 等在图像描述、视觉问答 (VQA) 等任务中表现卓越，但这些模型需要极大的计算与存储开销，尤其在推理时：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;KV Cache 占用高&lt;/strong&gt;：注意力机制中需存储 Key、Value，每层缓存大小随序列长度线性增长。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q/K/V 投影重复计算&lt;/strong&gt;：三组权重矩阵独立计算，造成算力浪费。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型量化困难&lt;/strong&gt;：激活分布存在极端 outliers，难以稳定进行低比特量化。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;QSVD 的目标是&lt;strong&gt;统一地对 Q/K/V 权重矩阵进行低秩近似&lt;/strong&gt;并结合&lt;strong&gt;后训练量化 (PTQ)&lt;/strong&gt;，实现以下三点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;减少参数量、计算量、缓存占用；&lt;/li&gt;
&lt;li&gt;保持模型性能；&lt;/li&gt;
&lt;li&gt;支持低精度硬件部署。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/QSVD/figure1.png&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-related-work&#34;&gt;2. Related Work&lt;/h2&gt;
&lt;h3 id=&#34;21-svd-in-large-models&#34;&gt;2.1 SVD in Large Models&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Singular Value Decomposition (SVD)&lt;/strong&gt; 是经典的矩阵分解方法。
对于矩阵 ( W \in \mathbb{R}^{m \times n} )，可分解为：&lt;/p&gt;
&lt;p&gt;$$
W = U \Sigma V^T
$$&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(U, V) 为正交矩阵；&lt;/li&gt;
&lt;li&gt;(\Sigma) 为奇异值对角矩阵；&lt;/li&gt;
&lt;li&gt;保留前 (r) 个奇异值可得到 rank-(r) 近似：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
W \approx U_r \Sigma_r V_r^T
$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning</title>
      <link>https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/</link>
      <pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/SpecVLM/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; &lt;strong&gt;EMNLP&#39;25&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/zju-jiyicheng/SpecVLM&#34;&gt;https://github.com/zju-jiyicheng/SpecVLM&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;p&gt;Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.&lt;/p&gt;
&lt;p&gt;例如：
LLaVA-OneVision (Li et al., 2024a) 将每一帧处理为 196 个视觉 token。若视频为两分钟、60 FPS，则总 token 数量超过 100 万。
如此大量的 video tokens 导致：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;序列长度急剧增加；&lt;/li&gt;
&lt;li&gt;Prefill 阶段的 attention 开销呈平方级增长；&lt;/li&gt;
&lt;li&gt;Decoding 阶段 KV cache 急速膨胀，成为显著的 GPU 内存瓶颈。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 autoregressive 生成过程中，每步生成的 KV cache 都必须与模型参数一起加载与存储于 GPU 显存，导致显著的 memory-bound 现象。&lt;/p&gt;</description>
    </item>
    <item>
      <title>AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</title>
      <link>https://jjl357.github.io/blog/posts/adaspec---selective-knowledge-distillation-for-efficient-speculative-decoders---neurips25-spotlight/</link>
      <pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/adaspec---selective-knowledge-distillation-for-efficient-speculative-decoders---neurips25-spotlight/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/AdaSpec/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; &lt;strong&gt;NeurIPS&#39;25 Spotlight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/yuezhouhu/adaspec&#34;&gt;https://github.com/yuezhouhu/adaspec&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;my-thoughts&#34;&gt;My Thoughts&lt;/h2&gt;
&lt;p&gt;这篇论文的 methods 挺简明的，感觉可以有个新 idea:&lt;br&gt;
&lt;strong&gt;将 MoSD 和 AdaSPEC 结合起来&lt;/strong&gt; —— 针对不同难度的 tokens distill 出不同的 draft models，利用 router 将不同难度的 tokens 选择最适合的对应 draft model 来进行 SD。&lt;/p&gt;
&lt;p&gt;问题在于：对于论文中提到的“hard tokens”，是否能 distill 出一个合适且有用的 draft model？&lt;br&gt;
论文结果显示当前方法对这些 token 效果较差（甚至比 reference model 还差），  因此需要新的方式来改进这一部分的蒸馏与利用机制。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;p&gt;Speculative Decoding (SD) accelerates large language model inference by employing a small &lt;strong&gt;draft model&lt;/strong&gt; to generate predictions, which are then verified by a larger &lt;strong&gt;target model&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>R-KV: Redundancy-aware KV Cache Compression for Reasoning Models</title>
      <link>https://jjl357.github.io/blog/posts/r-kv---redundancy-aware-kv-cache-compression-for-reasoning-models---neurips25/</link>
      <pubDate>Sat, 25 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/r-kv---redundancy-aware-kv-cache-compression-for-reasoning-models---neurips25/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/RKV/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; NeurIPS&#39;25&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/Zefan-Cai/R-KV&#34;&gt;https://github.com/Zefan-Cai/R-KV&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;my-thoughts&#34;&gt;My Thoughts&lt;/h2&gt;
&lt;p&gt;R-KV 针对previous works在efficient reasoning中由于只依靠attention scores而造成过多地保留了redundant tokens(重复而且对推理没有信息增益的token),R-KV的重点我觉得是增加了Redundancy Estimation via Semantic Similarity来解决这个问题,就Evaluation的结果来说，R-KV的效果是非常好的，即提升了Throughput还maintain了performance，甚至在budget充足的情况下还可以做到提点。&lt;/p&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;p&gt;Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning.
However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference.&lt;/p&gt;
&lt;p&gt;For instance, a &lt;strong&gt;DeepSeek-R1-Distill-Llama-8B&lt;/strong&gt; model may generate &lt;strong&gt;32K tokens&lt;/strong&gt; to solve a complex math problem, consuming &lt;strong&gt;15.5GB&lt;/strong&gt; of memory to load model weights and &lt;strong&gt;4.1GB&lt;/strong&gt; to store the KV cache.
This long CoT (chain-of-thought) generation necessitates the development of &lt;strong&gt;KV cache compression&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs</title>
      <link>https://jjl357.github.io/blog/posts/cdpruner---beyond-attention-or-similarity---maximizing-conditional-diversity-for-token-pruning-in-mllms-neuirips25/</link>
      <pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/cdpruner---beyond-attention-or-similarity---maximizing-conditional-diversity-for-token-pruning-in-mllms-neuirips25/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference&lt;/strong&gt;: &lt;strong&gt;NeurIPS&#39;25&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;github&lt;/strong&gt;: &lt;a href=&#34;https://github.com/Theia-4869/CDPruner&#34;&gt;https://github.com/Theia-4869/CDPruner&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;my-thoughts&#34;&gt;My Thoughts&lt;/h2&gt;
&lt;p&gt;相较于 Attention-based methods 中的 &lt;strong&gt;attention shift&lt;/strong&gt; 问题和 Similarity-based methods 中忽略了 query 的问题，这篇论文从 &lt;strong&gt;增加 visual tokens 全局多样性（同时保持对 query 的关注）&lt;/strong&gt; 的角度出发，提出了 &lt;strong&gt;CDPruner&lt;/strong&gt;，通过将 token 多样性建模为 DPP（Determinantal Point Process）求解问题，实现了 SOTA 的 pruning 效果。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;motivations&#34;&gt;Motivations&lt;/h2&gt;
&lt;p&gt;在多模态大语言模型（MLLMs）中，视觉 token 的输入长度往往远大于文本 token，从而带来高昂的推理开销。例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaVA-1.5 将一张 336×336 图像转换为 &lt;strong&gt;576 tokens&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;LLaVA-NeXT 的高分辨率版本在输入加倍的情况下生成 &lt;strong&gt;2,880 tokens&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;LongVA 处理 2,000 帧视频时生成超过 &lt;strong&gt;200K visual tokens&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;LongVILA 能处理 &lt;strong&gt;6,000 帧&lt;/strong&gt;并产生 &lt;strong&gt;超过 1M visual tokens&lt;/strong&gt;，导致巨大的计算成本。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/figure1.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;现有的视觉 token 剪枝方法主要分为两类：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cache-to-Cache: Direct Semantic Communication Between Large Language Models</title>
      <link>https://jjl357.github.io/blog/posts/cache-to-cache---direct-semantic-communication-between-large-language-models/</link>
      <pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/cache-to-cache---direct-semantic-communication-between-large-language-models/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/C2C/title.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;github：&lt;a href=&#34;https://github.com/thu-nics/C2C&#34;&gt;https://github.com/thu-nics/C2C&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-motivation动机--背景&#34;&gt;💡 Motivation（动机 / 背景）&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/C2C/figure1.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;现状：多-LLM 系统通常通过 &lt;strong&gt;文本（Text-to-Text, T2T）&lt;/strong&gt; 相互通信 —— 一个模型生成文本，另一个模型再读入文本并处理。这种方式存在的信息瓶颈（高维内部表示被压成文本）、歧义性（自然语言本身的模糊性）以及 &lt;strong&gt;串行解码带来的延迟&lt;/strong&gt; 等问题。&lt;/li&gt;
&lt;li&gt;问题：文本通信无法完整保留模型内部的高维语义信息（例如 KV-Cache 中的 rich semantics），同时文本通信需要逐 token 解码，增加通信与推理延迟。&lt;/li&gt;
&lt;li&gt;提出的问题（核心研究问）：&lt;strong&gt;LLM之间能否“超越文本”直接通信？能否通过共享/转换/融合 KV-Cache（key/value cache）来实现更丰富、更低延迟的语义通信？&lt;/strong&gt;（文中以 “Can LLMs communicate beyond text?” 作核心驱动。）&lt;/li&gt;
&lt;li&gt;主要观察驱动：作者的 Oracle 实验显示
(1) 在不扩大序列长度的前提下，丰富 KV-Cache 可以提升回答质量；
(2) 不同模型的 KV-Cache 在表示空间上是可转换/可对齐的（经过训练的简单 MLP 可将一个模型的 KV 映射到另一个模型空间）。
这些观察支持用 KV-Cache 作为通信媒介的可行性。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-challenges挑战&#34;&gt;⚔ Challenges（挑战）&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/C2C/figure2.png&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;跨模型的 KV-Cache 表示差异&lt;/strong&gt;：不同模型（不同族、不同规模、不同 tokenizer）在同一输入上产生的 KV-Cache 分布差异显著（t-SNE 可视化）。如何可靠地将一个模型的 cache “投影”到另一个模型的语义空间并被有效利用是挑战。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对齐问题（token-level &amp;amp; layer-level）&lt;/strong&gt;：不同 tokenizer 产生不同 token 划分，且模型层数不同，必须处理 token 对齐和层对齐问题（避免信息丢失或错位注入）。文中提出了 token 解码再 re-encode 的对齐策略和 “terminal alignment” 层对齐策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;避免破坏接收者原有语义&lt;/strong&gt;：直接用别人 cache 覆盖会破坏接收模型已有语义/结构，需设计残差式、可控的融合机制（即 Fuser 的设计）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;选择性注入（哪些层注入、注入多少）&lt;/strong&gt;：并非所有层注入都有利，单层/多层注入效果差异明显（Appendix 的单层实验显示有层增益也有层下降），因此需要 learnable gate 来选择注入位置与比例。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效率 / 延迟权衡&lt;/strong&gt;：虽然 C2C 目标是降低延迟，但在实现上要保证投影/融合本身不会引入比文本通信更高的开销（设计轻量 Fuser 并冻结主模型参数以降低训练/推理成本）。&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-observations--analysis&#34;&gt;🔍 Observations / Analysis&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/C2C/f34t12.png&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
