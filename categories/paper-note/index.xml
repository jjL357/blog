<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Paper Note on JJ&#39;s Blog</title>
    <link>https://jjl357.github.io/blog/categories/paper-note/</link>
    <description>Recent content in Paper Note on JJ&#39;s Blog</description>
    <generator>Hugo -- 0.152.2</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 30 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://jjl357.github.io/blog/categories/paper-note/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models</title>
      <link>https://jjl357.github.io/blog/posts/qsvd----efficient-low-rank-approximation-for-unified-query-key-value-weight-compression-in-low-precision-vision-language-models---neurips25-spotlight/</link>
      <pubDate>Thu, 30 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/qsvd----efficient-low-rank-approximation-for-unified-query-key-value-weight-compression-in-low-precision-vision-language-models---neurips25-spotlight/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/QSVD/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; NeurIPS&#39;25 Spotlight
&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/SAI-Lab-NYU/QSVD&#34;&gt;https://github.com/SAI-Lab-NYU/QSVD&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;p&gt;Visionâ€“Language Models (VLMs) å¦‚ LLaVAã€BLIP2 ç­‰åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­” (VQA) ç­‰ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†è¿™äº›æ¨¡å‹éœ€è¦æå¤§çš„è®¡ç®—ä¸å­˜å‚¨å¼€é”€ï¼Œå°¤å…¶åœ¨æ¨ç†æ—¶ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;KV Cache å ç”¨é«˜&lt;/strong&gt;ï¼šæ³¨æ„åŠ›æœºåˆ¶ä¸­éœ€å­˜å‚¨ Keyã€Valueï¼Œæ¯å±‚ç¼“å­˜å¤§å°éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q/K/V æŠ•å½±é‡å¤è®¡ç®—&lt;/strong&gt;ï¼šä¸‰ç»„æƒé‡çŸ©é˜µç‹¬ç«‹è®¡ç®—ï¼Œé€ æˆç®—åŠ›æµªè´¹ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;æ¨¡å‹é‡åŒ–å›°éš¾&lt;/strong&gt;ï¼šæ¿€æ´»åˆ†å¸ƒå­˜åœ¨æç«¯ outliersï¼Œéš¾ä»¥ç¨³å®šè¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;QSVD çš„ç›®æ ‡æ˜¯&lt;strong&gt;ç»Ÿä¸€åœ°å¯¹ Q/K/V æƒé‡çŸ©é˜µè¿›è¡Œä½ç§©è¿‘ä¼¼&lt;/strong&gt;å¹¶ç»“åˆ&lt;strong&gt;åè®­ç»ƒé‡åŒ– (PTQ)&lt;/strong&gt;ï¼Œå®ç°ä»¥ä¸‹ä¸‰ç‚¹ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å‡å°‘å‚æ•°é‡ã€è®¡ç®—é‡ã€ç¼“å­˜å ç”¨ï¼›&lt;/li&gt;
&lt;li&gt;ä¿æŒæ¨¡å‹æ€§èƒ½ï¼›&lt;/li&gt;
&lt;li&gt;æ”¯æŒä½ç²¾åº¦ç¡¬ä»¶éƒ¨ç½²ã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/QSVD/figure1.png&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-related-work&#34;&gt;2. Related Work&lt;/h2&gt;
&lt;h3 id=&#34;21-svd-in-large-models&#34;&gt;2.1 SVD in Large Models&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Singular Value Decomposition (SVD)&lt;/strong&gt; æ˜¯ç»å…¸çš„çŸ©é˜µåˆ†è§£æ–¹æ³•ã€‚
å¯¹äºçŸ©é˜µ ( W \in \mathbb{R}^{m \times n} )ï¼Œå¯åˆ†è§£ä¸ºï¼š&lt;/p&gt;
&lt;p&gt;$$
W = U \Sigma V^T
$$&lt;/p&gt;
&lt;p&gt;å…¶ä¸­ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(U, V) ä¸ºæ­£äº¤çŸ©é˜µï¼›&lt;/li&gt;
&lt;li&gt;(\Sigma) ä¸ºå¥‡å¼‚å€¼å¯¹è§’çŸ©é˜µï¼›&lt;/li&gt;
&lt;li&gt;ä¿ç•™å‰ (r) ä¸ªå¥‡å¼‚å€¼å¯å¾—åˆ° rank-(r) è¿‘ä¼¼ï¼š&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
W \approx U_r \Sigma_r V_r^T
$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>SPECVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning</title>
      <link>https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/</link>
      <pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/specvlm---enhancingspeculative-decoding-of-video-llms-via-verifier-guided-token-pruning---emnlp25/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/SpecVLM/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; &lt;strong&gt;EMNLP&#39;25&lt;/strong&gt;
&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/zju-jiyicheng/SpecVLM&#34;&gt;https://github.com/zju-jiyicheng/SpecVLM&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;p&gt;Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding.&lt;/p&gt;
&lt;p&gt;ä¾‹å¦‚ï¼š
LLaVA-OneVision (Li et al., 2024a) å°†æ¯ä¸€å¸§å¤„ç†ä¸º 196 ä¸ªè§†è§‰ tokenã€‚è‹¥è§†é¢‘ä¸ºä¸¤åˆ†é’Ÿã€60 FPSï¼Œåˆ™æ€» token æ•°é‡è¶…è¿‡ 100 ä¸‡ã€‚
å¦‚æ­¤å¤§é‡çš„ video tokens å¯¼è‡´ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åºåˆ—é•¿åº¦æ€¥å‰§å¢åŠ ï¼›&lt;/li&gt;
&lt;li&gt;Prefill é˜¶æ®µçš„ attention å¼€é”€å‘ˆå¹³æ–¹çº§å¢é•¿ï¼›&lt;/li&gt;
&lt;li&gt;Decoding é˜¶æ®µ KV cache æ€¥é€Ÿè†¨èƒ€ï¼Œæˆä¸ºæ˜¾è‘—çš„ GPU å†…å­˜ç“¶é¢ˆã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;åœ¨ autoregressive ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯æ­¥ç”Ÿæˆçš„ KV cache éƒ½å¿…é¡»ä¸æ¨¡å‹å‚æ•°ä¸€èµ·åŠ è½½ä¸å­˜å‚¨äº GPU æ˜¾å­˜ï¼Œå¯¼è‡´æ˜¾è‘—çš„ memory-bound ç°è±¡ã€‚&lt;/p&gt;</description>
    </item>
    <item>
      <title>AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</title>
      <link>https://jjl357.github.io/blog/posts/adaspec---selective-knowledge-distillation-for-efficient-speculative-decoders---neurips25-spotlight/</link>
      <pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/adaspec---selective-knowledge-distillation-for-efficient-speculative-decoders---neurips25-spotlight/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/AdaSpec/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; &lt;strong&gt;NeurIPS&#39;25 Spotlight&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/yuezhouhu/adaspec&#34;&gt;https://github.com/yuezhouhu/adaspec&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;my-thoughts&#34;&gt;My Thoughts&lt;/h2&gt;
&lt;p&gt;è¿™ç¯‡è®ºæ–‡çš„ methods æŒºç®€æ˜çš„ï¼Œæ„Ÿè§‰å¯ä»¥æœ‰ä¸ªæ–° idea:&lt;br&gt;
&lt;strong&gt;å°† MoSD å’Œ AdaSPEC ç»“åˆèµ·æ¥&lt;/strong&gt; â€”â€” é’ˆå¯¹ä¸åŒéš¾åº¦çš„ tokens distill å‡ºä¸åŒçš„ draft modelsï¼Œåˆ©ç”¨ router å°†ä¸åŒéš¾åº¦çš„ tokens é€‰æ‹©æœ€é€‚åˆçš„å¯¹åº” draft model æ¥è¿›è¡Œ SDã€‚&lt;/p&gt;
&lt;p&gt;é—®é¢˜åœ¨äºï¼šå¯¹äºè®ºæ–‡ä¸­æåˆ°çš„â€œhard tokensâ€ï¼Œæ˜¯å¦èƒ½ distill å‡ºä¸€ä¸ªåˆé€‚ä¸”æœ‰ç”¨çš„ draft modelï¼Ÿ&lt;br&gt;
è®ºæ–‡ç»“æœæ˜¾ç¤ºå½“å‰æ–¹æ³•å¯¹è¿™äº› token æ•ˆæœè¾ƒå·®ï¼ˆç”šè‡³æ¯” reference model è¿˜å·®ï¼‰ï¼Œ  å› æ­¤éœ€è¦æ–°çš„æ–¹å¼æ¥æ”¹è¿›è¿™ä¸€éƒ¨åˆ†çš„è’¸é¦ä¸åˆ©ç”¨æœºåˆ¶ã€‚&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;p&gt;Speculative Decoding (SD) accelerates large language model inference by employing a small &lt;strong&gt;draft model&lt;/strong&gt; to generate predictions, which are then verified by a larger &lt;strong&gt;target model&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>R-KV: Redundancy-aware KV Cache Compression for Reasoning Models</title>
      <link>https://jjl357.github.io/blog/posts/r-kv---redundancy-aware-kv-cache-compression-for-reasoning-models---neurips25/</link>
      <pubDate>Sat, 25 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/r-kv---redundancy-aware-kv-cache-compression-for-reasoning-models---neurips25/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/RKV/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; NeurIPS&#39;25
&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/Zefan-Cai/R-KV&#34;&gt;https://github.com/Zefan-Cai/R-KV&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;my-thoughts&#34;&gt;My Thoughts&lt;/h2&gt;
&lt;p&gt;R-KV é’ˆå¯¹previous worksåœ¨efficient reasoningä¸­ç”±äºåªä¾é attention scoresè€Œé€ æˆè¿‡å¤šåœ°ä¿ç•™äº†redundant tokens(é‡å¤è€Œä¸”å¯¹æ¨ç†æ²¡æœ‰ä¿¡æ¯å¢ç›Šçš„token),R-KVçš„é‡ç‚¹æˆ‘è§‰å¾—æ˜¯å¢åŠ äº†Redundancy Estimation via Semantic Similarityæ¥è§£å†³è¿™ä¸ªé—®é¢˜,å°±Evaluationçš„ç»“æœæ¥è¯´ï¼ŒR-KVçš„æ•ˆæœæ˜¯éå¸¸å¥½çš„ï¼Œå³æå‡äº†Throughputè¿˜maintainäº†performanceï¼Œç”šè‡³åœ¨budgetå……è¶³çš„æƒ…å†µä¸‹è¿˜å¯ä»¥åšåˆ°æç‚¹ã€‚&lt;/p&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;p&gt;Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning.
However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference.&lt;/p&gt;
&lt;p&gt;For instance, a &lt;strong&gt;DeepSeek-R1-Distill-Llama-8B&lt;/strong&gt; model may generate &lt;strong&gt;32K tokens&lt;/strong&gt; to solve a complex math problem, consuming &lt;strong&gt;15.5GB&lt;/strong&gt; of memory to load model weights and &lt;strong&gt;4.1GB&lt;/strong&gt; to store the KV cache.
This long CoT (chain-of-thought) generation necessitates the development of &lt;strong&gt;KV cache compression&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs</title>
      <link>https://jjl357.github.io/blog/posts/cdpruner---beyond-attention-or-similarity---maximizing-conditional-diversity-for-token-pruning-in-mllms-neuirips25/</link>
      <pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/cdpruner---beyond-attention-or-similarity---maximizing-conditional-diversity-for-token-pruning-in-mllms-neuirips25/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference&lt;/strong&gt;: &lt;strong&gt;NeurIPS&#39;25&lt;/strong&gt;
&lt;strong&gt;github&lt;/strong&gt;: &lt;a href=&#34;https://github.com/Theia-4869/CDPruner&#34;&gt;https://github.com/Theia-4869/CDPruner&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;my-thoughts&#34;&gt;My Thoughts&lt;/h2&gt;
&lt;p&gt;ç›¸è¾ƒäº Attention-based methods ä¸­çš„ &lt;strong&gt;attention shift&lt;/strong&gt; é—®é¢˜å’Œ Similarity-based methods ä¸­å¿½ç•¥äº† query çš„é—®é¢˜ï¼Œè¿™ç¯‡è®ºæ–‡ä» &lt;strong&gt;å¢åŠ  visual tokens å…¨å±€å¤šæ ·æ€§ï¼ˆåŒæ—¶ä¿æŒå¯¹ query çš„å…³æ³¨ï¼‰&lt;/strong&gt; çš„è§’åº¦å‡ºå‘ï¼Œæå‡ºäº† &lt;strong&gt;CDPruner&lt;/strong&gt;ï¼Œé€šè¿‡å°† token å¤šæ ·æ€§å»ºæ¨¡ä¸º DPPï¼ˆDeterminantal Point Processï¼‰æ±‚è§£é—®é¢˜ï¼Œå®ç°äº† SOTA çš„ pruning æ•ˆæœã€‚&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;motivations&#34;&gt;Motivations&lt;/h2&gt;
&lt;p&gt;åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ï¼Œè§†è§‰ token çš„è¾“å…¥é•¿åº¦å¾€å¾€è¿œå¤§äºæ–‡æœ¬ tokenï¼Œä»è€Œå¸¦æ¥é«˜æ˜‚çš„æ¨ç†å¼€é”€ã€‚ä¾‹å¦‚ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaVA-1.5 å°†ä¸€å¼  336Ã—336 å›¾åƒè½¬æ¢ä¸º &lt;strong&gt;576 tokens&lt;/strong&gt;ï¼›&lt;/li&gt;
&lt;li&gt;LLaVA-NeXT çš„é«˜åˆ†è¾¨ç‡ç‰ˆæœ¬åœ¨è¾“å…¥åŠ å€çš„æƒ…å†µä¸‹ç”Ÿæˆ &lt;strong&gt;2,880 tokens&lt;/strong&gt;ï¼›&lt;/li&gt;
&lt;li&gt;LongVA å¤„ç† 2,000 å¸§è§†é¢‘æ—¶ç”Ÿæˆè¶…è¿‡ &lt;strong&gt;200K visual tokens&lt;/strong&gt;ï¼›&lt;/li&gt;
&lt;li&gt;LongVILA èƒ½å¤„ç† &lt;strong&gt;6,000 å¸§&lt;/strong&gt;å¹¶äº§ç”Ÿ &lt;strong&gt;è¶…è¿‡ 1M visual tokens&lt;/strong&gt;ï¼Œå¯¼è‡´å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/CDPruner%20-%20Beyond%20Attention%20or%20Similarity%20-/figure1.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;ç°æœ‰çš„è§†è§‰ token å‰ªææ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼š&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cache-to-Cache: Direct Semantic Communication Between Large Language Models</title>
      <link>https://jjl357.github.io/blog/posts/cache-to-cache---direct-semantic-communication-between-large-language-models/</link>
      <pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/cache-to-cache---direct-semantic-communication-between-large-language-models/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/C2C/title.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;githubï¼š&lt;a href=&#34;https://github.com/thu-nics/C2C&#34;&gt;https://github.com/thu-nics/C2C&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-motivationåŠ¨æœº--èƒŒæ™¯&#34;&gt;ğŸ’¡ Motivationï¼ˆåŠ¨æœº / èƒŒæ™¯ï¼‰&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/C2C/figure1.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç°çŠ¶ï¼šå¤š-LLM ç³»ç»Ÿé€šå¸¸é€šè¿‡ &lt;strong&gt;æ–‡æœ¬ï¼ˆText-to-Text, T2Tï¼‰&lt;/strong&gt; ç›¸äº’é€šä¿¡ â€”â€” ä¸€ä¸ªæ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼Œå¦ä¸€ä¸ªæ¨¡å‹å†è¯»å…¥æ–‡æœ¬å¹¶å¤„ç†ã€‚è¿™ç§æ–¹å¼å­˜åœ¨çš„ä¿¡æ¯ç“¶é¢ˆï¼ˆé«˜ç»´å†…éƒ¨è¡¨ç¤ºè¢«å‹æˆæ–‡æœ¬ï¼‰ã€æ­§ä¹‰æ€§ï¼ˆè‡ªç„¶è¯­è¨€æœ¬èº«çš„æ¨¡ç³Šæ€§ï¼‰ä»¥åŠ &lt;strong&gt;ä¸²è¡Œè§£ç å¸¦æ¥çš„å»¶è¿Ÿ&lt;/strong&gt; ç­‰é—®é¢˜ã€‚&lt;/li&gt;
&lt;li&gt;é—®é¢˜ï¼šæ–‡æœ¬é€šä¿¡æ— æ³•å®Œæ•´ä¿ç•™æ¨¡å‹å†…éƒ¨çš„é«˜ç»´è¯­ä¹‰ä¿¡æ¯ï¼ˆä¾‹å¦‚ KV-Cache ä¸­çš„ rich semanticsï¼‰ï¼ŒåŒæ—¶æ–‡æœ¬é€šä¿¡éœ€è¦é€ token è§£ç ï¼Œå¢åŠ é€šä¿¡ä¸æ¨ç†å»¶è¿Ÿã€‚&lt;/li&gt;
&lt;li&gt;æå‡ºçš„é—®é¢˜ï¼ˆæ ¸å¿ƒç ”ç©¶é—®ï¼‰ï¼š&lt;strong&gt;LLMä¹‹é—´èƒ½å¦â€œè¶…è¶Šæ–‡æœ¬â€ç›´æ¥é€šä¿¡ï¼Ÿèƒ½å¦é€šè¿‡å…±äº«/è½¬æ¢/èåˆ KV-Cacheï¼ˆkey/value cacheï¼‰æ¥å®ç°æ›´ä¸°å¯Œã€æ›´ä½å»¶è¿Ÿçš„è¯­ä¹‰é€šä¿¡ï¼Ÿ&lt;/strong&gt;ï¼ˆæ–‡ä¸­ä»¥ â€œCan LLMs communicate beyond text?â€ ä½œæ ¸å¿ƒé©±åŠ¨ã€‚ï¼‰&lt;/li&gt;
&lt;li&gt;ä¸»è¦è§‚å¯Ÿé©±åŠ¨ï¼šä½œè€…çš„ Oracle å®éªŒæ˜¾ç¤º
(1) åœ¨ä¸æ‰©å¤§åºåˆ—é•¿åº¦çš„å‰æä¸‹ï¼Œä¸°å¯Œ KV-Cache å¯ä»¥æå‡å›ç­”è´¨é‡ï¼›
(2) ä¸åŒæ¨¡å‹çš„ KV-Cache åœ¨è¡¨ç¤ºç©ºé—´ä¸Šæ˜¯å¯è½¬æ¢/å¯å¯¹é½çš„ï¼ˆç»è¿‡è®­ç»ƒçš„ç®€å• MLP å¯å°†ä¸€ä¸ªæ¨¡å‹çš„ KV æ˜ å°„åˆ°å¦ä¸€ä¸ªæ¨¡å‹ç©ºé—´ï¼‰ã€‚
è¿™äº›è§‚å¯Ÿæ”¯æŒç”¨ KV-Cache ä½œä¸ºé€šä¿¡åª’ä»‹çš„å¯è¡Œæ€§ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-challengesæŒ‘æˆ˜&#34;&gt;âš” Challengesï¼ˆæŒ‘æˆ˜ï¼‰&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/C2C/figure2.png&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;è·¨æ¨¡å‹çš„ KV-Cache è¡¨ç¤ºå·®å¼‚&lt;/strong&gt;ï¼šä¸åŒæ¨¡å‹ï¼ˆä¸åŒæ—ã€ä¸åŒè§„æ¨¡ã€ä¸åŒ tokenizerï¼‰åœ¨åŒä¸€è¾“å…¥ä¸Šäº§ç”Ÿçš„ KV-Cache åˆ†å¸ƒå·®å¼‚æ˜¾è‘—ï¼ˆt-SNE å¯è§†åŒ–ï¼‰ã€‚å¦‚ä½•å¯é åœ°å°†ä¸€ä¸ªæ¨¡å‹çš„ cache â€œæŠ•å½±â€åˆ°å¦ä¸€ä¸ªæ¨¡å‹çš„è¯­ä¹‰ç©ºé—´å¹¶è¢«æœ‰æ•ˆåˆ©ç”¨æ˜¯æŒ‘æˆ˜ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;å¯¹é½é—®é¢˜ï¼ˆtoken-level &amp;amp; layer-levelï¼‰&lt;/strong&gt;ï¼šä¸åŒ tokenizer äº§ç”Ÿä¸åŒ token åˆ’åˆ†ï¼Œä¸”æ¨¡å‹å±‚æ•°ä¸åŒï¼Œå¿…é¡»å¤„ç† token å¯¹é½å’Œå±‚å¯¹é½é—®é¢˜ï¼ˆé¿å…ä¿¡æ¯ä¸¢å¤±æˆ–é”™ä½æ³¨å…¥ï¼‰ã€‚æ–‡ä¸­æå‡ºäº† token è§£ç å† re-encode çš„å¯¹é½ç­–ç•¥å’Œ â€œterminal alignmentâ€ å±‚å¯¹é½ç­–ç•¥ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;é¿å…ç ´åæ¥æ”¶è€…åŸæœ‰è¯­ä¹‰&lt;/strong&gt;ï¼šç›´æ¥ç”¨åˆ«äºº cache è¦†ç›–ä¼šç ´åæ¥æ”¶æ¨¡å‹å·²æœ‰è¯­ä¹‰/ç»“æ„ï¼Œéœ€è®¾è®¡æ®‹å·®å¼ã€å¯æ§çš„èåˆæœºåˆ¶ï¼ˆå³ Fuser çš„è®¾è®¡ï¼‰ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;é€‰æ‹©æ€§æ³¨å…¥ï¼ˆå“ªäº›å±‚æ³¨å…¥ã€æ³¨å…¥å¤šå°‘ï¼‰&lt;/strong&gt;ï¼šå¹¶éæ‰€æœ‰å±‚æ³¨å…¥éƒ½æœ‰åˆ©ï¼Œå•å±‚/å¤šå±‚æ³¨å…¥æ•ˆæœå·®å¼‚æ˜æ˜¾ï¼ˆAppendix çš„å•å±‚å®éªŒæ˜¾ç¤ºæœ‰å±‚å¢ç›Šä¹Ÿæœ‰å±‚ä¸‹é™ï¼‰ï¼Œå› æ­¤éœ€è¦ learnable gate æ¥é€‰æ‹©æ³¨å…¥ä½ç½®ä¸æ¯”ä¾‹ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;æ•ˆç‡ / å»¶è¿Ÿæƒè¡¡&lt;/strong&gt;ï¼šè™½ç„¶ C2C ç›®æ ‡æ˜¯é™ä½å»¶è¿Ÿï¼Œä½†åœ¨å®ç°ä¸Šè¦ä¿è¯æŠ•å½±/èåˆæœ¬èº«ä¸ä¼šå¼•å…¥æ¯”æ–‡æœ¬é€šä¿¡æ›´é«˜çš„å¼€é”€ï¼ˆè®¾è®¡è½»é‡ Fuser å¹¶å†»ç»“ä¸»æ¨¡å‹å‚æ•°ä»¥é™ä½è®­ç»ƒ/æ¨ç†æˆæœ¬ï¼‰ã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-observations--analysis&#34;&gt;ğŸ” Observations / Analysis&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/C2C/f34t12.png&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
