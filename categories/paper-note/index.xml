<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Paper Note on JJ&#39;s Blog</title>
    <link>https://jjl357.github.io/blog/categories/paper-note/</link>
    <description>Recent content in Paper Note on JJ&#39;s Blog</description>
    <generator>Hugo -- 0.152.2</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://jjl357.github.io/blog/categories/paper-note/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</title>
      <link>https://jjl357.github.io/blog/posts/adaspec---selective-knowledge-distillation-for-efficient-speculative-decoders---neurips25-spotlight/</link>
      <pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/adaspec---selective-knowledge-distillation-for-efficient-speculative-decoders---neurips25-spotlight/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/AdaSpec/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; &lt;strong&gt;NeurIPS&#39;25 Spotlight&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/yuezhouhu/adaspec&#34;&gt;https://github.com/yuezhouhu/adaspec&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;my-thoughts&#34;&gt;My Thoughts&lt;/h2&gt;
&lt;p&gt;è¿™ç¯‡è®ºæ–‡çš„ methods æŒºç®€æ˜çš„ï¼Œæ„Ÿè§‰å¯ä»¥æœ‰ä¸ªæ–° idea:&lt;br&gt;
&lt;strong&gt;å°† MoSD å’Œ AdaSPEC ç»“åˆèµ·æ¥&lt;/strong&gt; â€”â€” é’ˆå¯¹ä¸åŒéš¾åº¦çš„ tokens distill å‡ºä¸åŒçš„ draft modelsï¼Œåˆ©ç”¨ router å°†ä¸åŒéš¾åº¦çš„ tokens é€‰æ‹©æœ€é€‚åˆçš„å¯¹åº” draft model æ¥è¿›è¡Œ SDã€‚&lt;/p&gt;
&lt;p&gt;é—®é¢˜åœ¨äºï¼šå¯¹äºè®ºæ–‡ä¸­æåˆ°çš„â€œhard tokensâ€ï¼Œæ˜¯å¦èƒ½ distill å‡ºä¸€ä¸ªåˆé€‚ä¸”æœ‰ç”¨çš„ draft modelï¼Ÿ&lt;br&gt;
è®ºæ–‡ç»“æœæ˜¾ç¤ºå½“å‰æ–¹æ³•å¯¹è¿™äº› token æ•ˆæœè¾ƒå·®ï¼ˆç”šè‡³æ¯” reference model è¿˜å·®ï¼‰ï¼Œ  å› æ­¤éœ€è¦æ–°çš„æ–¹å¼æ¥æ”¹è¿›è¿™ä¸€éƒ¨åˆ†çš„è’¸é¦ä¸åˆ©ç”¨æœºåˆ¶ã€‚&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;p&gt;Speculative Decoding (SD) accelerates large language model inference by employing a small &lt;strong&gt;draft model&lt;/strong&gt; to generate predictions, which are then verified by a larger &lt;strong&gt;target model&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>R-KV: Redundancy-aware KV Cache Compression for Reasoning Models</title>
      <link>https://jjl357.github.io/blog/posts/r-kv---redundancy-aware-kv-cache-compression-for-reasoning-models---neurips25/</link>
      <pubDate>Sat, 25 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/r-kv---redundancy-aware-kv-cache-compression-for-reasoning-models---neurips25/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/RKV/title.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; NeurIPS&#39;25
&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/Zefan-Cai/R-KV&#34;&gt;https://github.com/Zefan-Cai/R-KV&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;my-thoughts&#34;&gt;My Thoughts&lt;/h2&gt;
&lt;p&gt;R-KV é’ˆå¯¹previous worksåœ¨efficient reasoningä¸­ç”±äºåªä¾é attention scoresè€Œé€ æˆè¿‡å¤šåœ°ä¿ç•™äº†redundant tokens(é‡å¤è€Œä¸”å¯¹æ¨ç†æ²¡æœ‰ä¿¡æ¯å¢ç›Šçš„token),R-KVçš„é‡ç‚¹æˆ‘è§‰å¾—æ˜¯å¢åŠ äº†Redundancy Estimation via Semantic Similarityæ¥è§£å†³è¿™ä¸ªé—®é¢˜,å°±Evaluationçš„ç»“æœæ¥è¯´ï¼ŒR-KVçš„æ•ˆæœæ˜¯éå¸¸å¥½çš„ï¼Œå³æå‡äº†Throughputè¿˜maintainäº†performanceï¼Œç”šè‡³åœ¨budgetå……è¶³çš„æƒ…å†µä¸‹è¿˜å¯ä»¥åšåˆ°æç‚¹ã€‚&lt;/p&gt;
&lt;h2 id=&#34;1-motivation&#34;&gt;1. Motivation&lt;/h2&gt;
&lt;p&gt;Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning.
However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference.&lt;/p&gt;
&lt;p&gt;For instance, a &lt;strong&gt;DeepSeek-R1-Distill-Llama-8B&lt;/strong&gt; model may generate &lt;strong&gt;32K tokens&lt;/strong&gt; to solve a complex math problem, consuming &lt;strong&gt;15.5GB&lt;/strong&gt; of memory to load model weights and &lt;strong&gt;4.1GB&lt;/strong&gt; to store the KV cache.
This long CoT (chain-of-thought) generation necessitates the development of &lt;strong&gt;KV cache compression&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cache-to-Cache: Direct Semantic Communication Between Large Language Models</title>
      <link>https://jjl357.github.io/blog/posts/cache-to-cache---direct-semantic-communication-between-large-language-models/</link>
      <pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jjl357.github.io/blog/posts/cache-to-cache---direct-semantic-communication-between-large-language-models/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/C2C/title.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;githubï¼š&lt;a href=&#34;https://github.com/thu-nics/C2C&#34;&gt;https://github.com/thu-nics/C2C&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-motivationåŠ¨æœº--èƒŒæ™¯&#34;&gt;ğŸ’¡ Motivationï¼ˆåŠ¨æœº / èƒŒæ™¯ï¼‰&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/C2C/figure1.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç°çŠ¶ï¼šå¤š-LLM ç³»ç»Ÿé€šå¸¸é€šè¿‡ &lt;strong&gt;æ–‡æœ¬ï¼ˆText-to-Text, T2Tï¼‰&lt;/strong&gt; ç›¸äº’é€šä¿¡ â€”â€” ä¸€ä¸ªæ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼Œå¦ä¸€ä¸ªæ¨¡å‹å†è¯»å…¥æ–‡æœ¬å¹¶å¤„ç†ã€‚è¿™ç§æ–¹å¼å­˜åœ¨çš„ä¿¡æ¯ç“¶é¢ˆï¼ˆé«˜ç»´å†…éƒ¨è¡¨ç¤ºè¢«å‹æˆæ–‡æœ¬ï¼‰ã€æ­§ä¹‰æ€§ï¼ˆè‡ªç„¶è¯­è¨€æœ¬èº«çš„æ¨¡ç³Šæ€§ï¼‰ä»¥åŠ &lt;strong&gt;ä¸²è¡Œè§£ç å¸¦æ¥çš„å»¶è¿Ÿ&lt;/strong&gt; ç­‰é—®é¢˜ã€‚&lt;/li&gt;
&lt;li&gt;é—®é¢˜ï¼šæ–‡æœ¬é€šä¿¡æ— æ³•å®Œæ•´ä¿ç•™æ¨¡å‹å†…éƒ¨çš„é«˜ç»´è¯­ä¹‰ä¿¡æ¯ï¼ˆä¾‹å¦‚ KV-Cache ä¸­çš„ rich semanticsï¼‰ï¼ŒåŒæ—¶æ–‡æœ¬é€šä¿¡éœ€è¦é€ token è§£ç ï¼Œå¢åŠ é€šä¿¡ä¸æ¨ç†å»¶è¿Ÿã€‚&lt;/li&gt;
&lt;li&gt;æå‡ºçš„é—®é¢˜ï¼ˆæ ¸å¿ƒç ”ç©¶é—®ï¼‰ï¼š&lt;strong&gt;LLMä¹‹é—´èƒ½å¦â€œè¶…è¶Šæ–‡æœ¬â€ç›´æ¥é€šä¿¡ï¼Ÿèƒ½å¦é€šè¿‡å…±äº«/è½¬æ¢/èåˆ KV-Cacheï¼ˆkey/value cacheï¼‰æ¥å®ç°æ›´ä¸°å¯Œã€æ›´ä½å»¶è¿Ÿçš„è¯­ä¹‰é€šä¿¡ï¼Ÿ&lt;/strong&gt;ï¼ˆæ–‡ä¸­ä»¥ â€œCan LLMs communicate beyond text?â€ ä½œæ ¸å¿ƒé©±åŠ¨ã€‚ï¼‰&lt;/li&gt;
&lt;li&gt;ä¸»è¦è§‚å¯Ÿé©±åŠ¨ï¼šä½œè€…çš„ Oracle å®éªŒæ˜¾ç¤º
(1) åœ¨ä¸æ‰©å¤§åºåˆ—é•¿åº¦çš„å‰æä¸‹ï¼Œä¸°å¯Œ KV-Cache å¯ä»¥æå‡å›ç­”è´¨é‡ï¼›
(2) ä¸åŒæ¨¡å‹çš„ KV-Cache åœ¨è¡¨ç¤ºç©ºé—´ä¸Šæ˜¯å¯è½¬æ¢/å¯å¯¹é½çš„ï¼ˆç»è¿‡è®­ç»ƒçš„ç®€å• MLP å¯å°†ä¸€ä¸ªæ¨¡å‹çš„ KV æ˜ å°„åˆ°å¦ä¸€ä¸ªæ¨¡å‹ç©ºé—´ï¼‰ã€‚
è¿™äº›è§‚å¯Ÿæ”¯æŒç”¨ KV-Cache ä½œä¸ºé€šä¿¡åª’ä»‹çš„å¯è¡Œæ€§ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-challengesæŒ‘æˆ˜&#34;&gt;âš” Challengesï¼ˆæŒ‘æˆ˜ï¼‰&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/C2C/figure2.png&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;è·¨æ¨¡å‹çš„ KV-Cache è¡¨ç¤ºå·®å¼‚&lt;/strong&gt;ï¼šä¸åŒæ¨¡å‹ï¼ˆä¸åŒæ—ã€ä¸åŒè§„æ¨¡ã€ä¸åŒ tokenizerï¼‰åœ¨åŒä¸€è¾“å…¥ä¸Šäº§ç”Ÿçš„ KV-Cache åˆ†å¸ƒå·®å¼‚æ˜¾è‘—ï¼ˆt-SNE å¯è§†åŒ–ï¼‰ã€‚å¦‚ä½•å¯é åœ°å°†ä¸€ä¸ªæ¨¡å‹çš„ cache â€œæŠ•å½±â€åˆ°å¦ä¸€ä¸ªæ¨¡å‹çš„è¯­ä¹‰ç©ºé—´å¹¶è¢«æœ‰æ•ˆåˆ©ç”¨æ˜¯æŒ‘æˆ˜ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;å¯¹é½é—®é¢˜ï¼ˆtoken-level &amp;amp; layer-levelï¼‰&lt;/strong&gt;ï¼šä¸åŒ tokenizer äº§ç”Ÿä¸åŒ token åˆ’åˆ†ï¼Œä¸”æ¨¡å‹å±‚æ•°ä¸åŒï¼Œå¿…é¡»å¤„ç† token å¯¹é½å’Œå±‚å¯¹é½é—®é¢˜ï¼ˆé¿å…ä¿¡æ¯ä¸¢å¤±æˆ–é”™ä½æ³¨å…¥ï¼‰ã€‚æ–‡ä¸­æå‡ºäº† token è§£ç å† re-encode çš„å¯¹é½ç­–ç•¥å’Œ â€œterminal alignmentâ€ å±‚å¯¹é½ç­–ç•¥ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;é¿å…ç ´åæ¥æ”¶è€…åŸæœ‰è¯­ä¹‰&lt;/strong&gt;ï¼šç›´æ¥ç”¨åˆ«äºº cache è¦†ç›–ä¼šç ´åæ¥æ”¶æ¨¡å‹å·²æœ‰è¯­ä¹‰/ç»“æ„ï¼Œéœ€è®¾è®¡æ®‹å·®å¼ã€å¯æ§çš„èåˆæœºåˆ¶ï¼ˆå³ Fuser çš„è®¾è®¡ï¼‰ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;é€‰æ‹©æ€§æ³¨å…¥ï¼ˆå“ªäº›å±‚æ³¨å…¥ã€æ³¨å…¥å¤šå°‘ï¼‰&lt;/strong&gt;ï¼šå¹¶éæ‰€æœ‰å±‚æ³¨å…¥éƒ½æœ‰åˆ©ï¼Œå•å±‚/å¤šå±‚æ³¨å…¥æ•ˆæœå·®å¼‚æ˜æ˜¾ï¼ˆAppendix çš„å•å±‚å®éªŒæ˜¾ç¤ºæœ‰å±‚å¢ç›Šä¹Ÿæœ‰å±‚ä¸‹é™ï¼‰ï¼Œå› æ­¤éœ€è¦ learnable gate æ¥é€‰æ‹©æ³¨å…¥ä½ç½®ä¸æ¯”ä¾‹ã€‚&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;æ•ˆç‡ / å»¶è¿Ÿæƒè¡¡&lt;/strong&gt;ï¼šè™½ç„¶ C2C ç›®æ ‡æ˜¯é™ä½å»¶è¿Ÿï¼Œä½†åœ¨å®ç°ä¸Šè¦ä¿è¯æŠ•å½±/èåˆæœ¬èº«ä¸ä¼šå¼•å…¥æ¯”æ–‡æœ¬é€šä¿¡æ›´é«˜çš„å¼€é”€ï¼ˆè®¾è®¡è½»é‡ Fuser å¹¶å†»ç»“ä¸»æ¨¡å‹å‚æ•°ä»¥é™ä½è®­ç»ƒ/æ¨ç†æˆæœ¬ï¼‰ã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-observations--analysis&#34;&gt;ğŸ” Observations / Analysis&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://jjl357.github.io/blog/image/C2C/f34t12.png&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
